{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import texthero as hero\n",
    "from texthero import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_json(r\"/home/shah/Desktop/FB-Marketplace-Recommendation-Ranking-System/data/products_table.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data['product_description']= hero.clean(text_data['product_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_pipeline = [preprocessing.fillna,\n",
    "                   #preprocessing.lowercase,\n",
    "                   preprocessing.remove_whitespace,\n",
    "                   preprocessing.remove_diacritics\n",
    "                   #preprocessing.remove_brackets\n",
    "                  ]\n",
    "text_data['product_description'] = hero.clean(text_data['product_description'], custom_pipeline)\n",
    "text_data['product_description'] = [n.replace('{','') for n in text_data['product_description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.drop(['product_name', 'price', 'location',\n",
    "       'page_id', 'create_time', 'category'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_n_a_rows(df, column: str):\n",
    "    # Swap N/A for the pandas nan, so we can drop them\n",
    "    temp_df = df[column].replace('N/A', np.nan)\n",
    "    temp_df = temp_df.dropna()\n",
    "    # Create a new df with only the records without the nans\n",
    "    clean_df = pd.merge(temp_df, df,\n",
    "                            left_index=True, right_index=True)\n",
    "    # The merge creates a duplicate column. Remove it.\n",
    "    clean_df.drop(column + '_x', inplace=True, axis=1)\n",
    "    # Rename the remaining category column\n",
    "    clean_df.rename(columns={column + '_y': column}, inplace=True)\n",
    "    # Commit the cleansed data to the dataframe\n",
    "    df = clean_df\n",
    "    return df\n",
    "\n",
    "new_text = remove_n_a_rows(new_text, 'product_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = new_text.dropna()\n",
    "new_text = new_text.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text['product_description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split() \n",
    "        input = [word2id[n] for n in word[:-1]]\n",
    "        target = word2id[word[-1]] \n",
    "\n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    print(input_batch)\n",
    "    print(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = \" \".join(text_data['product_description']).split()\n",
    "word_list = list(word_list)\n",
    "word2id = {w: i for i, w in enumerate(word_list)}\n",
    "id2word = {i: w for i, w in enumerate(word_list)}\n",
    "n_class = len(word2id)\n",
    "\n",
    "n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "class NNLM(nn.Module):\n",
    "   def __init__(self):\n",
    "       super(NNLM, self).__init__()\n",
    "       self.embeddings = nn.Embedding(n_class, m) #embedding layer or look up table\n",
    "\n",
    "       self.hidden1 = nn.Linear(n_step * m, n_hidden, bias=False)\n",
    "       self.ones = nn.Parameter(torch.ones(n_hidden))\n",
    "      \n",
    "       self.hidden2 = nn.Linear(n_hidden, n_class, bias=False)\n",
    "       self.hidden3 = nn.Linear(n_step * m, n_class, bias=False) #final layer\n",
    "      \n",
    "       self.bias = nn.Parameter(torch.ones(n_class))\n",
    "\n",
    "   def forward(self, X):\n",
    "       X = self.embeddings(X) # embeddings\n",
    "       X = X.view(-1, n_step * m) # first layer\n",
    "       tanh = torch.tanh(self.d + self.hidden1(X)) # tanh layer\n",
    "       output = self.b + self.hidden3(X) + self.hidden2(tanh) # summing up all the layers with bias\n",
    "       return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 2\n",
    "n_hidden = 2\n",
    "m = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNLM()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch, target_batch = make_batch(text_data['product_description'])\n",
    "input_batch = torch.LongTensor(input_batch)\n",
    "target_batch = torch.LongTensor(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    embeddings, output = model(input_batch)\n",
    "\n",
    "    # output : [batch_size, n_class], target_batch : [batch_size]\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predict\n",
    "predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "\n",
    "# Test\n",
    "print([sen.split()[:2] for sen in new_text['product_description']], '->', [id2word[n.item()] for n in predict.squeeze()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model using word2vec"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b099c207f7071e7802aeb8fc32298ef1a4dd7cfb6d3bb3ada8c8b299b44af55f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 ('rtx_3060')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
