{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah/miniconda3/envs/rtx_3060/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5796), started 1:41:15 ago. (Use '!kill 5796' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-da10fc5a639479f6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-da10fc5a639479f6\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "\n",
    "    'test': transforms.Compose([\n",
    "        #transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "    ])\n",
    "}\n",
    "\n",
    "path_dir = '/home/shah/Desktop/FB-Marketplace-Recommendation-Ranking-System/data/data_images/'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(path_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "                  \n",
    "train_dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,\n",
    "                                             shuffle=True, num_workers=0)\n",
    "              for x in ['train', 'val']}\n",
    "test_dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,\n",
    "                                             shuffle=False, num_workers=0)\n",
    "              for x in ['test']}\n",
    "\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val','test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "#class_names = {k: v for v, k in enumerate(class_names)}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Appliances ',\n",
       " 'Baby & Kids Stuff ',\n",
       " 'Clothes, Footwear & Accessories ',\n",
       " 'Computers & Software ',\n",
       " 'DIY Tools & Materials ',\n",
       " 'Health & Beauty ',\n",
       " 'Home & Garden ',\n",
       " 'Music, Films, Books & Games ',\n",
       " 'Office Furniture & Equipment ',\n",
       " 'Other Goods ',\n",
       " 'Phones, Mobile Phones & Telecoms ',\n",
       " 'Sports, Leisure & Travel ',\n",
       " 'Video Games & Consoles ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 8108\n",
       "    Root location: /home/shah/Desktop/FB-Marketplace-Recommendation-Ranking-System/data/data_images/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(train_dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, batch_size):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                print(\"Training in progress\")\n",
    "                model.train()  \n",
    "            elif phase == 'val':\n",
    "                print(\"Validating in progress\")\n",
    "                model.eval()   \n",
    "            \n",
    "            num_corrects = 0\n",
    "            num_samples = 0\n",
    "            counter = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in train_dataloaders[phase]:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                #with torch.set_grad_enabled(phase == 'train'):\n",
    "                counter += 1\n",
    "                predict = model(inputs)\n",
    "                labels = labels\n",
    "                _, preds = torch.max(predict, 1)\n",
    "                loss = criterion(predict, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if counter % 10 == 9:\n",
    "                    print(f'[{epoch + 1}, {counter + 1:5d}] loss: {loss}')\n",
    "                    if phase == 'train':\n",
    "                        writer.add_scalar('Training Loss', loss, epoch)\n",
    "                    else:\n",
    "                        writer.add_scalar('Validation Loss', loss, epoch)\n",
    "\n",
    "                  \n",
    "                num_corrects += torch.sum(preds == labels).sum()\n",
    "                num_samples += preds.size(0)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            acc = float(num_corrects) / num_samples\n",
    "\n",
    "            print(f'Got {num_corrects} / {num_samples} with accuracy: {acc * 100}%')\n",
    "            writer.add_scalar('Training Accuracy', acc, epoch)\n",
    "            writer.add_scalar('Validation Accuracy', acc, epoch)\n",
    "            # input(\"EPOCH COMPLETE: Press Enter to continue...\")\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), '/home/shah/Desktop/FB-Marketplace-Recommendation-Ranking-System/data/CNN.pt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(loader, model):\n",
    "\n",
    "    model.eval()\n",
    "    if loader == train_dataloaders['train']:\n",
    "        print('Checking accuracy on training set..')\n",
    "    else:\n",
    "        print('Checking accuracy on evaluation set')\n",
    "    label_correct = 0\n",
    "    num_samples = 0\n",
    "  \n",
    "    with torch.no_grad(): \n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1) \n",
    "\n",
    "            num_samples += preds.size(0)\n",
    "            label_correct += (preds == labels).sum()\n",
    "        acc = float(label_correct) / num_samples\n",
    "\n",
    "        print(f'Test Accuracy of the model: {label_correct}/ {num_samples} with accuracy of: {acc * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.features = models.resnet50(pretrained=True)\n",
    "        for i, param in enumerate(self.features.parameters()):\n",
    "            if i  < 47: #freeze 47 layers\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "        self.features.fc = nn.Sequential(\n",
    "                nn.Linear(2048, 1024),    \n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(1024, 512),\n",
    "                nn.ReLU(),\n",
    "                #nn.Dropout(),\n",
    "                nn.Linear(512, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(),\n",
    "                nn.Linear(128, 13))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.reshape[0], -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "#writer = SummaryWriter()\n",
    "#model = models.resnet50(pretrained=True)\n",
    "#summary(model, (3, 224, 224), device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "Training in progress\n",
      "[1,    10] loss: 2.597126007080078\n",
      "[1,    20] loss: 2.588656425476074\n",
      "[1,    30] loss: 2.5685501098632812\n",
      "[1,    40] loss: 2.5575122833251953\n",
      "[1,    50] loss: 2.5467286109924316\n",
      "[1,    60] loss: 2.529641628265381\n",
      "[1,    70] loss: 2.520297050476074\n",
      "[1,    80] loss: 2.5135385990142822\n",
      "[1,    90] loss: 2.572355270385742\n",
      "[1,   100] loss: 2.4942433834075928\n",
      "[1,   110] loss: 2.561079263687134\n",
      "[1,   120] loss: 2.5588297843933105\n",
      "[1,   130] loss: 2.592216968536377\n",
      "[1,   140] loss: 2.5132837295532227\n",
      "[1,   150] loss: 2.6036911010742188\n",
      "[1,   160] loss: 2.6303813457489014\n",
      "[1,   170] loss: 2.5719940662384033\n",
      "[1,   180] loss: 2.5853912830352783\n",
      "[1,   190] loss: 2.47983980178833\n",
      "[1,   200] loss: 2.609980821609497\n",
      "[1,   210] loss: 2.501509428024292\n",
      "[1,   220] loss: 2.5503311157226562\n",
      "[1,   230] loss: 2.696589708328247\n",
      "[1,   240] loss: 2.4289932250976562\n",
      "[1,   250] loss: 2.516127586364746\n",
      "[1,   260] loss: 2.5556552410125732\n",
      "[1,   270] loss: 2.5561256408691406\n",
      "[1,   280] loss: 2.3416380882263184\n",
      "[1,   290] loss: 2.4995737075805664\n",
      "[1,   300] loss: 2.619741916656494\n",
      "[1,   310] loss: 2.4475250244140625\n",
      "[1,   320] loss: 2.4326698780059814\n",
      "[1,   330] loss: 2.5911614894866943\n",
      "[1,   340] loss: 2.4809539318084717\n",
      "[1,   350] loss: 2.5560548305511475\n",
      "[1,   360] loss: 2.479954242706299\n",
      "[1,   370] loss: 2.458852767944336\n",
      "[1,   380] loss: 2.5399935245513916\n",
      "[1,   390] loss: 2.5180420875549316\n",
      "[1,   400] loss: 2.498060464859009\n",
      "[1,   410] loss: 2.458467960357666\n",
      "[1,   420] loss: 2.487605571746826\n",
      "[1,   430] loss: 2.6424827575683594\n",
      "[1,   440] loss: 2.5593318939208984\n",
      "[1,   450] loss: 2.63608980178833\n",
      "[1,   460] loss: 2.40811824798584\n",
      "[1,   470] loss: 2.599320650100708\n",
      "[1,   480] loss: 2.495681047439575\n",
      "[1,   490] loss: 2.507183074951172\n",
      "[1,   500] loss: 2.489562511444092\n",
      "[1,   510] loss: 2.6829915046691895\n",
      "[1,   520] loss: 2.5086917877197266\n",
      "[1,   530] loss: 2.6255393028259277\n",
      "[1,   540] loss: 2.456735372543335\n",
      "[1,   550] loss: 2.621974468231201\n",
      "[1,   560] loss: 2.578540325164795\n",
      "[1,   570] loss: 2.5603153705596924\n",
      "[1,   580] loss: 2.4862022399902344\n",
      "[1,   590] loss: 2.4813621044158936\n",
      "[1,   600] loss: 2.6030683517456055\n",
      "[1,   610] loss: 2.309400796890259\n",
      "[1,   620] loss: 2.362692356109619\n",
      "[1,   630] loss: 2.533179998397827\n",
      "[1,   640] loss: 2.5531749725341797\n",
      "[1,   650] loss: 2.205448627471924\n",
      "[1,   660] loss: 2.5417232513427734\n",
      "[1,   670] loss: 2.3833160400390625\n",
      "[1,   680] loss: 2.4070017337799072\n",
      "[1,   690] loss: 2.4947588443756104\n",
      "[1,   700] loss: 2.515477418899536\n",
      "[1,   710] loss: 2.4733660221099854\n",
      "[1,   720] loss: 2.25927734375\n",
      "[1,   730] loss: 2.400127410888672\n",
      "[1,   740] loss: 2.3704934120178223\n",
      "[1,   750] loss: 2.2614355087280273\n",
      "[1,   760] loss: 2.421638250350952\n",
      "[1,   770] loss: 2.273036479949951\n",
      "[1,   780] loss: 2.254631280899048\n",
      "[1,   790] loss: 2.1857943534851074\n",
      "[1,   800] loss: 2.649885654449463\n",
      "[1,   810] loss: 1.8013710975646973\n",
      "[1,   820] loss: 2.0238513946533203\n",
      "[1,   830] loss: 2.0150856971740723\n",
      "[1,   840] loss: 1.7666527032852173\n",
      "[1,   850] loss: 2.2910003662109375\n",
      "[1,   860] loss: 2.2459378242492676\n",
      "[1,   870] loss: 1.9421453475952148\n",
      "[1,   880] loss: 2.031466484069824\n",
      "[1,   890] loss: 2.477856159210205\n",
      "[1,   900] loss: 2.1038997173309326\n",
      "[1,   910] loss: 2.403334140777588\n",
      "[1,   920] loss: 2.3160197734832764\n",
      "[1,   930] loss: 1.988361120223999\n",
      "[1,   940] loss: 2.140519380569458\n",
      "[1,   950] loss: 2.6956472396850586\n",
      "[1,   960] loss: 2.027674674987793\n",
      "[1,   970] loss: 2.2019741535186768\n",
      "[1,   980] loss: 2.702760696411133\n",
      "[1,   990] loss: 1.8651899099349976\n",
      "[1,  1000] loss: 1.9018805027008057\n",
      "[1,  1010] loss: 2.0535244941711426\n",
      "Got 1338 / 8108 with accuracy: 16.502220029600394%\n",
      "Validating in progress\n",
      "[1,    10] loss: 2.2636051177978516\n",
      "[1,    20] loss: 2.1684041023254395\n",
      "[1,    30] loss: 2.087387800216675\n",
      "[1,    40] loss: 2.309235095977783\n",
      "[1,    50] loss: 1.959969401359558\n",
      "[1,    60] loss: 1.9450033903121948\n",
      "[1,    70] loss: 1.9557815790176392\n",
      "[1,    80] loss: 2.489819049835205\n",
      "[1,    90] loss: 2.1278440952301025\n",
      "[1,   100] loss: 1.7813001871109009\n",
      "[1,   110] loss: 2.1748223304748535\n",
      "[1,   120] loss: 1.734979271888733\n",
      "[1,   130] loss: 1.2389795780181885\n",
      "[1,   140] loss: 1.9299719333648682\n",
      "[1,   150] loss: 2.505613327026367\n",
      "[1,   160] loss: 2.164088249206543\n",
      "[1,   170] loss: 1.4929810762405396\n",
      "[1,   180] loss: 1.7706435918807983\n",
      "[1,   190] loss: 1.8251636028289795\n",
      "[1,   200] loss: 1.3361625671386719\n",
      "[1,   210] loss: 2.844233989715576\n",
      "[1,   220] loss: 2.256621837615967\n",
      "[1,   230] loss: 2.429173231124878\n",
      "[1,   240] loss: 2.538278341293335\n",
      "[1,   250] loss: 2.454759120941162\n",
      "[1,   260] loss: 1.3750723600387573\n",
      "[1,   270] loss: 2.207611083984375\n",
      "[1,   280] loss: 1.5452662706375122\n",
      "[1,   290] loss: 1.9302458763122559\n",
      "Got 773 / 2315 with accuracy: 33.39092872570195%\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "Training in progress\n",
      "[2,    10] loss: 1.8350375890731812\n",
      "[2,    20] loss: 1.8990026712417603\n",
      "[2,    30] loss: 2.06834077835083\n",
      "[2,    40] loss: 2.0590667724609375\n",
      "[2,    50] loss: 2.015415668487549\n",
      "[2,    60] loss: 2.218034029006958\n",
      "[2,    70] loss: 1.8078364133834839\n",
      "[2,    80] loss: 2.0611560344696045\n",
      "[2,    90] loss: 1.8869845867156982\n",
      "[2,   100] loss: 2.3739657402038574\n",
      "[2,   110] loss: 2.3206262588500977\n",
      "[2,   120] loss: 1.181463599205017\n",
      "[2,   130] loss: 2.1043272018432617\n",
      "[2,   140] loss: 2.583641529083252\n",
      "[2,   150] loss: 2.430819272994995\n",
      "[2,   160] loss: 2.571319580078125\n",
      "[2,   170] loss: 1.6436583995819092\n",
      "[2,   180] loss: 2.3433454036712646\n",
      "[2,   190] loss: 1.4610040187835693\n",
      "[2,   200] loss: 1.8053333759307861\n",
      "[2,   210] loss: 1.8547779321670532\n",
      "[2,   220] loss: 1.4169573783874512\n",
      "[2,   230] loss: 1.9019631147384644\n",
      "[2,   240] loss: 1.4608540534973145\n",
      "[2,   250] loss: 2.5710930824279785\n",
      "[2,   260] loss: 1.700472116470337\n",
      "[2,   270] loss: 1.6966146230697632\n",
      "[2,   280] loss: 1.5364171266555786\n",
      "[2,   290] loss: 2.235921621322632\n",
      "[2,   300] loss: 1.4944480657577515\n",
      "[2,   310] loss: 1.8040045499801636\n",
      "[2,   320] loss: 1.790185809135437\n",
      "[2,   330] loss: 1.7470024824142456\n",
      "[2,   340] loss: 1.8440868854522705\n",
      "[2,   350] loss: 2.00226092338562\n",
      "[2,   360] loss: 1.74460768699646\n",
      "[2,   370] loss: 1.847642421722412\n",
      "[2,   380] loss: 2.3586976528167725\n",
      "[2,   390] loss: 1.8008302450180054\n",
      "[2,   400] loss: 1.8070268630981445\n",
      "[2,   410] loss: 1.432325005531311\n",
      "[2,   420] loss: 1.8532183170318604\n",
      "[2,   430] loss: 2.1362690925598145\n",
      "[2,   440] loss: 1.7495613098144531\n",
      "[2,   450] loss: 2.480940103530884\n",
      "[2,   460] loss: 1.5552862882614136\n",
      "[2,   470] loss: 1.6397087574005127\n",
      "[2,   480] loss: 1.818629264831543\n",
      "[2,   490] loss: 1.4854066371917725\n",
      "[2,   500] loss: 1.8459696769714355\n",
      "[2,   510] loss: 1.95294988155365\n",
      "[2,   520] loss: 1.671849012374878\n",
      "[2,   530] loss: 1.787266492843628\n",
      "[2,   540] loss: 1.4221872091293335\n",
      "[2,   550] loss: 1.6957772970199585\n",
      "[2,   560] loss: 2.470850706100464\n",
      "[2,   570] loss: 1.8616582155227661\n",
      "[2,   580] loss: 2.3131778240203857\n",
      "[2,   590] loss: 2.1317789554595947\n",
      "[2,   600] loss: 1.124499797821045\n",
      "[2,   610] loss: 2.4505205154418945\n",
      "[2,   620] loss: 1.4873963594436646\n",
      "[2,   630] loss: 1.4472540616989136\n",
      "[2,   640] loss: 1.8347564935684204\n",
      "[2,   650] loss: 2.650108575820923\n",
      "[2,   660] loss: 1.5913587808609009\n",
      "[2,   670] loss: 1.927107572555542\n",
      "[2,   680] loss: 1.3246111869812012\n",
      "[2,   690] loss: 2.2683775424957275\n",
      "[2,   700] loss: 1.9196357727050781\n",
      "[2,   710] loss: 1.8844612836837769\n",
      "[2,   720] loss: 1.7214443683624268\n",
      "[2,   730] loss: 1.188752293586731\n",
      "[2,   740] loss: 1.8477284908294678\n",
      "[2,   750] loss: 1.7856413125991821\n",
      "[2,   760] loss: 1.5475850105285645\n",
      "[2,   770] loss: 1.3191057443618774\n",
      "[2,   780] loss: 2.43701434135437\n",
      "[2,   790] loss: 1.8503165245056152\n",
      "[2,   800] loss: 2.0202722549438477\n",
      "[2,   810] loss: 1.6024190187454224\n",
      "[2,   820] loss: 2.3686318397521973\n",
      "[2,   830] loss: 1.8021204471588135\n",
      "[2,   840] loss: 1.4542276859283447\n",
      "[2,   850] loss: 2.425248861312866\n",
      "[2,   860] loss: 1.8220231533050537\n",
      "[2,   870] loss: 2.418492078781128\n",
      "[2,   880] loss: 1.5701063871383667\n",
      "[2,   890] loss: 1.8745989799499512\n",
      "[2,   900] loss: 1.7525649070739746\n",
      "[2,   910] loss: 2.0024185180664062\n",
      "[2,   920] loss: 1.6734293699264526\n",
      "[2,   930] loss: 1.6212518215179443\n",
      "[2,   940] loss: 1.8149800300598145\n",
      "[2,   950] loss: 1.9153270721435547\n",
      "[2,   960] loss: 2.1278984546661377\n",
      "[2,   970] loss: 1.4749360084533691\n",
      "[2,   980] loss: 2.2691948413848877\n",
      "[2,   990] loss: 1.7449326515197754\n",
      "[2,  1000] loss: 1.4456777572631836\n",
      "[2,  1010] loss: 1.8756989240646362\n",
      "Got 3194 / 8108 with accuracy: 39.39319190922546%\n",
      "Validating in progress\n",
      "[2,    10] loss: 1.4264705181121826\n",
      "[2,    20] loss: 1.7567241191864014\n",
      "[2,    30] loss: 2.2428081035614014\n",
      "[2,    40] loss: 1.805910587310791\n",
      "[2,    50] loss: 2.163788318634033\n",
      "[2,    60] loss: 1.6432368755340576\n",
      "[2,    70] loss: 1.7449650764465332\n",
      "[2,    80] loss: 1.3506929874420166\n",
      "[2,    90] loss: 1.6431158781051636\n",
      "[2,   100] loss: 1.5397169589996338\n",
      "[2,   110] loss: 1.8257769346237183\n",
      "[2,   120] loss: 1.811282753944397\n",
      "[2,   130] loss: 1.4354114532470703\n",
      "[2,   140] loss: 1.5683701038360596\n",
      "[2,   150] loss: 1.4498443603515625\n",
      "[2,   160] loss: 1.0454121828079224\n",
      "[2,   170] loss: 1.681294322013855\n",
      "[2,   180] loss: 1.1899747848510742\n",
      "[2,   190] loss: 1.586668848991394\n",
      "[2,   200] loss: 1.3776260614395142\n",
      "[2,   210] loss: 2.027017593383789\n",
      "[2,   220] loss: 0.997702956199646\n",
      "[2,   230] loss: 1.718123435974121\n",
      "[2,   240] loss: 2.241852283477783\n",
      "[2,   250] loss: 1.4466795921325684\n",
      "[2,   260] loss: 2.4880874156951904\n",
      "[2,   270] loss: 2.733504295349121\n",
      "[2,   280] loss: 2.1833114624023438\n",
      "[2,   290] loss: 2.2703773975372314\n",
      "Got 1025 / 2315 with accuracy: 44.27645788336933%\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "Training in progress\n",
      "[3,    10] loss: 1.7676713466644287\n",
      "[3,    20] loss: 1.7867071628570557\n",
      "[3,    30] loss: 1.5400071144104004\n",
      "[3,    40] loss: 0.9498172998428345\n",
      "[3,    50] loss: 1.7869065999984741\n",
      "[3,    60] loss: 2.585508108139038\n",
      "[3,    70] loss: 1.4518064260482788\n",
      "[3,    80] loss: 1.5536680221557617\n",
      "[3,    90] loss: 1.2983826398849487\n",
      "[3,   100] loss: 1.7191905975341797\n",
      "[3,   110] loss: 1.516391396522522\n",
      "[3,   120] loss: 1.3422919511795044\n",
      "[3,   130] loss: 1.6225247383117676\n",
      "[3,   140] loss: 2.2377889156341553\n",
      "[3,   150] loss: 2.4118854999542236\n",
      "[3,   160] loss: 1.4114618301391602\n",
      "[3,   170] loss: 1.9855921268463135\n",
      "[3,   180] loss: 1.1441676616668701\n",
      "[3,   190] loss: 1.2709271907806396\n",
      "[3,   200] loss: 2.1890101432800293\n",
      "[3,   210] loss: 1.3452513217926025\n",
      "[3,   220] loss: 1.8055334091186523\n",
      "[3,   230] loss: 0.8197143077850342\n",
      "[3,   240] loss: 1.5035195350646973\n",
      "[3,   250] loss: 1.0042316913604736\n",
      "[3,   260] loss: 1.2453049421310425\n",
      "[3,   270] loss: 1.8944121599197388\n",
      "[3,   280] loss: 1.2955061197280884\n",
      "[3,   290] loss: 1.665393352508545\n",
      "[3,   300] loss: 1.1547112464904785\n",
      "[3,   310] loss: 1.7591482400894165\n",
      "[3,   320] loss: 1.638912558555603\n",
      "[3,   330] loss: 1.9926784038543701\n",
      "[3,   340] loss: 1.4922574758529663\n",
      "[3,   350] loss: 1.456695556640625\n",
      "[3,   360] loss: 1.4769498109817505\n",
      "[3,   370] loss: 1.2415887117385864\n",
      "[3,   380] loss: 1.2228705883026123\n",
      "[3,   390] loss: 1.0667297840118408\n",
      "[3,   400] loss: 1.4326953887939453\n",
      "[3,   410] loss: 0.8746182322502136\n",
      "[3,   420] loss: 1.9254441261291504\n",
      "[3,   430] loss: 1.0505017042160034\n",
      "[3,   440] loss: 1.5266579389572144\n",
      "[3,   450] loss: 1.8555725812911987\n",
      "[3,   460] loss: 1.4607253074645996\n",
      "[3,   470] loss: 1.0008089542388916\n",
      "[3,   480] loss: 2.1779346466064453\n",
      "[3,   490] loss: 0.914825975894928\n",
      "[3,   500] loss: 1.7033016681671143\n",
      "[3,   510] loss: 1.5424364805221558\n",
      "[3,   520] loss: 1.7725765705108643\n",
      "[3,   530] loss: 1.8347530364990234\n",
      "[3,   540] loss: 1.5156179666519165\n",
      "[3,   550] loss: 2.4803895950317383\n",
      "[3,   560] loss: 1.4433133602142334\n",
      "[3,   570] loss: 1.427107334136963\n",
      "[3,   580] loss: 2.273301839828491\n",
      "[3,   590] loss: 0.9444395899772644\n",
      "[3,   600] loss: 1.3099398612976074\n",
      "[3,   610] loss: 1.96638822555542\n",
      "[3,   620] loss: 1.7476398944854736\n",
      "[3,   630] loss: 1.3051526546478271\n",
      "[3,   640] loss: 1.325627088546753\n",
      "[3,   650] loss: 1.5980753898620605\n",
      "[3,   660] loss: 1.3078958988189697\n",
      "[3,   670] loss: 1.5088982582092285\n",
      "[3,   680] loss: 1.289901852607727\n",
      "[3,   690] loss: 1.08388352394104\n",
      "[3,   700] loss: 1.114730715751648\n",
      "[3,   710] loss: 0.9299138784408569\n",
      "[3,   720] loss: 0.5762879252433777\n",
      "[3,   730] loss: 1.3379026651382446\n",
      "[3,   740] loss: 0.8025317788124084\n",
      "[3,   750] loss: 1.446562647819519\n",
      "[3,   760] loss: 1.2766263484954834\n",
      "[3,   770] loss: 2.091374397277832\n",
      "[3,   780] loss: 1.7892951965332031\n",
      "[3,   790] loss: 1.585221767425537\n",
      "[3,   800] loss: 1.8553717136383057\n",
      "[3,   810] loss: 1.1597222089767456\n",
      "[3,   820] loss: 1.1222912073135376\n",
      "[3,   830] loss: 1.6931135654449463\n",
      "[3,   840] loss: 1.1055845022201538\n",
      "[3,   850] loss: 1.5229154825210571\n",
      "[3,   860] loss: 1.4107897281646729\n",
      "[3,   870] loss: 1.701727032661438\n",
      "[3,   880] loss: 1.2579936981201172\n",
      "[3,   890] loss: 1.8780224323272705\n",
      "[3,   900] loss: 0.6363605856895447\n",
      "[3,   910] loss: 1.6210092306137085\n",
      "[3,   920] loss: 1.1123777627944946\n",
      "[3,   930] loss: 1.5646239519119263\n",
      "[3,   940] loss: 1.7264487743377686\n",
      "[3,   950] loss: 0.710239589214325\n",
      "[3,   960] loss: 1.9633853435516357\n",
      "[3,   970] loss: 1.3677667379379272\n",
      "[3,   980] loss: 1.8783594369888306\n",
      "[3,   990] loss: 0.6778256893157959\n",
      "[3,  1000] loss: 1.5820006132125854\n",
      "[3,  1010] loss: 1.7768702507019043\n",
      "Got 4073 / 8108 with accuracy: 50.23433645781944%\n",
      "Validating in progress\n",
      "[3,    10] loss: 1.8074710369110107\n",
      "[3,    20] loss: 1.4710476398468018\n",
      "[3,    30] loss: 1.0661046504974365\n",
      "[3,    40] loss: 2.070013999938965\n",
      "[3,    50] loss: 2.1013283729553223\n",
      "[3,    60] loss: 2.3018577098846436\n",
      "[3,    70] loss: 2.329202175140381\n",
      "[3,    80] loss: 2.8156566619873047\n",
      "[3,    90] loss: 1.3961448669433594\n",
      "[3,   100] loss: 1.849027156829834\n",
      "[3,   110] loss: 2.0786993503570557\n",
      "[3,   120] loss: 1.687343716621399\n",
      "[3,   130] loss: 1.0137430429458618\n",
      "[3,   140] loss: 1.8378640413284302\n",
      "[3,   150] loss: 0.6505308151245117\n",
      "[3,   160] loss: 1.5084649324417114\n",
      "[3,   170] loss: 0.7577735781669617\n",
      "[3,   180] loss: 1.2692821025848389\n",
      "[3,   190] loss: 1.8462201356887817\n",
      "[3,   200] loss: 1.394153356552124\n",
      "[3,   210] loss: 1.3202826976776123\n",
      "[3,   220] loss: 1.6293545961380005\n",
      "[3,   230] loss: 1.1735103130340576\n",
      "[3,   240] loss: 1.5943032503128052\n",
      "[3,   250] loss: 1.60951566696167\n",
      "[3,   260] loss: 2.733236312866211\n",
      "[3,   270] loss: 1.9441564083099365\n",
      "[3,   280] loss: 0.8060155510902405\n",
      "[3,   290] loss: 1.3978170156478882\n",
      "Got 1101 / 2315 with accuracy: 47.559395248380135%\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "Training in progress\n",
      "[4,    10] loss: 1.100958228111267\n",
      "[4,    20] loss: 0.7287493348121643\n",
      "[4,    30] loss: 1.8168094158172607\n",
      "[4,    40] loss: 1.4518282413482666\n",
      "[4,    50] loss: 1.7306947708129883\n",
      "[4,    60] loss: 1.0905977487564087\n",
      "[4,    70] loss: 2.3277807235717773\n",
      "[4,    80] loss: 2.3319733142852783\n",
      "[4,    90] loss: 1.0884760618209839\n",
      "[4,   100] loss: 1.137878179550171\n",
      "[4,   110] loss: 0.6712622046470642\n",
      "[4,   120] loss: 0.894133985042572\n",
      "[4,   130] loss: 1.7375109195709229\n",
      "[4,   140] loss: 1.393984079360962\n",
      "[4,   150] loss: 1.160776138305664\n",
      "[4,   160] loss: 1.4386552572250366\n",
      "[4,   170] loss: 2.1649694442749023\n",
      "[4,   180] loss: 0.9584482312202454\n",
      "[4,   190] loss: 1.2607225179672241\n",
      "[4,   200] loss: 1.0169554948806763\n",
      "[4,   210] loss: 1.0749833583831787\n",
      "[4,   220] loss: 1.4809597730636597\n",
      "[4,   230] loss: 1.5691955089569092\n",
      "[4,   240] loss: 0.9576826095581055\n",
      "[4,   250] loss: 1.6150346994400024\n",
      "[4,   260] loss: 1.501397728919983\n",
      "[4,   270] loss: 1.9353156089782715\n",
      "[4,   280] loss: 1.812398910522461\n",
      "[4,   290] loss: 1.0345066785812378\n",
      "[4,   300] loss: 1.5885002613067627\n",
      "[4,   310] loss: 0.9297617673873901\n",
      "[4,   320] loss: 1.2824232578277588\n",
      "[4,   330] loss: 1.4420477151870728\n",
      "[4,   340] loss: 1.2507458925247192\n",
      "[4,   350] loss: 1.8596833944320679\n",
      "[4,   360] loss: 1.2006915807724\n",
      "[4,   370] loss: 1.3651466369628906\n",
      "[4,   380] loss: 1.276933193206787\n",
      "[4,   390] loss: 1.2749444246292114\n",
      "[4,   400] loss: 1.5814902782440186\n",
      "[4,   410] loss: 1.3219258785247803\n",
      "[4,   420] loss: 1.2643308639526367\n",
      "[4,   430] loss: 0.8333151340484619\n",
      "[4,   440] loss: 1.8101571798324585\n",
      "[4,   450] loss: 0.9694327116012573\n",
      "[4,   460] loss: 1.9305496215820312\n",
      "[4,   470] loss: 1.569319725036621\n",
      "[4,   480] loss: 1.2430301904678345\n",
      "[4,   490] loss: 1.0709011554718018\n",
      "[4,   500] loss: 1.694846510887146\n",
      "[4,   510] loss: 0.8632016777992249\n",
      "[4,   520] loss: 1.5908926725387573\n",
      "[4,   530] loss: 1.6004478931427002\n",
      "[4,   540] loss: 1.650855302810669\n",
      "[4,   550] loss: 0.9398012757301331\n",
      "[4,   560] loss: 0.7631232738494873\n",
      "[4,   570] loss: 1.2938796281814575\n",
      "[4,   580] loss: 0.8742231130599976\n",
      "[4,   590] loss: 0.7178241610527039\n",
      "[4,   600] loss: 0.914038360118866\n",
      "[4,   610] loss: 2.231860637664795\n",
      "[4,   620] loss: 0.6406850814819336\n",
      "[4,   630] loss: 1.2113196849822998\n",
      "[4,   640] loss: 2.151176929473877\n",
      "[4,   650] loss: 1.1381553411483765\n",
      "[4,   660] loss: 1.806160807609558\n",
      "[4,   670] loss: 1.0997155904769897\n",
      "[4,   680] loss: 0.9670082926750183\n",
      "[4,   690] loss: 1.0753284692764282\n",
      "[4,   700] loss: 1.2757031917572021\n",
      "[4,   710] loss: 1.4372525215148926\n",
      "[4,   720] loss: 1.4630722999572754\n",
      "[4,   730] loss: 1.109031319618225\n",
      "[4,   740] loss: 1.3842582702636719\n",
      "[4,   750] loss: 0.9457268714904785\n",
      "[4,   760] loss: 1.8810184001922607\n",
      "[4,   770] loss: 1.3108792304992676\n",
      "[4,   780] loss: 1.1274998188018799\n",
      "[4,   790] loss: 1.7026355266571045\n",
      "[4,   800] loss: 0.8244284987449646\n",
      "[4,   810] loss: 1.3299338817596436\n",
      "[4,   820] loss: 0.7066710591316223\n",
      "[4,   830] loss: 1.4137473106384277\n",
      "[4,   840] loss: 0.9107751250267029\n",
      "[4,   850] loss: 0.5194469094276428\n",
      "[4,   860] loss: 0.5963519215583801\n",
      "[4,   870] loss: 1.950433373451233\n",
      "[4,   880] loss: 0.9425685405731201\n",
      "[4,   890] loss: 1.4269413948059082\n",
      "[4,   900] loss: 0.5941709280014038\n",
      "[4,   910] loss: 0.6941361427307129\n",
      "[4,   920] loss: 1.7680796384811401\n",
      "[4,   930] loss: 1.4698216915130615\n",
      "[4,   940] loss: 1.6953952312469482\n",
      "[4,   950] loss: 1.2525290250778198\n",
      "[4,   960] loss: 1.4533272981643677\n",
      "[4,   970] loss: 0.6917020082473755\n",
      "[4,   980] loss: 1.2787011861801147\n",
      "[4,   990] loss: 1.3226622343063354\n",
      "[4,  1000] loss: 1.2680108547210693\n",
      "[4,  1010] loss: 2.0697062015533447\n",
      "Got 4678 / 8108 with accuracy: 57.69610261470153%\n",
      "Validating in progress\n",
      "[4,    10] loss: 1.4584431648254395\n",
      "[4,    20] loss: 2.286121129989624\n",
      "[4,    30] loss: 0.9333971738815308\n",
      "[4,    40] loss: 1.680088996887207\n",
      "[4,    50] loss: 0.8505971431732178\n",
      "[4,    60] loss: 1.5544564723968506\n",
      "[4,    70] loss: 0.9851833581924438\n",
      "[4,    80] loss: 2.087722063064575\n",
      "[4,    90] loss: 1.4459244012832642\n",
      "[4,   100] loss: 1.6407830715179443\n",
      "[4,   110] loss: 1.6358362436294556\n",
      "[4,   120] loss: 2.1235153675079346\n",
      "[4,   130] loss: 1.9589786529541016\n",
      "[4,   140] loss: 1.386088490486145\n",
      "[4,   150] loss: 1.48210608959198\n",
      "[4,   160] loss: 1.4925305843353271\n",
      "[4,   170] loss: 1.5587146282196045\n",
      "[4,   180] loss: 1.3472384214401245\n",
      "[4,   190] loss: 1.1198004484176636\n",
      "[4,   200] loss: 1.1040825843811035\n",
      "[4,   210] loss: 1.622194766998291\n",
      "[4,   220] loss: 1.1156314611434937\n",
      "[4,   230] loss: 0.9669747352600098\n",
      "[4,   240] loss: 1.508316993713379\n",
      "[4,   250] loss: 2.125833511352539\n",
      "[4,   260] loss: 0.8734430074691772\n",
      "[4,   270] loss: 1.7992888689041138\n",
      "[4,   280] loss: 1.681672215461731\n",
      "[4,   290] loss: 1.6359093189239502\n",
      "Got 1204 / 2315 with accuracy: 52.00863930885529%\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "Training in progress\n",
      "[5,    10] loss: 1.6086758375167847\n",
      "[5,    20] loss: 1.021870493888855\n",
      "[5,    30] loss: 0.5019788146018982\n",
      "[5,    40] loss: 1.8404746055603027\n",
      "[5,    50] loss: 1.3933079242706299\n",
      "[5,    60] loss: 0.8245786428451538\n",
      "[5,    70] loss: 0.9785125255584717\n",
      "[5,    80] loss: 0.6586264967918396\n",
      "[5,    90] loss: 0.6551358103752136\n",
      "[5,   100] loss: 1.0270920991897583\n",
      "[5,   110] loss: 1.3204736709594727\n",
      "[5,   120] loss: 1.1598397493362427\n",
      "[5,   130] loss: 0.923183262348175\n",
      "[5,   140] loss: 0.6073356866836548\n",
      "[5,   150] loss: 0.8675881028175354\n",
      "[5,   160] loss: 1.708488941192627\n",
      "[5,   170] loss: 0.7507755160331726\n",
      "[5,   180] loss: 1.0842846632003784\n",
      "[5,   190] loss: 1.1966874599456787\n",
      "[5,   200] loss: 1.868257761001587\n",
      "[5,   210] loss: 1.3412353992462158\n",
      "[5,   220] loss: 1.8647332191467285\n",
      "[5,   230] loss: 0.7710480093955994\n",
      "[5,   240] loss: 1.3056061267852783\n",
      "[5,   250] loss: 1.1797511577606201\n",
      "[5,   260] loss: 0.5758265256881714\n",
      "[5,   270] loss: 0.7456014156341553\n",
      "[5,   280] loss: 0.5054957866668701\n",
      "[5,   290] loss: 0.7241251468658447\n",
      "[5,   300] loss: 0.40558499097824097\n",
      "[5,   310] loss: 0.864983856678009\n",
      "[5,   320] loss: 1.031068205833435\n",
      "[5,   330] loss: 1.0063164234161377\n",
      "[5,   340] loss: 0.8329911828041077\n",
      "[5,   350] loss: 0.7750704884529114\n",
      "[5,   360] loss: 0.5734893083572388\n",
      "[5,   370] loss: 1.1062207221984863\n",
      "[5,   380] loss: 1.4255629777908325\n",
      "[5,   390] loss: 1.1824783086776733\n",
      "[5,   400] loss: 1.3999111652374268\n",
      "[5,   410] loss: 1.0528854131698608\n",
      "[5,   420] loss: 0.9764012098312378\n",
      "[5,   430] loss: 1.6677603721618652\n",
      "[5,   440] loss: 0.6514059901237488\n",
      "[5,   450] loss: 1.7265211343765259\n",
      "[5,   460] loss: 1.4402741193771362\n",
      "[5,   470] loss: 0.5226699113845825\n",
      "[5,   480] loss: 1.848215103149414\n",
      "[5,   490] loss: 0.4165565073490143\n",
      "[5,   500] loss: 1.9966686964035034\n",
      "[5,   510] loss: 0.3449678122997284\n",
      "[5,   520] loss: 1.3162542581558228\n",
      "[5,   530] loss: 0.9639576077461243\n",
      "[5,   540] loss: 0.9590022563934326\n",
      "[5,   550] loss: 0.9264930486679077\n",
      "[5,   560] loss: 0.795158326625824\n",
      "[5,   570] loss: 0.69879150390625\n",
      "[5,   580] loss: 1.3309130668640137\n",
      "[5,   590] loss: 0.8103939294815063\n",
      "[5,   600] loss: 1.8359074592590332\n",
      "[5,   610] loss: 2.0883753299713135\n",
      "[5,   620] loss: 0.5918092131614685\n",
      "[5,   630] loss: 0.6855499148368835\n",
      "[5,   640] loss: 0.9754104614257812\n",
      "[5,   650] loss: 1.022603154182434\n",
      "[5,   660] loss: 1.0403773784637451\n",
      "[5,   670] loss: 1.5559331178665161\n",
      "[5,   680] loss: 0.39588743448257446\n",
      "[5,   690] loss: 1.446650505065918\n",
      "[5,   700] loss: 0.5979520678520203\n",
      "[5,   710] loss: 0.9080394506454468\n",
      "[5,   720] loss: 0.8651725649833679\n",
      "[5,   730] loss: 1.4489675760269165\n",
      "[5,   740] loss: 1.075774073600769\n",
      "[5,   750] loss: 0.992218554019928\n",
      "[5,   760] loss: 0.8308519124984741\n",
      "[5,   770] loss: 1.6754145622253418\n",
      "[5,   780] loss: 1.1319046020507812\n",
      "[5,   790] loss: 0.8055030107498169\n",
      "[5,   800] loss: 0.9644570350646973\n",
      "[5,   810] loss: 0.8364428877830505\n",
      "[5,   820] loss: 0.8764970302581787\n",
      "[5,   830] loss: 0.8725724816322327\n",
      "[5,   840] loss: 1.0417530536651611\n",
      "[5,   850] loss: 2.0042312145233154\n",
      "[5,   860] loss: 1.9317697286605835\n",
      "[5,   870] loss: 1.2167165279388428\n",
      "[5,   880] loss: 0.9784411191940308\n",
      "[5,   890] loss: 0.8698381185531616\n",
      "[5,   900] loss: 1.3759756088256836\n",
      "[5,   910] loss: 1.0184364318847656\n",
      "[5,   920] loss: 1.3517463207244873\n",
      "[5,   930] loss: 0.9680054187774658\n",
      "[5,   940] loss: 0.8073402047157288\n",
      "[5,   950] loss: 1.0949699878692627\n",
      "[5,   960] loss: 1.0322301387786865\n",
      "[5,   970] loss: 0.4752146303653717\n",
      "[5,   980] loss: 0.9880921840667725\n",
      "[5,   990] loss: 0.36643728613853455\n",
      "[5,  1000] loss: 0.6227285861968994\n",
      "[5,  1010] loss: 0.8259705305099487\n",
      "Got 5128 / 8108 with accuracy: 63.24617661568821%\n",
      "Validating in progress\n",
      "[5,    10] loss: 1.0951621532440186\n",
      "[5,    20] loss: 2.076152801513672\n",
      "[5,    30] loss: 1.4207254648208618\n",
      "[5,    40] loss: 1.077081322669983\n",
      "[5,    50] loss: 1.3569070100784302\n",
      "[5,    60] loss: 1.063302993774414\n",
      "[5,    70] loss: 0.9087865352630615\n",
      "[5,    80] loss: 1.823030710220337\n",
      "[5,    90] loss: 1.5722533464431763\n",
      "[5,   100] loss: 0.8982532024383545\n",
      "[5,   110] loss: 0.8629708886146545\n",
      "[5,   120] loss: 1.230359435081482\n",
      "[5,   130] loss: 0.4045869708061218\n",
      "[5,   140] loss: 1.6473147869110107\n",
      "[5,   150] loss: 1.7454752922058105\n",
      "[5,   160] loss: 1.3787027597427368\n",
      "[5,   170] loss: 2.2362759113311768\n",
      "[5,   180] loss: 1.0761053562164307\n",
      "[5,   190] loss: 2.0623557567596436\n",
      "[5,   200] loss: 1.20942223072052\n",
      "[5,   210] loss: 1.7295057773590088\n",
      "[5,   220] loss: 1.1618235111236572\n",
      "[5,   230] loss: 1.1574513912200928\n",
      "[5,   240] loss: 1.6445682048797607\n",
      "[5,   250] loss: 2.0098681449890137\n",
      "[5,   260] loss: 1.6259732246398926\n",
      "[5,   270] loss: 0.932400107383728\n",
      "[5,   280] loss: 1.8775951862335205\n",
      "[5,   290] loss: 2.166891098022461\n",
      "Got 1271 / 2315 with accuracy: 54.90280777537797%\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "Training in progress\n",
      "[6,    10] loss: 0.8017216324806213\n",
      "[6,    20] loss: 1.1396152973175049\n",
      "[6,    30] loss: 0.7806078195571899\n",
      "[6,    40] loss: 0.5004051923751831\n",
      "[6,    50] loss: 1.2668200731277466\n",
      "[6,    60] loss: 1.3577247858047485\n",
      "[6,    70] loss: 0.5787088871002197\n",
      "[6,    80] loss: 0.31942614912986755\n",
      "[6,    90] loss: 0.7831811308860779\n",
      "[6,   100] loss: 1.3660293817520142\n",
      "[6,   110] loss: 0.8061124086380005\n",
      "[6,   120] loss: 0.6906996965408325\n",
      "[6,   130] loss: 0.6340572834014893\n",
      "[6,   140] loss: 1.1217714548110962\n",
      "[6,   150] loss: 0.5494863986968994\n",
      "[6,   160] loss: 0.7295596599578857\n",
      "[6,   170] loss: 0.35863906145095825\n",
      "[6,   180] loss: 0.6455321311950684\n",
      "[6,   190] loss: 1.1333881616592407\n",
      "[6,   200] loss: 0.5131247639656067\n",
      "[6,   210] loss: 1.2444958686828613\n",
      "[6,   220] loss: 1.6016474962234497\n",
      "[6,   230] loss: 0.39900684356689453\n",
      "[6,   240] loss: 1.126190185546875\n",
      "[6,   250] loss: 0.48784855008125305\n",
      "[6,   260] loss: 0.8629586696624756\n",
      "[6,   270] loss: 0.9444931745529175\n",
      "[6,   280] loss: 0.8588881492614746\n",
      "[6,   290] loss: 1.6611225605010986\n",
      "[6,   300] loss: 0.8485429883003235\n",
      "[6,   310] loss: 0.6166454553604126\n",
      "[6,   320] loss: 0.6696903109550476\n",
      "[6,   330] loss: 1.9840874671936035\n",
      "[6,   340] loss: 0.7219858169555664\n",
      "[6,   350] loss: 0.1938912719488144\n",
      "[6,   360] loss: 0.8480479717254639\n",
      "[6,   370] loss: 0.8508924841880798\n",
      "[6,   380] loss: 1.0838072299957275\n",
      "[6,   390] loss: 1.475558876991272\n",
      "[6,   400] loss: 0.9860018491744995\n",
      "[6,   410] loss: 1.4816046953201294\n",
      "[6,   420] loss: 1.093794822692871\n",
      "[6,   430] loss: 1.2244514226913452\n",
      "[6,   440] loss: 0.5885215997695923\n",
      "[6,   450] loss: 0.7281290292739868\n",
      "[6,   460] loss: 0.6458268165588379\n",
      "[6,   470] loss: 1.1604697704315186\n",
      "[6,   480] loss: 1.1214431524276733\n",
      "[6,   490] loss: 1.4515879154205322\n",
      "[6,   500] loss: 0.34105393290519714\n",
      "[6,   510] loss: 1.7297403812408447\n",
      "[6,   520] loss: 0.8385064005851746\n",
      "[6,   530] loss: 0.44401848316192627\n",
      "[6,   540] loss: 1.1288235187530518\n",
      "[6,   550] loss: 0.7931411862373352\n",
      "[6,   560] loss: 0.37624791264533997\n",
      "[6,   570] loss: 0.38501620292663574\n",
      "[6,   580] loss: 0.9359363317489624\n",
      "[6,   590] loss: 0.9516327977180481\n",
      "[6,   600] loss: 1.30551016330719\n",
      "[6,   610] loss: 0.7272137403488159\n",
      "[6,   620] loss: 1.2874772548675537\n",
      "[6,   630] loss: 1.0540406703948975\n",
      "[6,   640] loss: 1.2271264791488647\n",
      "[6,   650] loss: 0.9053843021392822\n",
      "[6,   660] loss: 1.3065240383148193\n",
      "[6,   670] loss: 0.6823917627334595\n",
      "[6,   680] loss: 1.3529248237609863\n",
      "[6,   690] loss: 0.8421747088432312\n",
      "[6,   700] loss: 0.6448850631713867\n",
      "[6,   710] loss: 0.8832381963729858\n",
      "[6,   720] loss: 2.2175378799438477\n",
      "[6,   730] loss: 1.0611517429351807\n",
      "[6,   740] loss: 1.2823376655578613\n",
      "[6,   750] loss: 0.9832310676574707\n",
      "[6,   760] loss: 1.242316722869873\n",
      "[6,   770] loss: 0.9281643629074097\n",
      "[6,   780] loss: 1.4615075588226318\n",
      "[6,   790] loss: 0.9116021394729614\n",
      "[6,   800] loss: 1.1082597970962524\n",
      "[6,   810] loss: 1.5182620286941528\n",
      "[6,   820] loss: 1.0671133995056152\n",
      "[6,   830] loss: 0.4646345376968384\n",
      "[6,   840] loss: 1.4044376611709595\n",
      "[6,   850] loss: 0.9454820156097412\n",
      "[6,   860] loss: 0.6608873605728149\n",
      "[6,   870] loss: 1.7464029788970947\n",
      "[6,   880] loss: 1.1669700145721436\n",
      "[6,   890] loss: 0.6919444799423218\n",
      "[6,   900] loss: 1.1564829349517822\n",
      "[6,   910] loss: 0.7285915613174438\n",
      "[6,   920] loss: 1.1768404245376587\n",
      "[6,   930] loss: 0.4570564329624176\n",
      "[6,   940] loss: 0.8040015697479248\n",
      "[6,   950] loss: 0.8943412899971008\n",
      "[6,   960] loss: 0.8099966049194336\n",
      "[6,   970] loss: 1.464479923248291\n",
      "[6,   980] loss: 0.7778717279434204\n",
      "[6,   990] loss: 1.7325547933578491\n",
      "[6,  1000] loss: 0.7562371492385864\n",
      "[6,  1010] loss: 0.8796103596687317\n",
      "Got 5627 / 8108 with accuracy: 69.40059200789344%\n",
      "Validating in progress\n",
      "[6,    10] loss: 1.5343427658081055\n",
      "[6,    20] loss: 1.7827816009521484\n",
      "[6,    30] loss: 1.9034810066223145\n",
      "[6,    40] loss: 0.9169279932975769\n",
      "[6,    50] loss: 0.8528550863265991\n",
      "[6,    60] loss: 1.166184902191162\n",
      "[6,    70] loss: 1.6572902202606201\n",
      "[6,    80] loss: 0.5058329701423645\n",
      "[6,    90] loss: 0.8223809599876404\n",
      "[6,   100] loss: 1.7123608589172363\n",
      "[6,   110] loss: 0.2781955301761627\n",
      "[6,   120] loss: 1.2138502597808838\n",
      "[6,   130] loss: 1.1618231534957886\n",
      "[6,   140] loss: 0.748633086681366\n",
      "[6,   150] loss: 2.524399518966675\n",
      "[6,   160] loss: 1.7372783422470093\n",
      "[6,   170] loss: 1.2040618658065796\n",
      "[6,   180] loss: 1.3488479852676392\n",
      "[6,   190] loss: 1.7402961254119873\n",
      "[6,   200] loss: 1.5012619495391846\n",
      "[6,   210] loss: 1.1996368169784546\n",
      "[6,   220] loss: 0.9746844172477722\n",
      "[6,   230] loss: 1.295933723449707\n",
      "[6,   240] loss: 1.3721506595611572\n",
      "[6,   250] loss: 0.7021110653877258\n",
      "[6,   260] loss: 0.41279837489128113\n",
      "[6,   270] loss: 1.5438053607940674\n",
      "[6,   280] loss: 0.6814386248588562\n",
      "[6,   290] loss: 1.1259361505508423\n",
      "Got 1377 / 2315 with accuracy: 59.4816414686825%\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "Training in progress\n",
      "[7,    10] loss: 0.7921059131622314\n",
      "[7,    20] loss: 0.9127733111381531\n",
      "[7,    30] loss: 1.276869535446167\n",
      "[7,    40] loss: 1.4783062934875488\n",
      "[7,    50] loss: 0.6036436557769775\n",
      "[7,    60] loss: 0.5631190538406372\n",
      "[7,    70] loss: 0.3118739426136017\n",
      "[7,    80] loss: 0.8549952507019043\n",
      "[7,    90] loss: 0.3230209946632385\n",
      "[7,   100] loss: 1.2851393222808838\n",
      "[7,   110] loss: 0.36374521255493164\n",
      "[7,   120] loss: 0.4034368097782135\n",
      "[7,   130] loss: 0.29217690229415894\n",
      "[7,   140] loss: 0.8409457802772522\n",
      "[7,   150] loss: 0.7080017924308777\n",
      "[7,   160] loss: 0.5687363743782043\n",
      "[7,   170] loss: 1.422925591468811\n",
      "[7,   180] loss: 1.0787466764450073\n",
      "[7,   190] loss: 0.4983814060688019\n",
      "[7,   200] loss: 0.5736563801765442\n",
      "[7,   210] loss: 0.8300016522407532\n",
      "[7,   220] loss: 1.32484769821167\n",
      "[7,   230] loss: 0.5953301787376404\n",
      "[7,   240] loss: 0.41497719287872314\n",
      "[7,   250] loss: 0.5521169900894165\n",
      "[7,   260] loss: 0.19641946256160736\n",
      "[7,   270] loss: 1.5160329341888428\n",
      "[7,   280] loss: 1.0029932260513306\n",
      "[7,   290] loss: 0.808164656162262\n",
      "[7,   300] loss: 1.3635684251785278\n",
      "[7,   310] loss: 1.669290542602539\n",
      "[7,   320] loss: 0.8767956495285034\n",
      "[7,   330] loss: 0.7722113132476807\n",
      "[7,   340] loss: 1.453747272491455\n",
      "[7,   350] loss: 0.9031938314437866\n",
      "[7,   360] loss: 0.5124491453170776\n",
      "[7,   370] loss: 0.5879994034767151\n",
      "[7,   380] loss: 1.0127007961273193\n",
      "[7,   390] loss: 0.4780391752719879\n",
      "[7,   400] loss: 0.8560712933540344\n",
      "[7,   410] loss: 0.6862714290618896\n",
      "[7,   420] loss: 0.8213146924972534\n",
      "[7,   430] loss: 1.4761626720428467\n",
      "[7,   440] loss: 0.36002030968666077\n",
      "[7,   450] loss: 0.7136334180831909\n",
      "[7,   460] loss: 1.3806304931640625\n",
      "[7,   470] loss: 2.1207685470581055\n",
      "[7,   480] loss: 1.6481459140777588\n",
      "[7,   490] loss: 0.7103857398033142\n",
      "[7,   500] loss: 0.3245357871055603\n",
      "[7,   510] loss: 0.6894983649253845\n",
      "[7,   520] loss: 0.874466598033905\n",
      "[7,   530] loss: 1.2418140172958374\n",
      "[7,   540] loss: 0.615253210067749\n",
      "[7,   550] loss: 0.737401008605957\n",
      "[7,   560] loss: 0.6454282402992249\n",
      "[7,   570] loss: 0.8502401113510132\n",
      "[7,   580] loss: 0.5560866594314575\n",
      "[7,   590] loss: 1.2794476747512817\n",
      "[7,   600] loss: 0.7319715619087219\n",
      "[7,   610] loss: 1.0352613925933838\n",
      "[7,   620] loss: 2.0660674571990967\n",
      "[7,   630] loss: 1.562145709991455\n",
      "[7,   640] loss: 0.9403873682022095\n",
      "[7,   650] loss: 1.7667864561080933\n",
      "[7,   660] loss: 0.2519606351852417\n",
      "[7,   670] loss: 0.925457239151001\n",
      "[7,   680] loss: 0.9228096008300781\n",
      "[7,   690] loss: 0.45588165521621704\n",
      "[7,   700] loss: 1.1593880653381348\n",
      "[7,   710] loss: 0.9020755887031555\n",
      "[7,   720] loss: 0.8115241527557373\n",
      "[7,   730] loss: 0.22309842705726624\n",
      "[7,   740] loss: 0.9557822942733765\n",
      "[7,   750] loss: 0.7176215648651123\n",
      "[7,   760] loss: 0.8158888816833496\n",
      "[7,   770] loss: 0.4347280263900757\n",
      "[7,   780] loss: 0.26462554931640625\n",
      "[7,   790] loss: 1.044738531112671\n",
      "[7,   800] loss: 0.9858281016349792\n",
      "[7,   810] loss: 0.8506464958190918\n",
      "[7,   820] loss: 1.4388729333877563\n",
      "[7,   830] loss: 0.38209593296051025\n",
      "[7,   840] loss: 0.9185711145401001\n",
      "[7,   850] loss: 1.095984697341919\n",
      "[7,   860] loss: 1.0329731702804565\n",
      "[7,   870] loss: 0.46488866209983826\n",
      "[7,   880] loss: 0.3465556502342224\n",
      "[7,   890] loss: 0.5291427969932556\n",
      "[7,   900] loss: 0.9192584156990051\n",
      "[7,   910] loss: 0.3566986620426178\n",
      "[7,   920] loss: 0.8112706542015076\n",
      "[7,   930] loss: 0.6105474233627319\n",
      "[7,   940] loss: 1.2802801132202148\n",
      "[7,   950] loss: 0.7784076929092407\n",
      "[7,   960] loss: 1.0863890647888184\n",
      "[7,   970] loss: 0.960660457611084\n",
      "[7,   980] loss: 1.5451602935791016\n",
      "[7,   990] loss: 1.4739857912063599\n",
      "[7,  1000] loss: 0.746938943862915\n",
      "[7,  1010] loss: 0.534339189529419\n",
      "Got 5915 / 8108 with accuracy: 72.95263936852491%\n",
      "Validating in progress\n",
      "[7,    10] loss: 0.44823476672172546\n",
      "[7,    20] loss: 0.44207197427749634\n",
      "[7,    30] loss: 0.45857349038124084\n",
      "[7,    40] loss: 0.5377286076545715\n",
      "[7,    50] loss: 0.5096519589424133\n",
      "[7,    60] loss: 1.2874927520751953\n",
      "[7,    70] loss: 0.36775487661361694\n",
      "[7,    80] loss: 0.7380207180976868\n",
      "[7,    90] loss: 0.06457969546318054\n",
      "[7,   100] loss: 0.08935210853815079\n",
      "[7,   110] loss: 0.699237585067749\n",
      "[7,   120] loss: 0.39901378750801086\n",
      "[7,   130] loss: 0.8809480667114258\n",
      "[7,   140] loss: 0.49192720651626587\n",
      "[7,   150] loss: 0.9498780965805054\n",
      "[7,   160] loss: 0.7063467502593994\n",
      "[7,   170] loss: 0.8023462891578674\n",
      "[7,   180] loss: 0.8563331961631775\n",
      "[7,   190] loss: 0.6152653098106384\n",
      "[7,   200] loss: 0.37020495533943176\n",
      "[7,   210] loss: 0.1533704400062561\n",
      "[7,   220] loss: 0.36204132437705994\n",
      "[7,   230] loss: 0.20116928219795227\n",
      "[7,   240] loss: 0.39741936326026917\n",
      "[7,   250] loss: 0.819484531879425\n",
      "[7,   260] loss: 0.35776641964912415\n",
      "[7,   270] loss: 0.13788877427577972\n",
      "[7,   280] loss: 0.5577072501182556\n",
      "[7,   290] loss: 0.5845400094985962\n",
      "Got 1893 / 2315 with accuracy: 81.77105831533478%\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "Training in progress\n",
      "[8,    10] loss: 0.5433512926101685\n",
      "[8,    20] loss: 0.9300759434700012\n",
      "[8,    30] loss: 0.13484303653240204\n",
      "[8,    40] loss: 0.17636914551258087\n",
      "[8,    50] loss: 0.5229584574699402\n",
      "[8,    60] loss: 0.4665552079677582\n",
      "[8,    70] loss: 0.2135932445526123\n",
      "[8,    80] loss: 0.4455682635307312\n",
      "[8,    90] loss: 0.27412283420562744\n",
      "[8,   100] loss: 0.42252063751220703\n",
      "[8,   110] loss: 0.9926812648773193\n",
      "[8,   120] loss: 0.872783362865448\n",
      "[8,   130] loss: 0.12891250848770142\n",
      "[8,   140] loss: 0.8626802563667297\n",
      "[8,   150] loss: 0.1497117578983307\n",
      "[8,   160] loss: 0.18457278609275818\n",
      "[8,   170] loss: 0.20758461952209473\n",
      "[8,   180] loss: 0.19872765243053436\n",
      "[8,   190] loss: 0.3352481722831726\n",
      "[8,   200] loss: 0.6817106008529663\n",
      "[8,   210] loss: 0.36108899116516113\n",
      "[8,   220] loss: 0.9631147384643555\n",
      "[8,   230] loss: 0.40679559111595154\n",
      "[8,   240] loss: 0.23933544754981995\n",
      "[8,   250] loss: 0.6236213445663452\n",
      "[8,   260] loss: 0.40246695280075073\n",
      "[8,   270] loss: 1.404547095298767\n",
      "[8,   280] loss: 0.16698193550109863\n",
      "[8,   290] loss: 0.3291797935962677\n",
      "[8,   300] loss: 0.5582377314567566\n",
      "[8,   310] loss: 0.07358581572771072\n",
      "[8,   320] loss: 0.4446333348751068\n",
      "[8,   330] loss: 0.2938215434551239\n",
      "[8,   340] loss: 0.439056396484375\n",
      "[8,   350] loss: 0.0965052992105484\n",
      "[8,   360] loss: 0.28952348232269287\n",
      "[8,   370] loss: 0.9682056307792664\n",
      "[8,   380] loss: 0.4444817900657654\n",
      "[8,   390] loss: 0.34074345231056213\n",
      "[8,   400] loss: 0.1413407176733017\n",
      "[8,   410] loss: 0.1306566596031189\n",
      "[8,   420] loss: 0.9545097351074219\n",
      "[8,   430] loss: 0.1882428675889969\n",
      "[8,   440] loss: 0.5831215977668762\n",
      "[8,   450] loss: 0.06298090517520905\n",
      "[8,   460] loss: 0.191072016954422\n",
      "[8,   470] loss: 0.38433703780174255\n",
      "[8,   480] loss: 0.45045775175094604\n",
      "[8,   490] loss: 0.48997533321380615\n",
      "[8,   500] loss: 0.4196821451187134\n",
      "[8,   510] loss: 0.3991541564464569\n",
      "[8,   520] loss: 0.6903479099273682\n",
      "[8,   530] loss: 0.5468321442604065\n",
      "[8,   540] loss: 0.4939741790294647\n",
      "[8,   550] loss: 0.48245182633399963\n",
      "[8,   560] loss: 0.6527449488639832\n",
      "[8,   570] loss: 0.08782152831554413\n",
      "[8,   580] loss: 0.41973602771759033\n",
      "[8,   590] loss: 0.12008179724216461\n",
      "[8,   600] loss: 0.3112703859806061\n",
      "[8,   610] loss: 0.2911432981491089\n",
      "[8,   620] loss: 0.4007619619369507\n",
      "[8,   630] loss: 0.11413943767547607\n",
      "[8,   640] loss: 0.9609770774841309\n",
      "[8,   650] loss: 0.4233759343624115\n",
      "[8,   660] loss: 0.08636447042226791\n",
      "[8,   670] loss: 0.18750794231891632\n",
      "[8,   680] loss: 0.580090343952179\n",
      "[8,   690] loss: 0.10697103291749954\n",
      "[8,   700] loss: 0.23845697939395905\n",
      "[8,   710] loss: 1.0248204469680786\n",
      "[8,   720] loss: 0.1511136144399643\n",
      "[8,   730] loss: 0.47469964623451233\n",
      "[8,   740] loss: 0.06884374469518661\n",
      "[8,   750] loss: 0.7891088724136353\n",
      "[8,   760] loss: 0.5634444952011108\n",
      "[8,   770] loss: 0.251069575548172\n",
      "[8,   780] loss: 0.45960962772369385\n",
      "[8,   790] loss: 0.1426873803138733\n",
      "[8,   800] loss: 0.6468348503112793\n",
      "[8,   810] loss: 0.6404717564582825\n",
      "[8,   820] loss: 0.24046683311462402\n",
      "[8,   830] loss: 0.061524104326963425\n",
      "[8,   840] loss: 1.028907299041748\n",
      "[8,   850] loss: 0.24672144651412964\n",
      "[8,   860] loss: 0.6359407901763916\n",
      "[8,   870] loss: 0.10367916524410248\n",
      "[8,   880] loss: 0.8503695726394653\n",
      "[8,   890] loss: 0.6747016906738281\n",
      "[8,   900] loss: 0.21294736862182617\n",
      "[8,   910] loss: 0.5990467071533203\n",
      "[8,   920] loss: 0.8138651847839355\n",
      "[8,   930] loss: 1.0472183227539062\n",
      "[8,   940] loss: 0.36533302068710327\n",
      "[8,   950] loss: 0.123453289270401\n",
      "[8,   960] loss: 0.6068650484085083\n",
      "[8,   970] loss: 0.8971478939056396\n",
      "[8,   980] loss: 0.367986261844635\n",
      "[8,   990] loss: 0.1268501728773117\n",
      "[8,  1000] loss: 0.5613839626312256\n",
      "[8,  1010] loss: 0.1374180167913437\n",
      "Got 6996 / 8108 with accuracy: 86.28515046867291%\n",
      "Validating in progress\n",
      "[8,    10] loss: 0.25472456216812134\n",
      "[8,    20] loss: 0.7250056266784668\n",
      "[8,    30] loss: 0.19520311057567596\n",
      "[8,    40] loss: 0.34798887372016907\n",
      "[8,    50] loss: 0.5607799291610718\n",
      "[8,    60] loss: 0.084522545337677\n",
      "[8,    70] loss: 0.10555051267147064\n",
      "[8,    80] loss: 0.03282652795314789\n",
      "[8,    90] loss: 0.11362522840499878\n",
      "[8,   100] loss: 0.7100239396095276\n",
      "[8,   110] loss: 0.5105962753295898\n",
      "[8,   120] loss: 0.5117714405059814\n",
      "[8,   130] loss: 1.4572168588638306\n",
      "[8,   140] loss: 0.3543359935283661\n",
      "[8,   150] loss: 0.3839619755744934\n",
      "[8,   160] loss: 0.38037145137786865\n",
      "[8,   170] loss: 0.9513307213783264\n",
      "[8,   180] loss: 0.16245530545711517\n",
      "[8,   190] loss: 0.5535773634910583\n",
      "[8,   200] loss: 0.2640092968940735\n",
      "[8,   210] loss: 0.48046043515205383\n",
      "[8,   220] loss: 0.09719384461641312\n",
      "[8,   230] loss: 0.1095944344997406\n",
      "[8,   240] loss: 0.09888073801994324\n",
      "[8,   250] loss: 0.05350927263498306\n",
      "[8,   260] loss: 0.6277843713760376\n",
      "[8,   270] loss: 0.10456299781799316\n",
      "[8,   280] loss: 0.13118280470371246\n",
      "[8,   290] loss: 0.4683074951171875\n",
      "Got 2077 / 2315 with accuracy: 89.71922246220302%\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "Training in progress\n",
      "[9,    10] loss: 0.14244773983955383\n",
      "[9,    20] loss: 0.13652223348617554\n",
      "[9,    30] loss: 0.9123098254203796\n",
      "[9,    40] loss: 0.18838202953338623\n",
      "[9,    50] loss: 0.5172199606895447\n",
      "[9,    60] loss: 0.5826828479766846\n",
      "[9,    70] loss: 0.09626532346010208\n",
      "[9,    80] loss: 0.2764244079589844\n",
      "[9,    90] loss: 0.3171515464782715\n",
      "[9,   100] loss: 0.4272445738315582\n",
      "[9,   110] loss: 0.76771080493927\n",
      "[9,   120] loss: 0.6109607219696045\n",
      "[9,   130] loss: 0.507671594619751\n",
      "[9,   140] loss: 0.41379883885383606\n",
      "[9,   150] loss: 0.1582040935754776\n",
      "[9,   160] loss: 0.10143724083900452\n",
      "[9,   170] loss: 0.4505251348018646\n",
      "[9,   180] loss: 0.1924373358488083\n",
      "[9,   190] loss: 0.14095214009284973\n",
      "[9,   200] loss: 0.4478391706943512\n",
      "[9,   210] loss: 0.6747531294822693\n",
      "[9,   220] loss: 0.1684509515762329\n",
      "[9,   230] loss: 1.3337799310684204\n",
      "[9,   240] loss: 0.09490557760000229\n",
      "[9,   250] loss: 0.5893173217773438\n",
      "[9,   260] loss: 0.9994954466819763\n",
      "[9,   270] loss: 0.4612935483455658\n",
      "[9,   280] loss: 0.5125232934951782\n",
      "[9,   290] loss: 0.19452671706676483\n",
      "[9,   300] loss: 0.04446341469883919\n",
      "[9,   310] loss: 1.0266615152359009\n",
      "[9,   320] loss: 0.07185392826795578\n",
      "[9,   330] loss: 0.7493401765823364\n",
      "[9,   340] loss: 0.6511328220367432\n",
      "[9,   350] loss: 0.12503081560134888\n",
      "[9,   360] loss: 0.4763983190059662\n",
      "[9,   370] loss: 0.2843765914440155\n",
      "[9,   380] loss: 0.12500125169754028\n",
      "[9,   390] loss: 0.14955167472362518\n",
      "[9,   400] loss: 0.17351371049880981\n",
      "[9,   410] loss: 0.17012308537960052\n",
      "[9,   420] loss: 0.32316723465919495\n",
      "[9,   430] loss: 0.3199785053730011\n",
      "[9,   440] loss: 0.2895797789096832\n",
      "[9,   450] loss: 0.9284108877182007\n",
      "[9,   460] loss: 0.49951326847076416\n",
      "[9,   470] loss: 0.08894253522157669\n",
      "[9,   480] loss: 0.42940834164619446\n",
      "[9,   490] loss: 0.0503162182867527\n",
      "[9,   500] loss: 0.04285341128706932\n",
      "[9,   510] loss: 0.18507510423660278\n",
      "[9,   520] loss: 0.19656871259212494\n",
      "[9,   530] loss: 0.6205855011940002\n",
      "[9,   540] loss: 0.42292875051498413\n",
      "[9,   550] loss: 1.181829810142517\n",
      "[9,   560] loss: 0.27666082978248596\n",
      "[9,   570] loss: 0.11564816534519196\n",
      "[9,   580] loss: 0.08071576803922653\n",
      "[9,   590] loss: 0.19243554770946503\n",
      "[9,   600] loss: 0.1362473964691162\n",
      "[9,   610] loss: 0.3894205093383789\n",
      "[9,   620] loss: 1.0385162830352783\n",
      "[9,   630] loss: 0.022635923698544502\n",
      "[9,   640] loss: 0.32268252968788147\n",
      "[9,   650] loss: 0.4851756989955902\n",
      "[9,   660] loss: 0.8279967308044434\n",
      "[9,   670] loss: 0.7201281785964966\n",
      "[9,   680] loss: 0.23498956859111786\n",
      "[9,   690] loss: 0.10309696197509766\n",
      "[9,   700] loss: 0.12216506898403168\n",
      "[9,   710] loss: 0.46531885862350464\n",
      "[9,   720] loss: 0.09005703777074814\n",
      "[9,   730] loss: 0.591433584690094\n",
      "[9,   740] loss: 0.19478124380111694\n",
      "[9,   750] loss: 0.04170019552111626\n",
      "[9,   760] loss: 0.22461578249931335\n",
      "[9,   770] loss: 0.13843375444412231\n",
      "[9,   780] loss: 0.28103387355804443\n",
      "[9,   790] loss: 0.10912393778562546\n",
      "[9,   800] loss: 0.2598876953125\n",
      "[9,   810] loss: 0.07888031005859375\n",
      "[9,   820] loss: 0.06638071686029434\n",
      "[9,   830] loss: 0.5460166931152344\n",
      "[9,   840] loss: 0.3732661008834839\n",
      "[9,   850] loss: 0.0830613300204277\n",
      "[9,   860] loss: 0.6063553690910339\n",
      "[9,   870] loss: 1.1543018817901611\n",
      "[9,   880] loss: 0.4466184377670288\n",
      "[9,   890] loss: 0.048629291355609894\n",
      "[9,   900] loss: 0.34575992822647095\n",
      "[9,   910] loss: 0.07085996866226196\n",
      "[9,   920] loss: 0.37891867756843567\n",
      "[9,   930] loss: 0.23102018237113953\n",
      "[9,   940] loss: 0.34697046875953674\n",
      "[9,   950] loss: 0.28665754199028015\n",
      "[9,   960] loss: 0.2242005467414856\n",
      "[9,   970] loss: 0.16354383528232574\n",
      "[9,   980] loss: 0.3036040961742401\n",
      "[9,   990] loss: 0.03248318284749985\n",
      "[9,  1000] loss: 0.6250024437904358\n",
      "[9,  1010] loss: 0.32103708386421204\n",
      "Got 7203 / 8108 with accuracy: 88.83818450912678%\n",
      "Validating in progress\n",
      "[9,    10] loss: 0.03442668914794922\n",
      "[9,    20] loss: 0.2642573118209839\n",
      "[9,    30] loss: 0.6011238098144531\n",
      "[9,    40] loss: 0.07027940452098846\n",
      "[9,    50] loss: 0.15521734952926636\n",
      "[9,    60] loss: 0.13844697177410126\n",
      "[9,    70] loss: 0.43311217427253723\n",
      "[9,    80] loss: 0.38178056478500366\n",
      "[9,    90] loss: 0.05933330953121185\n",
      "[9,   100] loss: 0.11120782047510147\n",
      "[9,   110] loss: 0.14030928909778595\n",
      "[9,   120] loss: 0.8391348719596863\n",
      "[9,   130] loss: 0.09129032492637634\n",
      "[9,   140] loss: 0.37605178356170654\n",
      "[9,   150] loss: 0.645944356918335\n",
      "[9,   160] loss: 0.30230897665023804\n",
      "[9,   170] loss: 0.032265082001686096\n",
      "[9,   180] loss: 0.007345865480601788\n",
      "[9,   190] loss: 0.04900161549448967\n",
      "[9,   200] loss: 0.37021636962890625\n",
      "[9,   210] loss: 0.1301603615283966\n",
      "[9,   220] loss: 0.9595596194267273\n",
      "[9,   230] loss: 0.3434358239173889\n",
      "[9,   240] loss: 0.04099345952272415\n",
      "[9,   250] loss: 0.0737813413143158\n",
      "[9,   260] loss: 0.26397424936294556\n",
      "[9,   270] loss: 0.07640856504440308\n",
      "[9,   280] loss: 0.03199025243520737\n",
      "[9,   290] loss: 0.36298221349716187\n",
      "Got 2185 / 2315 with accuracy: 94.38444924406048%\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "Training in progress\n",
      "[10,    10] loss: 0.25372251868247986\n",
      "[10,    20] loss: 0.2357291281223297\n",
      "[10,    30] loss: 0.7920396327972412\n",
      "[10,    40] loss: 0.24562212824821472\n",
      "[10,    50] loss: 0.07310812175273895\n",
      "[10,    60] loss: 0.20648156106472015\n",
      "[10,    70] loss: 0.31331509351730347\n",
      "[10,    80] loss: 0.551737904548645\n",
      "[10,    90] loss: 0.2357768416404724\n",
      "[10,   100] loss: 0.07491183280944824\n",
      "[10,   110] loss: 0.029147671535611153\n",
      "[10,   120] loss: 0.08684253692626953\n",
      "[10,   130] loss: 0.6634097695350647\n",
      "[10,   140] loss: 0.743433952331543\n",
      "[10,   150] loss: 0.1731531322002411\n",
      "[10,   160] loss: 0.054454755038022995\n",
      "[10,   170] loss: 0.027010520920157433\n",
      "[10,   180] loss: 0.4111243486404419\n",
      "[10,   190] loss: 0.2158171832561493\n",
      "[10,   200] loss: 0.4484156668186188\n",
      "[10,   210] loss: 0.036494191735982895\n",
      "[10,   220] loss: 0.5418897867202759\n",
      "[10,   230] loss: 0.20719018578529358\n",
      "[10,   240] loss: 0.033996108919382095\n",
      "[10,   250] loss: 0.08897826075553894\n",
      "[10,   260] loss: 0.4166654348373413\n",
      "[10,   270] loss: 0.27403467893600464\n",
      "[10,   280] loss: 0.16414223611354828\n",
      "[10,   290] loss: 0.22482159733772278\n",
      "[10,   300] loss: 0.11982423812150955\n",
      "[10,   310] loss: 0.153518944978714\n",
      "[10,   320] loss: 0.05640654265880585\n",
      "[10,   330] loss: 0.08649744838476181\n",
      "[10,   340] loss: 0.3380917012691498\n",
      "[10,   350] loss: 0.05444791913032532\n",
      "[10,   360] loss: 0.36872419714927673\n",
      "[10,   370] loss: 0.05826159566640854\n",
      "[10,   380] loss: 0.2167290300130844\n",
      "[10,   390] loss: 0.3663672208786011\n",
      "[10,   400] loss: 0.2680429220199585\n",
      "[10,   410] loss: 0.15757809579372406\n",
      "[10,   420] loss: 0.09800149500370026\n",
      "[10,   430] loss: 0.34759408235549927\n",
      "[10,   440] loss: 0.0723927691578865\n",
      "[10,   450] loss: 0.11670958250761032\n",
      "[10,   460] loss: 0.43475088477134705\n",
      "[10,   470] loss: 0.0957479178905487\n",
      "[10,   480] loss: 0.36021876335144043\n",
      "[10,   490] loss: 0.0692104771733284\n",
      "[10,   500] loss: 0.16344279050827026\n",
      "[10,   510] loss: 0.326454758644104\n",
      "[10,   520] loss: 0.2322663962841034\n",
      "[10,   530] loss: 0.07633666694164276\n",
      "[10,   540] loss: 0.31756579875946045\n",
      "[10,   550] loss: 0.1661798506975174\n",
      "[10,   560] loss: 0.033313486725091934\n",
      "[10,   570] loss: 0.14110510051250458\n",
      "[10,   580] loss: 0.06977122277021408\n",
      "[10,   590] loss: 0.12376365065574646\n",
      "[10,   600] loss: 0.20726990699768066\n",
      "[10,   610] loss: 0.1761959046125412\n",
      "[10,   620] loss: 0.07455748319625854\n",
      "[10,   630] loss: 0.45967766642570496\n",
      "[10,   640] loss: 0.29493001103401184\n",
      "[10,   650] loss: 0.22395089268684387\n",
      "[10,   660] loss: 0.021347831934690475\n",
      "[10,   670] loss: 0.11017651855945587\n",
      "[10,   680] loss: 0.12428561598062515\n",
      "[10,   690] loss: 0.29698818922042847\n",
      "[10,   700] loss: 0.286014586687088\n",
      "[10,   710] loss: 0.07686354219913483\n",
      "[10,   720] loss: 0.39136314392089844\n",
      "[10,   730] loss: 0.8005850911140442\n",
      "[10,   740] loss: 0.19565144181251526\n",
      "[10,   750] loss: 0.428149551153183\n",
      "[10,   760] loss: 0.41686388850212097\n",
      "[10,   770] loss: 0.060510311275720596\n",
      "[10,   780] loss: 0.4009442925453186\n",
      "[10,   790] loss: 0.3482186198234558\n",
      "[10,   800] loss: 0.24482224881649017\n",
      "[10,   810] loss: 0.2546426057815552\n",
      "[10,   820] loss: 0.580265462398529\n",
      "[10,   830] loss: 0.1273203194141388\n",
      "[10,   840] loss: 0.09553930163383484\n",
      "[10,   850] loss: 0.6971307396888733\n",
      "[10,   860] loss: 0.27029064297676086\n",
      "[10,   870] loss: 0.27388012409210205\n",
      "[10,   880] loss: 0.33975765109062195\n",
      "[10,   890] loss: 0.027238700538873672\n",
      "[10,   900] loss: 0.12195233255624771\n",
      "[10,   910] loss: 0.16829457879066467\n",
      "[10,   920] loss: 0.056151095777750015\n",
      "[10,   930] loss: 0.18757733702659607\n",
      "[10,   940] loss: 0.13458918035030365\n",
      "[10,   950] loss: 0.6186296343803406\n",
      "[10,   960] loss: 0.12261492013931274\n",
      "[10,   970] loss: 0.367524117231369\n",
      "[10,   980] loss: 0.4853209853172302\n",
      "[10,   990] loss: 1.0022406578063965\n",
      "[10,  1000] loss: 0.37525370717048645\n",
      "[10,  1010] loss: 0.6016002297401428\n",
      "Got 7447 / 8108 with accuracy: 91.84755796743957%\n",
      "Validating in progress\n",
      "[10,    10] loss: 0.11517442762851715\n",
      "[10,    20] loss: 0.057072337716817856\n",
      "[10,    30] loss: 0.2375756800174713\n",
      "[10,    40] loss: 0.3111513555049896\n",
      "[10,    50] loss: 0.03704380616545677\n",
      "[10,    60] loss: 0.016080940142273903\n",
      "[10,    70] loss: 0.4962020814418793\n",
      "[10,    80] loss: 0.03840490058064461\n",
      "[10,    90] loss: 0.13115046918392181\n",
      "[10,   100] loss: 0.09236506372690201\n",
      "[10,   110] loss: 0.01777656003832817\n",
      "[10,   120] loss: 0.09763018786907196\n",
      "[10,   130] loss: 0.001431243377737701\n",
      "[10,   140] loss: 0.020586896687746048\n",
      "[10,   150] loss: 0.04412132874131203\n",
      "[10,   160] loss: 0.011884834617376328\n",
      "[10,   170] loss: 0.011145737953484058\n",
      "[10,   180] loss: 0.5226832032203674\n",
      "[10,   190] loss: 0.01953227072954178\n",
      "[10,   200] loss: 0.006910108961164951\n",
      "[10,   210] loss: 0.0024824892170727253\n",
      "[10,   220] loss: 0.05069684982299805\n",
      "[10,   230] loss: 0.1971341073513031\n",
      "[10,   240] loss: 0.3848578929901123\n",
      "[10,   250] loss: 0.07379456609487534\n",
      "[10,   260] loss: 0.03042936697602272\n",
      "[10,   270] loss: 0.09439879655838013\n",
      "[10,   280] loss: 0.010655302554368973\n",
      "[10,   290] loss: 0.004763864912092686\n",
      "Got 2249 / 2315 with accuracy: 97.14902807775378%\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "Training in progress\n",
      "[11,    10] loss: 0.3484414219856262\n",
      "[11,    20] loss: 0.11835817992687225\n",
      "[11,    30] loss: 0.08820809423923492\n",
      "[11,    40] loss: 0.18165241181850433\n",
      "[11,    50] loss: 0.06000012904405594\n",
      "[11,    60] loss: 0.22259697318077087\n",
      "[11,    70] loss: 0.19734561443328857\n",
      "[11,    80] loss: 0.16089200973510742\n",
      "[11,    90] loss: 0.27997639775276184\n",
      "[11,   100] loss: 0.35385170578956604\n",
      "[11,   110] loss: 0.007816673256456852\n",
      "[11,   120] loss: 0.15865328907966614\n",
      "[11,   130] loss: 0.03506976738572121\n",
      "[11,   140] loss: 0.15115578472614288\n",
      "[11,   150] loss: 0.033022377640008926\n",
      "[11,   160] loss: 0.02315838262438774\n",
      "[11,   170] loss: 0.16438612341880798\n",
      "[11,   180] loss: 0.03331815451383591\n",
      "[11,   190] loss: 0.4199844002723694\n",
      "[11,   200] loss: 0.01673363521695137\n",
      "[11,   210] loss: 0.06472736597061157\n",
      "[11,   220] loss: 0.12279485166072845\n",
      "[11,   230] loss: 0.31796106696128845\n",
      "[11,   240] loss: 0.01545716729015112\n",
      "[11,   250] loss: 0.12514153122901917\n",
      "[11,   260] loss: 0.11282246559858322\n",
      "[11,   270] loss: 0.15996471047401428\n",
      "[11,   280] loss: 0.3191857635974884\n",
      "[11,   290] loss: 0.08316905796527863\n",
      "[11,   300] loss: 0.0690033808350563\n",
      "[11,   310] loss: 0.12792448699474335\n",
      "[11,   320] loss: 0.02351449429988861\n",
      "[11,   330] loss: 0.20815686881542206\n",
      "[11,   340] loss: 0.12114957720041275\n",
      "[11,   350] loss: 0.18461807072162628\n",
      "[11,   360] loss: 0.10772719979286194\n",
      "[11,   370] loss: 0.35709148645401\n",
      "[11,   380] loss: 0.30901995301246643\n",
      "[11,   390] loss: 0.17827214300632477\n",
      "[11,   400] loss: 0.16654908657073975\n",
      "[11,   410] loss: 0.19995355606079102\n",
      "[11,   420] loss: 0.027911826968193054\n",
      "[11,   430] loss: 0.07167630642652512\n",
      "[11,   440] loss: 0.020186666399240494\n",
      "[11,   450] loss: 0.03143119439482689\n",
      "[11,   460] loss: 0.022008173167705536\n",
      "[11,   470] loss: 0.05352383852005005\n",
      "[11,   480] loss: 0.3189820349216461\n",
      "[11,   490] loss: 0.039723124355077744\n",
      "[11,   500] loss: 0.018372705206274986\n",
      "[11,   510] loss: 0.848204493522644\n",
      "[11,   520] loss: 0.160360649228096\n",
      "[11,   530] loss: 0.16599632799625397\n",
      "[11,   540] loss: 0.032329678535461426\n",
      "[11,   550] loss: 0.4314976632595062\n",
      "[11,   560] loss: 0.11121462285518646\n",
      "[11,   570] loss: 0.93978351354599\n",
      "[11,   580] loss: 0.02922472357749939\n",
      "[11,   590] loss: 0.1692868024110794\n",
      "[11,   600] loss: 0.08694830536842346\n",
      "[11,   610] loss: 0.13206203281879425\n",
      "[11,   620] loss: 0.5791200995445251\n",
      "[11,   630] loss: 0.024849658831954002\n",
      "[11,   640] loss: 0.02753506973385811\n",
      "[11,   650] loss: 0.010529906488955021\n",
      "[11,   660] loss: 0.3643162250518799\n",
      "[11,   670] loss: 0.08625559508800507\n",
      "[11,   680] loss: 0.7163969278335571\n",
      "[11,   690] loss: 0.33612269163131714\n",
      "[11,   700] loss: 0.045011553913354874\n",
      "[11,   710] loss: 0.1540932059288025\n",
      "[11,   720] loss: 0.09763449430465698\n",
      "[11,   730] loss: 0.1738882064819336\n",
      "[11,   740] loss: 0.17472589015960693\n",
      "[11,   750] loss: 0.1968221366405487\n",
      "[11,   760] loss: 0.023145221173763275\n",
      "[11,   770] loss: 0.05091480910778046\n",
      "[11,   780] loss: 0.3995944857597351\n",
      "[11,   790] loss: 0.48027798533439636\n",
      "[11,   800] loss: 0.2174445390701294\n",
      "[11,   810] loss: 0.22054573893547058\n",
      "[11,   820] loss: 0.10693308711051941\n",
      "[11,   830] loss: 0.2193421721458435\n",
      "[11,   840] loss: 0.021752983331680298\n",
      "[11,   850] loss: 0.02516062557697296\n",
      "[11,   860] loss: 0.11950907111167908\n",
      "[11,   870] loss: 0.04062876105308533\n",
      "[11,   880] loss: 0.6604302525520325\n",
      "[11,   890] loss: 0.05648723617196083\n",
      "[11,   900] loss: 0.08599604666233063\n",
      "[11,   910] loss: 0.03804339841008186\n",
      "[11,   920] loss: 0.26547500491142273\n",
      "[11,   930] loss: 0.10338805615901947\n",
      "[11,   940] loss: 0.49032074213027954\n",
      "[11,   950] loss: 0.024899205192923546\n",
      "[11,   960] loss: 0.15457050502300262\n",
      "[11,   970] loss: 0.0351896770298481\n",
      "[11,   980] loss: 0.34978750348091125\n",
      "[11,   990] loss: 0.053717657923698425\n",
      "[11,  1000] loss: 0.02250567451119423\n",
      "[11,  1010] loss: 0.03881794214248657\n",
      "Got 7529 / 8108 with accuracy: 92.85890478539713%\n",
      "Validating in progress\n",
      "[11,    10] loss: 0.003642444498836994\n",
      "[11,    20] loss: 0.004282727837562561\n",
      "[11,    30] loss: 0.00544769549742341\n",
      "[11,    40] loss: 0.11496677249670029\n",
      "[11,    50] loss: 0.014531844295561314\n",
      "[11,    60] loss: 0.005701068323105574\n",
      "[11,    70] loss: 0.020338773727416992\n",
      "[11,    80] loss: 0.015206309966742992\n",
      "[11,    90] loss: 0.020222172141075134\n",
      "[11,   100] loss: 0.09331299364566803\n",
      "[11,   110] loss: 0.016449986025691032\n",
      "[11,   120] loss: 0.02815781906247139\n",
      "[11,   130] loss: 0.015292412601411343\n",
      "[11,   140] loss: 0.0049108099192380905\n",
      "[11,   150] loss: 0.007283847779035568\n",
      "[11,   160] loss: 0.00552217336371541\n",
      "[11,   170] loss: 0.044490549713373184\n",
      "[11,   180] loss: 0.06944422423839569\n",
      "[11,   190] loss: 0.002962020691484213\n",
      "[11,   200] loss: 0.36966657638549805\n",
      "[11,   210] loss: 0.005135123617947102\n",
      "[11,   220] loss: 0.05271722003817558\n",
      "[11,   230] loss: 0.8471174836158752\n",
      "[11,   240] loss: 0.11212081462144852\n",
      "[11,   250] loss: 0.0022133428137749434\n",
      "[11,   260] loss: 0.01915762573480606\n",
      "[11,   270] loss: 0.42669641971588135\n",
      "[11,   280] loss: 0.002021393971517682\n",
      "[11,   290] loss: 0.01366827730089426\n",
      "Got 2272 / 2315 with accuracy: 98.1425485961123%\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "Training in progress\n",
      "[12,    10] loss: 0.08163714408874512\n",
      "[12,    20] loss: 0.030256832018494606\n",
      "[12,    30] loss: 0.0030822313856333494\n",
      "[12,    40] loss: 0.5310927033424377\n",
      "[12,    50] loss: 0.06838840991258621\n",
      "[12,    60] loss: 0.6474428176879883\n",
      "[12,    70] loss: 0.0525238960981369\n",
      "[12,    80] loss: 0.2914358079433441\n",
      "[12,    90] loss: 0.5309556126594543\n",
      "[12,   100] loss: 0.0447423979640007\n",
      "[12,   110] loss: 0.008339046500623226\n",
      "[12,   120] loss: 0.2939378321170807\n",
      "[12,   130] loss: 0.07879617810249329\n",
      "[12,   140] loss: 0.23570337891578674\n",
      "[12,   150] loss: 0.2082805186510086\n",
      "[12,   160] loss: 0.04503881186246872\n",
      "[12,   170] loss: 0.35363224148750305\n",
      "[12,   180] loss: 0.08195619285106659\n",
      "[12,   190] loss: 0.7209465503692627\n",
      "[12,   200] loss: 0.05854937061667442\n",
      "[12,   210] loss: 0.20093020796775818\n",
      "[12,   220] loss: 0.06561367958784103\n",
      "[12,   230] loss: 0.345626562833786\n",
      "[12,   240] loss: 0.12910732626914978\n",
      "[12,   250] loss: 0.039314597845077515\n",
      "[12,   260] loss: 0.08315201848745346\n",
      "[12,   270] loss: 0.046264152973890305\n",
      "[12,   280] loss: 0.7923605442047119\n",
      "[12,   290] loss: 0.42649737000465393\n",
      "[12,   300] loss: 0.103727787733078\n",
      "[12,   310] loss: 0.14053218066692352\n",
      "[12,   320] loss: 0.15694494545459747\n",
      "[12,   330] loss: 0.02314315363764763\n",
      "[12,   340] loss: 0.07173161953687668\n",
      "[12,   350] loss: 0.17431969940662384\n",
      "[12,   360] loss: 0.9793747067451477\n",
      "[12,   370] loss: 0.22039540112018585\n",
      "[12,   380] loss: 0.5868802070617676\n",
      "[12,   390] loss: 0.0700439065694809\n",
      "[12,   400] loss: 0.04256278648972511\n",
      "[12,   410] loss: 0.01448522787541151\n",
      "[12,   420] loss: 0.2609027922153473\n",
      "[12,   430] loss: 0.07270678132772446\n",
      "[12,   440] loss: 0.09456402063369751\n",
      "[12,   450] loss: 0.08527638763189316\n",
      "[12,   460] loss: 0.006968549452722073\n",
      "[12,   470] loss: 0.08688466250896454\n",
      "[12,   480] loss: 0.040222495794296265\n",
      "[12,   490] loss: 0.04271145164966583\n",
      "[12,   500] loss: 0.08958864957094193\n",
      "[12,   510] loss: 0.030496980994939804\n",
      "[12,   520] loss: 0.10658504068851471\n",
      "[12,   530] loss: 0.47946465015411377\n",
      "[12,   540] loss: 0.2702944874763489\n",
      "[12,   550] loss: 0.2113407552242279\n",
      "[12,   560] loss: 0.06036822870373726\n",
      "[12,   570] loss: 0.10019083321094513\n",
      "[12,   580] loss: 0.10997729003429413\n",
      "[12,   590] loss: 0.18077850341796875\n",
      "[12,   600] loss: 0.19959290325641632\n",
      "[12,   610] loss: 0.012173265218734741\n",
      "[12,   620] loss: 0.5171862840652466\n",
      "[12,   630] loss: 0.007399171125143766\n",
      "[12,   640] loss: 0.43882954120635986\n",
      "[12,   650] loss: 0.03027743101119995\n",
      "[12,   660] loss: 0.3941657543182373\n",
      "[12,   670] loss: 0.22409331798553467\n",
      "[12,   680] loss: 0.09256256371736526\n",
      "[12,   690] loss: 0.17019006609916687\n",
      "[12,   700] loss: 0.07045000791549683\n",
      "[12,   710] loss: 0.2101757824420929\n",
      "[12,   720] loss: 0.5187251567840576\n",
      "[12,   730] loss: 0.11800096184015274\n",
      "[12,   740] loss: 0.20086678862571716\n",
      "[12,   750] loss: 0.7320574522018433\n",
      "[12,   760] loss: 0.0378052219748497\n",
      "[12,   770] loss: 0.26586079597473145\n",
      "[12,   780] loss: 0.018138131126761436\n",
      "[12,   790] loss: 0.3650946021080017\n",
      "[12,   800] loss: 0.06808426231145859\n",
      "[12,   810] loss: 0.024072136729955673\n",
      "[12,   820] loss: 0.05846989527344704\n",
      "[12,   830] loss: 0.5897626876831055\n",
      "[12,   840] loss: 0.03080575354397297\n",
      "[12,   850] loss: 0.055404774844646454\n",
      "[12,   860] loss: 0.9372564554214478\n",
      "[12,   870] loss: 0.4707399010658264\n",
      "[12,   880] loss: 0.016394536942243576\n",
      "[12,   890] loss: 0.02656368911266327\n",
      "[12,   900] loss: 0.017653029412031174\n",
      "[12,   910] loss: 0.06606361269950867\n",
      "[12,   920] loss: 0.09476015716791153\n",
      "[12,   930] loss: 0.2075255960226059\n",
      "[12,   940] loss: 0.9805290102958679\n",
      "[12,   950] loss: 0.1750672310590744\n",
      "[12,   960] loss: 0.4614161252975464\n",
      "[12,   970] loss: 0.318881630897522\n",
      "[12,   980] loss: 0.015291588380932808\n",
      "[12,   990] loss: 0.015078534372150898\n",
      "[12,  1000] loss: 0.11495817452669144\n",
      "[12,  1010] loss: 0.18287649750709534\n",
      "Got 7651 / 8108 with accuracy: 94.36359151455352%\n",
      "Validating in progress\n",
      "[12,    10] loss: 0.09362104535102844\n",
      "[12,    20] loss: 0.27403923869132996\n",
      "[12,    30] loss: 0.0035826023668050766\n",
      "[12,    40] loss: 0.004615573212504387\n",
      "[12,    50] loss: 0.02684641443192959\n",
      "[12,    60] loss: 0.0096987159922719\n",
      "[12,    70] loss: 0.004662851337343454\n",
      "[12,    80] loss: 0.016075685620307922\n",
      "[12,    90] loss: 8.417612843913957e-05\n",
      "[12,   100] loss: 0.011663456447422504\n",
      "[12,   110] loss: 0.07472690939903259\n",
      "[12,   120] loss: 0.07588260620832443\n",
      "[12,   130] loss: 0.09331239014863968\n",
      "[12,   140] loss: 0.031377144157886505\n",
      "[12,   150] loss: 0.012658318504691124\n",
      "[12,   160] loss: 0.011533267796039581\n",
      "[12,   170] loss: 0.004349410999566317\n",
      "[12,   180] loss: 0.22580641508102417\n",
      "[12,   190] loss: 0.06035200133919716\n",
      "[12,   200] loss: 0.0010760201839730144\n",
      "[12,   210] loss: 0.12453099340200424\n",
      "[12,   220] loss: 0.00020037783542647958\n",
      "[12,   230] loss: 0.0034216351341456175\n",
      "[12,   240] loss: 0.04013083130121231\n",
      "[12,   250] loss: 0.005505546927452087\n",
      "[12,   260] loss: 0.010993449948728085\n",
      "[12,   270] loss: 0.005672452040016651\n",
      "[12,   280] loss: 0.010478334501385689\n",
      "[12,   290] loss: 0.13610535860061646\n",
      "Got 2291 / 2315 with accuracy: 98.96328293736502%\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "Training in progress\n",
      "[13,    10] loss: 0.020855745300650597\n",
      "[13,    20] loss: 0.022519059479236603\n",
      "[13,    30] loss: 0.03163807839155197\n",
      "[13,    40] loss: 0.22356009483337402\n",
      "[13,    50] loss: 0.40252599120140076\n",
      "[13,    60] loss: 0.36028853058815\n",
      "[13,    70] loss: 0.25166869163513184\n",
      "[13,    80] loss: 0.18186581134796143\n",
      "[13,    90] loss: 0.03310849517583847\n",
      "[13,   100] loss: 0.008582278154790401\n",
      "[13,   110] loss: 0.16224688291549683\n",
      "[13,   120] loss: 0.028194766491651535\n",
      "[13,   130] loss: 0.11602507531642914\n",
      "[13,   140] loss: 0.33016228675842285\n",
      "[13,   150] loss: 0.0075814565643668175\n",
      "[13,   160] loss: 0.8855169415473938\n",
      "[13,   170] loss: 0.30466464161872864\n",
      "[13,   180] loss: 0.027509544044733047\n",
      "[13,   190] loss: 0.05614227056503296\n",
      "[13,   200] loss: 0.32092607021331787\n",
      "[13,   210] loss: 0.011353954672813416\n",
      "[13,   220] loss: 0.16132426261901855\n",
      "[13,   230] loss: 0.3799074590206146\n",
      "[13,   240] loss: 0.03068835474550724\n",
      "[13,   250] loss: 0.1295301467180252\n",
      "[13,   260] loss: 0.9877040386199951\n",
      "[13,   270] loss: 0.26033878326416016\n",
      "[13,   280] loss: 0.06732747703790665\n",
      "[13,   290] loss: 0.4484289288520813\n",
      "[13,   300] loss: 0.05476606637239456\n",
      "[13,   310] loss: 0.06639732420444489\n",
      "[13,   320] loss: 0.08248982578516006\n",
      "[13,   330] loss: 0.25422316789627075\n",
      "[13,   340] loss: 0.7073118090629578\n",
      "[13,   350] loss: 0.34405580163002014\n",
      "[13,   360] loss: 0.026286184787750244\n",
      "[13,   370] loss: 0.029670551419258118\n",
      "[13,   380] loss: 0.03326336666941643\n",
      "[13,   390] loss: 0.32596760988235474\n",
      "[13,   400] loss: 0.10350808501243591\n",
      "[13,   410] loss: 0.42370927333831787\n",
      "[13,   420] loss: 0.5148771405220032\n",
      "[13,   430] loss: 0.06858348101377487\n",
      "[13,   440] loss: 0.02240431122481823\n",
      "[13,   450] loss: 0.05383071303367615\n",
      "[13,   460] loss: 0.06959902495145798\n",
      "[13,   470] loss: 0.17605100572109222\n",
      "[13,   480] loss: 0.05009973794221878\n",
      "[13,   490] loss: 0.14909027516841888\n",
      "[13,   500] loss: 0.16564254462718964\n",
      "[13,   510] loss: 0.12290163338184357\n",
      "[13,   520] loss: 0.030785802751779556\n",
      "[13,   530] loss: 0.08820971101522446\n",
      "[13,   540] loss: 0.07219596207141876\n",
      "[13,   550] loss: 0.008597863838076591\n",
      "[13,   560] loss: 0.3166663646697998\n",
      "[13,   570] loss: 0.10761630535125732\n",
      "[13,   580] loss: 0.05450580641627312\n",
      "[13,   590] loss: 0.19871453940868378\n",
      "[13,   600] loss: 0.2955402135848999\n",
      "[13,   610] loss: 0.19591158628463745\n",
      "[13,   620] loss: 0.06545831263065338\n",
      "[13,   630] loss: 0.08544828742742538\n",
      "[13,   640] loss: 0.24648596346378326\n",
      "[13,   650] loss: 0.04233910143375397\n",
      "[13,   660] loss: 0.030105773359537125\n",
      "[13,   670] loss: 0.19306166470050812\n",
      "[13,   680] loss: 0.051079317927360535\n",
      "[13,   690] loss: 0.35285425186157227\n",
      "[13,   700] loss: 0.04639522731304169\n",
      "[13,   710] loss: 0.03373933956027031\n",
      "[13,   720] loss: 0.012783602811396122\n",
      "[13,   730] loss: 0.018778551369905472\n",
      "[13,   740] loss: 0.0337495282292366\n",
      "[13,   750] loss: 0.5577753186225891\n",
      "[13,   760] loss: 0.0322723351418972\n",
      "[13,   770] loss: 0.15528282523155212\n",
      "[13,   780] loss: 0.14470793306827545\n",
      "[13,   790] loss: 0.04182989522814751\n",
      "[13,   800] loss: 0.02414163388311863\n",
      "[13,   810] loss: 0.05357232689857483\n",
      "[13,   820] loss: 0.11844180524349213\n",
      "[13,   830] loss: 0.05802307277917862\n",
      "[13,   840] loss: 0.028816066682338715\n",
      "[13,   850] loss: 0.030512602999806404\n",
      "[13,   860] loss: 0.02097124420106411\n",
      "[13,   870] loss: 0.10295122116804123\n",
      "[13,   880] loss: 0.19272476434707642\n",
      "[13,   890] loss: 0.013408208265900612\n",
      "[13,   900] loss: 0.28394970297813416\n",
      "[13,   910] loss: 0.485018789768219\n",
      "[13,   920] loss: 0.022845976054668427\n",
      "[13,   930] loss: 0.17989496886730194\n",
      "[13,   940] loss: 0.05833010375499725\n",
      "[13,   950] loss: 0.03434339910745621\n",
      "[13,   960] loss: 0.04771416634321213\n",
      "[13,   970] loss: 0.25693050026893616\n",
      "[13,   980] loss: 0.04925273731350899\n",
      "[13,   990] loss: 0.020026370882987976\n",
      "[13,  1000] loss: 0.44031399488449097\n",
      "[13,  1010] loss: 0.06739030033349991\n",
      "Got 7674 / 8108 with accuracy: 94.64726196349285%\n",
      "Validating in progress\n",
      "[13,    10] loss: 0.000697400770150125\n",
      "[13,    20] loss: 0.0012789536267518997\n",
      "[13,    30] loss: 0.060882341116666794\n",
      "[13,    40] loss: 0.002731634769588709\n",
      "[13,    50] loss: 0.015600964426994324\n",
      "[13,    60] loss: 0.017794005572795868\n",
      "[13,    70] loss: 0.0021612835116684437\n",
      "[13,    80] loss: 0.005838323850184679\n",
      "[13,    90] loss: 0.004092151764780283\n",
      "[13,   100] loss: 0.024750716984272003\n",
      "[13,   110] loss: 0.0009812639327719808\n",
      "[13,   120] loss: 0.04419748857617378\n",
      "[13,   130] loss: 0.022687390446662903\n",
      "[13,   140] loss: 0.05236686393618584\n",
      "[13,   150] loss: 0.002673510229215026\n",
      "[13,   160] loss: 0.01408215332776308\n",
      "[13,   170] loss: 0.02787932939827442\n",
      "[13,   180] loss: 0.012355886399745941\n",
      "[13,   190] loss: 0.006746482569724321\n",
      "[13,   200] loss: 0.003910220228135586\n",
      "[13,   210] loss: 0.0025894097052514553\n",
      "[13,   220] loss: 0.005054030567407608\n",
      "[13,   230] loss: 0.020785178989171982\n",
      "[13,   240] loss: 0.0020097147207707167\n",
      "[13,   250] loss: 0.012054958380758762\n",
      "[13,   260] loss: 0.014670238830149174\n",
      "[13,   270] loss: 0.004354913718998432\n",
      "[13,   280] loss: 0.4636044502258301\n",
      "[13,   290] loss: 0.11609732359647751\n",
      "Got 2305 / 2315 with accuracy: 99.56803455723542%\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "Training in progress\n",
      "[14,    10] loss: 0.0944828987121582\n",
      "[14,    20] loss: 0.011959320865571499\n",
      "[14,    30] loss: 0.06535523384809494\n",
      "[14,    40] loss: 0.01707097515463829\n",
      "[14,    50] loss: 0.19865211844444275\n",
      "[14,    60] loss: 0.1419956237077713\n",
      "[14,    70] loss: 0.014062268659472466\n",
      "[14,    80] loss: 0.02302500605583191\n",
      "[14,    90] loss: 0.11100258678197861\n",
      "[14,   100] loss: 0.38001808524131775\n",
      "[14,   110] loss: 0.09685083478689194\n",
      "[14,   120] loss: 0.01018211618065834\n",
      "[14,   130] loss: 0.10800054669380188\n",
      "[14,   140] loss: 0.08308158814907074\n",
      "[14,   150] loss: 0.011164398863911629\n",
      "[14,   160] loss: 0.18105924129486084\n",
      "[14,   170] loss: 0.07692714035511017\n",
      "[14,   180] loss: 0.21902894973754883\n",
      "[14,   190] loss: 0.0754489004611969\n",
      "[14,   200] loss: 0.13174042105674744\n",
      "[14,   210] loss: 0.16689574718475342\n",
      "[14,   220] loss: 0.10814365744590759\n",
      "[14,   230] loss: 0.36447620391845703\n",
      "[14,   240] loss: 0.01973874680697918\n",
      "[14,   250] loss: 0.02359718084335327\n",
      "[14,   260] loss: 0.07033859193325043\n",
      "[14,   270] loss: 0.05873510614037514\n",
      "[14,   280] loss: 0.016502875834703445\n",
      "[14,   290] loss: 0.29902422428131104\n",
      "[14,   300] loss: 0.03627849370241165\n",
      "[14,   310] loss: 0.28384748101234436\n",
      "[14,   320] loss: 0.023505963385105133\n",
      "[14,   330] loss: 0.19505208730697632\n",
      "[14,   340] loss: 0.025480758398771286\n",
      "[14,   350] loss: 0.0070730214938521385\n",
      "[14,   360] loss: 0.004498131573200226\n",
      "[14,   370] loss: 0.014827975071966648\n",
      "[14,   380] loss: 0.15526455640792847\n",
      "[14,   390] loss: 0.309536874294281\n",
      "[14,   400] loss: 0.07447928190231323\n",
      "[14,   410] loss: 0.09838374704122543\n",
      "[14,   420] loss: 0.006289143115282059\n",
      "[14,   430] loss: 0.10562623292207718\n",
      "[14,   440] loss: 0.07069963961839676\n",
      "[14,   450] loss: 0.028924398124217987\n",
      "[14,   460] loss: 0.05850636586546898\n",
      "[14,   470] loss: 0.01822824217379093\n",
      "[14,   480] loss: 0.06601427495479584\n",
      "[14,   490] loss: 0.17423337697982788\n",
      "[14,   500] loss: 0.02394275739789009\n",
      "[14,   510] loss: 0.03140759840607643\n",
      "[14,   520] loss: 0.08985092490911484\n",
      "[14,   530] loss: 0.2612115740776062\n",
      "[14,   540] loss: 0.42407023906707764\n",
      "[14,   550] loss: 0.10445179790258408\n",
      "[14,   560] loss: 0.022402947768568993\n",
      "[14,   570] loss: 0.18738389015197754\n",
      "[14,   580] loss: 0.028720330446958542\n",
      "[14,   590] loss: 0.02786005660891533\n",
      "[14,   600] loss: 0.20576117932796478\n",
      "[14,   610] loss: 0.052631571888923645\n",
      "[14,   620] loss: 0.020794562995433807\n",
      "[14,   630] loss: 0.011911418288946152\n",
      "[14,   640] loss: 0.15576671063899994\n",
      "[14,   650] loss: 0.2162608653306961\n",
      "[14,   660] loss: 0.020250389352440834\n",
      "[14,   670] loss: 0.09351472556591034\n",
      "[14,   680] loss: 0.2185385525226593\n",
      "[14,   690] loss: 0.04400887340307236\n",
      "[14,   700] loss: 0.20366477966308594\n",
      "[14,   710] loss: 0.009366952814161777\n",
      "[14,   720] loss: 0.015258166939020157\n",
      "[14,   730] loss: 0.047656700015068054\n",
      "[14,   740] loss: 0.06168672814965248\n",
      "[14,   750] loss: 0.289376437664032\n",
      "[14,   760] loss: 0.5271244645118713\n",
      "[14,   770] loss: 1.153239130973816\n",
      "[14,   780] loss: 0.151521697640419\n",
      "[14,   790] loss: 0.026383334770798683\n",
      "[14,   800] loss: 0.3783746361732483\n",
      "[14,   810] loss: 0.13390839099884033\n",
      "[14,   820] loss: 0.03839613497257233\n",
      "[14,   830] loss: 0.03808615356683731\n",
      "[14,   840] loss: 0.06883738189935684\n",
      "[14,   850] loss: 0.00910181924700737\n",
      "[14,   860] loss: 0.186358243227005\n",
      "[14,   870] loss: 0.10127672553062439\n",
      "[14,   880] loss: 0.06290993839502335\n",
      "[14,   890] loss: 0.23865890502929688\n",
      "[14,   900] loss: 0.09052786231040955\n",
      "[14,   910] loss: 0.4569660425186157\n",
      "[14,   920] loss: 0.528432309627533\n",
      "[14,   930] loss: 0.005836638621985912\n",
      "[14,   940] loss: 0.3711209297180176\n",
      "[14,   950] loss: 0.01212114468216896\n",
      "[14,   960] loss: 0.009903255850076675\n",
      "[14,   970] loss: 0.015415728092193604\n",
      "[14,   980] loss: 0.012000746093690395\n",
      "[14,   990] loss: 0.08147609233856201\n",
      "[14,  1000] loss: 0.2036287933588028\n",
      "[14,  1010] loss: 0.03597918525338173\n",
      "Got 7767 / 8108 with accuracy: 95.79427725703009%\n",
      "Validating in progress\n",
      "[14,    10] loss: 0.1808471381664276\n",
      "[14,    20] loss: 0.005288637708872557\n",
      "[14,    30] loss: 0.0043313102796673775\n",
      "[14,    40] loss: 0.002066562417894602\n",
      "[14,    50] loss: 0.01755201257765293\n",
      "[14,    60] loss: 0.009974511340260506\n",
      "[14,    70] loss: 0.011282268911600113\n",
      "[14,    80] loss: 0.00731871509924531\n",
      "[14,    90] loss: 0.0357498824596405\n",
      "[14,   100] loss: 0.002742976415902376\n",
      "[14,   110] loss: 0.04491577297449112\n",
      "[14,   120] loss: 0.003019779920578003\n",
      "[14,   130] loss: 0.023291926831007004\n",
      "[14,   140] loss: 0.0781363770365715\n",
      "[14,   150] loss: 0.014321710914373398\n",
      "[14,   160] loss: 0.0162632018327713\n",
      "[14,   170] loss: 0.00541187496855855\n",
      "[14,   180] loss: 0.004398656543344259\n",
      "[14,   190] loss: 0.004875724203884602\n",
      "[14,   200] loss: 0.021251488476991653\n",
      "[14,   210] loss: 0.01451879646629095\n",
      "[14,   220] loss: 0.0053436532616615295\n",
      "[14,   230] loss: 0.006289627868682146\n",
      "[14,   240] loss: 0.0015121486503630877\n",
      "[14,   250] loss: 0.05673649162054062\n",
      "[14,   260] loss: 0.025737710297107697\n",
      "[14,   270] loss: 0.01382630318403244\n",
      "[14,   280] loss: 0.012529924511909485\n",
      "[14,   290] loss: 0.0073516336269676685\n",
      "Got 2310 / 2315 with accuracy: 99.78401727861771%\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "Training in progress\n",
      "[15,    10] loss: 0.04178425669670105\n",
      "[15,    20] loss: 0.012869307771325111\n",
      "[15,    30] loss: 0.016813335940241814\n",
      "[15,    40] loss: 0.4563145041465759\n",
      "[15,    50] loss: 0.05750079080462456\n",
      "[15,    60] loss: 0.2151140719652176\n",
      "[15,    70] loss: 0.007669125683605671\n",
      "[15,    80] loss: 0.0307125486433506\n",
      "[15,    90] loss: 0.010012270882725716\n",
      "[15,   100] loss: 0.09909042716026306\n",
      "[15,   110] loss: 0.026499193161725998\n",
      "[15,   120] loss: 1.0108516216278076\n",
      "[15,   130] loss: 0.05377050116658211\n",
      "[15,   140] loss: 0.005019709933549166\n",
      "[15,   150] loss: 0.083697110414505\n",
      "[15,   160] loss: 0.05455946922302246\n",
      "[15,   170] loss: 0.007378309033811092\n",
      "[15,   180] loss: 0.08722614496946335\n",
      "[15,   190] loss: 0.3233935534954071\n",
      "[15,   200] loss: 0.022435078397393227\n",
      "[15,   210] loss: 0.025061476975679398\n",
      "[15,   220] loss: 0.055979128926992416\n",
      "[15,   230] loss: 0.015051525086164474\n",
      "[15,   240] loss: 0.045321330428123474\n",
      "[15,   250] loss: 0.009338035248219967\n",
      "[15,   260] loss: 0.01671522855758667\n",
      "[15,   270] loss: 0.1006939634680748\n",
      "[15,   280] loss: 0.04991089552640915\n",
      "[15,   290] loss: 0.006749426946043968\n",
      "[15,   300] loss: 0.0732361376285553\n",
      "[15,   310] loss: 0.021260017529129982\n",
      "[15,   320] loss: 0.48933762311935425\n",
      "[15,   330] loss: 0.052157167345285416\n",
      "[15,   340] loss: 0.09554004669189453\n",
      "[15,   350] loss: 0.12498002499341965\n",
      "[15,   360] loss: 0.032473817467689514\n",
      "[15,   370] loss: 0.10137827694416046\n",
      "[15,   380] loss: 0.03554042428731918\n",
      "[15,   390] loss: 0.028807004913687706\n",
      "[15,   400] loss: 0.014375009573996067\n",
      "[15,   410] loss: 0.0022354412358254194\n",
      "[15,   420] loss: 0.966780960559845\n",
      "[15,   430] loss: 0.022723693400621414\n",
      "[15,   440] loss: 0.04915124177932739\n",
      "[15,   450] loss: 0.12229423224925995\n",
      "[15,   460] loss: 0.23081451654434204\n",
      "[15,   470] loss: 0.4962156414985657\n",
      "[15,   480] loss: 0.03550847992300987\n",
      "[15,   490] loss: 0.04350675642490387\n",
      "[15,   500] loss: 0.009555469267070293\n",
      "[15,   510] loss: 0.0772855281829834\n",
      "[15,   520] loss: 0.035737521946430206\n",
      "[15,   530] loss: 0.09365405887365341\n",
      "[15,   540] loss: 1.3638197183609009\n",
      "[15,   550] loss: 0.014980168081820011\n",
      "[15,   560] loss: 0.058641158044338226\n",
      "[15,   570] loss: 0.26117244362831116\n",
      "[15,   580] loss: 0.03445511683821678\n",
      "[15,   590] loss: 0.27907124161720276\n",
      "[15,   600] loss: 0.07691311836242676\n",
      "[15,   610] loss: 0.11201564967632294\n",
      "[15,   620] loss: 0.04146905243396759\n",
      "[15,   630] loss: 0.054540425539016724\n",
      "[15,   640] loss: 0.19802023470401764\n",
      "[15,   650] loss: 0.23373278975486755\n",
      "[15,   660] loss: 0.4279809594154358\n",
      "[15,   670] loss: 0.081857830286026\n",
      "[15,   680] loss: 0.03438764810562134\n",
      "[15,   690] loss: 0.214568093419075\n",
      "[15,   700] loss: 0.1157444417476654\n",
      "[15,   710] loss: 0.19614672660827637\n",
      "[15,   720] loss: 0.3931850492954254\n",
      "[15,   730] loss: 0.027481338009238243\n",
      "[15,   740] loss: 0.05675415322184563\n",
      "[15,   750] loss: 0.05955357849597931\n",
      "[15,   760] loss: 0.038009654730558395\n",
      "[15,   770] loss: 0.006950675509870052\n",
      "[15,   780] loss: 0.1725219339132309\n",
      "[15,   790] loss: 0.0412813276052475\n",
      "[15,   800] loss: 0.30547013878822327\n",
      "[15,   810] loss: 0.2204235941171646\n",
      "[15,   820] loss: 0.6577375531196594\n",
      "[15,   830] loss: 0.006284048315137625\n",
      "[15,   840] loss: 1.0616564750671387\n",
      "[15,   850] loss: 0.35636454820632935\n",
      "[15,   860] loss: 0.009056917391717434\n",
      "[15,   870] loss: 0.001758052036166191\n",
      "[15,   880] loss: 0.11866450309753418\n",
      "[15,   890] loss: 0.05453456938266754\n",
      "[15,   900] loss: 0.09198121726512909\n",
      "[15,   910] loss: 0.04678989574313164\n",
      "[15,   920] loss: 0.18360456824302673\n",
      "[15,   930] loss: 0.0028392975218594074\n",
      "[15,   940] loss: 0.035627949982881546\n",
      "[15,   950] loss: 0.10889380425214767\n",
      "[15,   960] loss: 0.005524943582713604\n",
      "[15,   970] loss: 0.16044023633003235\n",
      "[15,   980] loss: 0.07618232816457748\n",
      "[15,   990] loss: 0.10686739534139633\n",
      "[15,  1000] loss: 0.04896930977702141\n",
      "[15,  1010] loss: 0.4566707909107208\n",
      "Got 7818 / 8108 with accuracy: 96.42328564380858%\n",
      "Validating in progress\n",
      "[15,    10] loss: 0.0035249884240329266\n",
      "[15,    20] loss: 0.0015229005366563797\n",
      "[15,    30] loss: 0.023039568215608597\n",
      "[15,    40] loss: 0.06816531717777252\n",
      "[15,    50] loss: 0.008748571388423443\n",
      "[15,    60] loss: 0.003017673036083579\n",
      "[15,    70] loss: 0.014238745905458927\n",
      "[15,    80] loss: 0.00779208168387413\n",
      "[15,    90] loss: 0.02264305204153061\n",
      "[15,   100] loss: 0.0008956575184129179\n",
      "[15,   110] loss: 0.01737534999847412\n",
      "[15,   120] loss: 0.0017557600513100624\n",
      "[15,   130] loss: 0.0017936008516699076\n",
      "[15,   140] loss: 0.0014916113577783108\n",
      "[15,   150] loss: 0.0017383303493261337\n",
      "[15,   160] loss: 0.00904238224029541\n",
      "[15,   170] loss: 0.007375395391136408\n",
      "[15,   180] loss: 0.0013516657054424286\n",
      "[15,   190] loss: 0.024354876950383186\n",
      "[15,   200] loss: 0.01179918460547924\n",
      "[15,   210] loss: 0.01083515863865614\n",
      "[15,   220] loss: 0.00463481480255723\n",
      "[15,   230] loss: 0.05356762558221817\n",
      "[15,   240] loss: 0.007207776419818401\n",
      "[15,   250] loss: 0.011627739295363426\n",
      "[15,   260] loss: 0.009849335998296738\n",
      "[15,   270] loss: 0.000979699892923236\n",
      "[15,   280] loss: 0.0015134355053305626\n",
      "[15,   290] loss: 0.014477952383458614\n",
      "Got 2310 / 2315 with accuracy: 99.78401727861771%\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "Training in progress\n",
      "[16,    10] loss: 0.04126551374793053\n",
      "[16,    20] loss: 0.006803637370467186\n",
      "[16,    30] loss: 0.02668886072933674\n",
      "[16,    40] loss: 0.018608877435326576\n",
      "[16,    50] loss: 0.020522207021713257\n",
      "[16,    60] loss: 0.06453735381364822\n",
      "[16,    70] loss: 0.012355310842394829\n",
      "[16,    80] loss: 0.007756680250167847\n",
      "[16,    90] loss: 0.026889825239777565\n",
      "[16,   100] loss: 0.09439203143119812\n",
      "[16,   110] loss: 0.2515597641468048\n",
      "[16,   120] loss: 0.026011575013399124\n",
      "[16,   130] loss: 0.002657716628164053\n",
      "[16,   140] loss: 0.09642934054136276\n",
      "[16,   150] loss: 0.09076335281133652\n",
      "[16,   160] loss: 0.05512382462620735\n",
      "[16,   170] loss: 0.014729159884154797\n",
      "[16,   180] loss: 0.1544979363679886\n",
      "[16,   190] loss: 0.01868748851120472\n",
      "[16,   200] loss: 0.011274691671133041\n",
      "[16,   210] loss: 0.005112763028591871\n",
      "[16,   220] loss: 0.17447464168071747\n",
      "[16,   230] loss: 0.03466147184371948\n",
      "[16,   240] loss: 0.17772389948368073\n",
      "[16,   250] loss: 0.0021584827918559313\n",
      "[16,   260] loss: 0.021738043054938316\n",
      "[16,   270] loss: 0.010699600912630558\n",
      "[16,   280] loss: 0.07091167569160461\n",
      "[16,   290] loss: 0.018798833712935448\n",
      "[16,   300] loss: 0.06964749097824097\n",
      "[16,   310] loss: 0.060910217463970184\n",
      "[16,   320] loss: 0.08029265701770782\n",
      "[16,   330] loss: 0.01009397767484188\n",
      "[16,   340] loss: 0.005256645381450653\n",
      "[16,   350] loss: 0.05212683230638504\n",
      "[16,   360] loss: 0.07592641562223434\n",
      "[16,   370] loss: 0.04667786881327629\n",
      "[16,   380] loss: 0.015129896812140942\n",
      "[16,   390] loss: 0.011654422618448734\n",
      "[16,   400] loss: 0.20799396932125092\n",
      "[16,   410] loss: 0.024869050830602646\n",
      "[16,   420] loss: 0.019627869129180908\n",
      "[16,   430] loss: 0.025576550513505936\n",
      "[16,   440] loss: 0.00985478051006794\n",
      "[16,   450] loss: 0.036739859730005264\n",
      "[16,   460] loss: 0.5531293749809265\n",
      "[16,   470] loss: 0.004612350836396217\n",
      "[16,   480] loss: 0.03271994739770889\n",
      "[16,   490] loss: 0.005108639132231474\n",
      "[16,   500] loss: 0.10783417522907257\n",
      "[16,   510] loss: 0.025943543761968613\n",
      "[16,   520] loss: 0.036451611667871475\n",
      "[16,   530] loss: 0.05623354762792587\n",
      "[16,   540] loss: 0.06375650316476822\n",
      "[16,   550] loss: 0.009489718824625015\n",
      "[16,   560] loss: 0.03990452364087105\n",
      "[16,   570] loss: 0.029558099806308746\n",
      "[16,   580] loss: 0.10569682717323303\n",
      "[16,   590] loss: 0.01261631865054369\n",
      "[16,   600] loss: 0.24439793825149536\n",
      "[16,   610] loss: 0.14851652085781097\n",
      "[16,   620] loss: 0.06260395795106888\n",
      "[16,   630] loss: 0.4627593457698822\n",
      "[16,   640] loss: 0.02638562023639679\n",
      "[16,   650] loss: 0.07161197811365128\n",
      "[16,   660] loss: 0.12537431716918945\n",
      "[16,   670] loss: 0.027928313240408897\n",
      "[16,   680] loss: 0.06311845034360886\n",
      "[16,   690] loss: 0.018140636384487152\n",
      "[16,   700] loss: 0.034059129655361176\n",
      "[16,   710] loss: 0.9479185342788696\n",
      "[16,   720] loss: 0.02312607318162918\n",
      "[16,   730] loss: 0.004400050267577171\n",
      "[16,   740] loss: 0.015367700718343258\n",
      "[16,   750] loss: 0.41618090867996216\n",
      "[16,   760] loss: 0.1733294576406479\n",
      "[16,   770] loss: 0.0680646002292633\n",
      "[16,   780] loss: 0.011476375162601471\n",
      "[16,   790] loss: 0.07750754058361053\n",
      "[16,   800] loss: 0.040590982884168625\n",
      "[16,   810] loss: 0.07492558658123016\n",
      "[16,   820] loss: 0.06067634001374245\n",
      "[16,   830] loss: 0.4482332170009613\n",
      "[16,   840] loss: 0.035941798239946365\n",
      "[16,   850] loss: 0.07831498235464096\n",
      "[16,   860] loss: 0.0045886714942753315\n",
      "[16,   870] loss: 0.016327304765582085\n",
      "[16,   880] loss: 0.021211687475442886\n",
      "[16,   890] loss: 0.035895153880119324\n",
      "[16,   900] loss: 0.11461731791496277\n",
      "[16,   910] loss: 0.09732925146818161\n",
      "[16,   920] loss: 0.23193833231925964\n",
      "[16,   930] loss: 0.04553520679473877\n",
      "[16,   940] loss: 0.051851190626621246\n",
      "[16,   950] loss: 0.010703901760280132\n",
      "[16,   960] loss: 0.01153510995209217\n",
      "[16,   970] loss: 0.08560411632061005\n",
      "[16,   980] loss: 0.06546621769666672\n",
      "[16,   990] loss: 0.0013237118255347013\n",
      "[16,  1000] loss: 0.03571362420916557\n",
      "[16,  1010] loss: 0.07345829159021378\n",
      "Got 7833 / 8108 with accuracy: 96.60828811050814%\n",
      "Validating in progress\n",
      "[16,    10] loss: 0.001849562511779368\n",
      "[16,    20] loss: 0.013356613926589489\n",
      "[16,    30] loss: 0.010768244974315166\n",
      "[16,    40] loss: 0.042116064578294754\n",
      "[16,    50] loss: 0.001330556464381516\n",
      "[16,    60] loss: 0.006917709484696388\n",
      "[16,    70] loss: 0.0798678770661354\n",
      "[16,    80] loss: 0.016114071011543274\n",
      "[16,    90] loss: 0.02200457826256752\n",
      "[16,   100] loss: 0.0017515885410830379\n",
      "[16,   110] loss: 0.010342871770262718\n",
      "[16,   120] loss: 0.022309478372335434\n",
      "[16,   130] loss: 0.020451735705137253\n",
      "[16,   140] loss: 0.059809908270835876\n",
      "[16,   150] loss: 0.01131551992148161\n",
      "[16,   160] loss: 0.0040261573158204556\n",
      "[16,   170] loss: 0.000547119474504143\n",
      "[16,   180] loss: 0.03454111889004707\n",
      "[16,   190] loss: 0.019297804683446884\n",
      "[16,   200] loss: 0.008922116830945015\n",
      "[16,   210] loss: 0.0018761965911835432\n",
      "[16,   220] loss: 0.006220247130841017\n",
      "[16,   230] loss: 0.001619798131287098\n",
      "[16,   240] loss: 0.002802056958898902\n",
      "[16,   250] loss: 0.02205214463174343\n",
      "[16,   260] loss: 0.02408915013074875\n",
      "[16,   270] loss: 0.003643132746219635\n",
      "[16,   280] loss: 0.0384686253964901\n",
      "[16,   290] loss: 0.013137617148458958\n",
      "Got 2312 / 2315 with accuracy: 99.87041036717062%\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "Training in progress\n",
      "[17,    10] loss: 0.34996819496154785\n",
      "[17,    20] loss: 0.07597355544567108\n",
      "[17,    30] loss: 0.056639209389686584\n",
      "[17,    40] loss: 0.20562611520290375\n",
      "[17,    50] loss: 0.1652861088514328\n",
      "[17,    60] loss: 0.0438232347369194\n",
      "[17,    70] loss: 0.0756644606590271\n",
      "[17,    80] loss: 0.06498774886131287\n",
      "[17,    90] loss: 0.021408848464488983\n",
      "[17,   100] loss: 0.007603435777127743\n",
      "[17,   110] loss: 0.09582745283842087\n",
      "[17,   120] loss: 0.11517633497714996\n",
      "[17,   130] loss: 0.2628352642059326\n",
      "[17,   140] loss: 0.07070861011743546\n",
      "[17,   150] loss: 0.01921069249510765\n",
      "[17,   160] loss: 0.05892004445195198\n",
      "[17,   170] loss: 0.06104296073317528\n",
      "[17,   180] loss: 0.021117128431797028\n",
      "[17,   190] loss: 0.4544414281845093\n",
      "[17,   200] loss: 0.19567495584487915\n",
      "[17,   210] loss: 0.10037446022033691\n",
      "[17,   220] loss: 0.0791969895362854\n",
      "[17,   230] loss: 0.06850840151309967\n",
      "[17,   240] loss: 0.349206805229187\n",
      "[17,   250] loss: 0.01134641282260418\n",
      "[17,   260] loss: 0.3601953089237213\n",
      "[17,   270] loss: 0.4839977025985718\n",
      "[17,   280] loss: 0.24138225615024567\n",
      "[17,   290] loss: 0.016782883554697037\n",
      "[17,   300] loss: 0.5281305909156799\n",
      "[17,   310] loss: 0.25061264634132385\n",
      "[17,   320] loss: 0.2505893111228943\n",
      "[17,   330] loss: 0.09829922020435333\n",
      "[17,   340] loss: 0.026080820709466934\n",
      "[17,   350] loss: 0.04993105307221413\n",
      "[17,   360] loss: 0.8667730093002319\n",
      "[17,   370] loss: 0.06286011636257172\n",
      "[17,   380] loss: 0.00200846279039979\n",
      "[17,   390] loss: 0.05730259418487549\n",
      "[17,   400] loss: 0.12424174696207047\n",
      "[17,   410] loss: 0.8035581707954407\n",
      "[17,   420] loss: 0.053782571107149124\n",
      "[17,   430] loss: 0.349602073431015\n",
      "[17,   440] loss: 0.12017014622688293\n",
      "[17,   450] loss: 0.007653712294995785\n",
      "[17,   460] loss: 0.03210745006799698\n",
      "[17,   470] loss: 0.1514403373003006\n",
      "[17,   480] loss: 0.06327873468399048\n",
      "[17,   490] loss: 0.0038255809340626\n",
      "[17,   500] loss: 0.011413696222007275\n",
      "[17,   510] loss: 0.05804811418056488\n",
      "[17,   520] loss: 0.050224218517541885\n",
      "[17,   530] loss: 0.023927433416247368\n",
      "[17,   540] loss: 0.6136744022369385\n",
      "[17,   550] loss: 0.22182950377464294\n",
      "[17,   560] loss: 0.030291592702269554\n",
      "[17,   570] loss: 0.1139889657497406\n",
      "[17,   580] loss: 0.20111016929149628\n",
      "[17,   590] loss: 0.05402100831270218\n",
      "[17,   600] loss: 0.28870248794555664\n",
      "[17,   610] loss: 0.05256349593400955\n",
      "[17,   620] loss: 0.17492641508579254\n",
      "[17,   630] loss: 0.09791721403598785\n",
      "[17,   640] loss: 0.08151086419820786\n",
      "[17,   650] loss: 0.22691255807876587\n",
      "[17,   660] loss: 0.46315523982048035\n",
      "[17,   670] loss: 0.09323617070913315\n",
      "[17,   680] loss: 0.00445452518761158\n",
      "[17,   690] loss: 0.007283331360667944\n",
      "[17,   700] loss: 0.24000655114650726\n",
      "[17,   710] loss: 0.06453455984592438\n",
      "[17,   720] loss: 0.005236435681581497\n",
      "[17,   730] loss: 0.040909651666879654\n",
      "[17,   740] loss: 0.040349461138248444\n",
      "[17,   750] loss: 0.7761328220367432\n",
      "[17,   760] loss: 0.047302018851041794\n",
      "[17,   770] loss: 0.02359173074364662\n",
      "[17,   780] loss: 0.026443568989634514\n",
      "[17,   790] loss: 0.08879095315933228\n",
      "[17,   800] loss: 0.1548500657081604\n",
      "[17,   810] loss: 0.014753622934222221\n",
      "[17,   820] loss: 0.25103944540023804\n",
      "[17,   830] loss: 0.0088993264362216\n",
      "[17,   840] loss: 0.027118364349007607\n",
      "[17,   850] loss: 0.1393190324306488\n",
      "[17,   860] loss: 0.026670824736356735\n",
      "[17,   870] loss: 0.02310919016599655\n",
      "[17,   880] loss: 0.06866563856601715\n",
      "[17,   890] loss: 0.003071096260100603\n",
      "[17,   900] loss: 0.03946038335561752\n",
      "[17,   910] loss: 0.0348944291472435\n",
      "[17,   920] loss: 0.11824256926774979\n",
      "[17,   930] loss: 0.10755826532840729\n",
      "[17,   940] loss: 0.013773716986179352\n",
      "[17,   950] loss: 0.15807101130485535\n",
      "[17,   960] loss: 0.052121423184871674\n",
      "[17,   970] loss: 0.4717044234275818\n",
      "[17,   980] loss: 0.07706557214260101\n",
      "[17,   990] loss: 0.014076542109251022\n",
      "[17,  1000] loss: 0.2579055428504944\n",
      "[17,  1010] loss: 0.015193333849310875\n",
      "Got 7820 / 8108 with accuracy: 96.44795263936852%\n",
      "Validating in progress\n",
      "[17,    10] loss: 0.004411616362631321\n",
      "[17,    20] loss: 0.02437416836619377\n",
      "[17,    30] loss: 0.00948163028806448\n",
      "[17,    40] loss: 0.0005636246642097831\n",
      "[17,    50] loss: 0.004875327926129103\n",
      "[17,    60] loss: 0.01207960769534111\n",
      "[17,    70] loss: 0.004643729887902737\n",
      "[17,    80] loss: 0.0063659013248980045\n",
      "[17,    90] loss: 0.007573834620416164\n",
      "[17,   100] loss: 0.023650864139199257\n",
      "[17,   110] loss: 0.008869612589478493\n",
      "[17,   120] loss: 0.018558653071522713\n",
      "[17,   130] loss: 0.0014851922169327736\n",
      "[17,   140] loss: 0.03024810552597046\n",
      "[17,   150] loss: 0.15334296226501465\n",
      "[17,   160] loss: 0.026766948401927948\n",
      "[17,   170] loss: 0.01249660924077034\n",
      "[17,   180] loss: 0.035281307995319366\n",
      "[17,   190] loss: 0.04811551049351692\n",
      "[17,   200] loss: 0.010311473160982132\n",
      "[17,   210] loss: 0.006534048356115818\n",
      "[17,   220] loss: 0.024025818333029747\n",
      "[17,   230] loss: 0.01624818705022335\n",
      "[17,   240] loss: 0.002397939097136259\n",
      "[17,   250] loss: 0.017457302659749985\n",
      "[17,   260] loss: 0.007333672605454922\n",
      "[17,   270] loss: 0.004857624415308237\n",
      "[17,   280] loss: 0.0015594251453876495\n",
      "[17,   290] loss: 0.019833920523524284\n",
      "Got 2312 / 2315 with accuracy: 99.87041036717062%\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "Training in progress\n",
      "[18,    10] loss: 0.057946473360061646\n",
      "[18,    20] loss: 0.0611872524023056\n",
      "[18,    30] loss: 0.015318146906793118\n",
      "[18,    40] loss: 0.007504090666770935\n",
      "[18,    50] loss: 0.04404592141509056\n",
      "[18,    60] loss: 0.3639356791973114\n",
      "[18,    70] loss: 0.022275861352682114\n",
      "[18,    80] loss: 0.07348699122667313\n",
      "[18,    90] loss: 0.05385173112154007\n",
      "[18,   100] loss: 0.016789093613624573\n",
      "[18,   110] loss: 0.09943529218435287\n",
      "[18,   120] loss: 0.02378867380321026\n",
      "[18,   130] loss: 0.1624557226896286\n",
      "[18,   140] loss: 0.014213227666914463\n",
      "[18,   150] loss: 0.006658968515694141\n",
      "[18,   160] loss: 0.227153018116951\n",
      "[18,   170] loss: 0.057419244199991226\n",
      "[18,   180] loss: 0.007909510284662247\n",
      "[18,   190] loss: 0.15362092852592468\n",
      "[18,   200] loss: 0.009390715509653091\n",
      "[18,   210] loss: 0.06477285921573639\n",
      "[18,   220] loss: 0.004394610412418842\n",
      "[18,   230] loss: 0.4173608720302582\n",
      "[18,   240] loss: 0.6640217304229736\n",
      "[18,   250] loss: 0.14803200960159302\n",
      "[18,   260] loss: 0.048009518533945084\n",
      "[18,   270] loss: 0.3053598999977112\n",
      "[18,   280] loss: 0.0040221670642495155\n",
      "[18,   290] loss: 0.04538293555378914\n",
      "[18,   300] loss: 0.016629138961434364\n",
      "[18,   310] loss: 0.5550857186317444\n",
      "[18,   320] loss: 0.019441844895482063\n",
      "[18,   330] loss: 0.01803436689078808\n",
      "[18,   340] loss: 0.08140131086111069\n",
      "[18,   350] loss: 0.37352126836776733\n",
      "[18,   360] loss: 0.015720387920737267\n",
      "[18,   370] loss: 0.027372784912586212\n",
      "[18,   380] loss: 0.701422393321991\n",
      "[18,   390] loss: 0.17581681907176971\n",
      "[18,   400] loss: 0.14309799671173096\n",
      "[18,   410] loss: 0.02515704184770584\n",
      "[18,   420] loss: 0.017305945977568626\n",
      "[18,   430] loss: 0.10051897913217545\n",
      "[18,   440] loss: 0.07896719872951508\n",
      "[18,   450] loss: 0.05167817696928978\n",
      "[18,   460] loss: 0.06129695847630501\n",
      "[18,   470] loss: 0.03977575525641441\n",
      "[18,   480] loss: 0.5844413042068481\n",
      "[18,   490] loss: 0.11505157500505447\n",
      "[18,   500] loss: 0.004711384419351816\n",
      "[18,   510] loss: 0.012237929739058018\n",
      "[18,   520] loss: 0.028152313083410263\n",
      "[18,   530] loss: 0.017072001472115517\n",
      "[18,   540] loss: 0.1130545437335968\n",
      "[18,   550] loss: 0.018384315073490143\n",
      "[18,   560] loss: 0.11389916390180588\n",
      "[18,   570] loss: 0.11425457149744034\n",
      "[18,   580] loss: 0.037799812853336334\n",
      "[18,   590] loss: 0.03677397966384888\n",
      "[18,   600] loss: 0.1978633850812912\n",
      "[18,   610] loss: 0.07133416086435318\n",
      "[18,   620] loss: 0.2545525133609772\n",
      "[18,   630] loss: 0.2436811625957489\n",
      "[18,   640] loss: 0.05114781856536865\n",
      "[18,   650] loss: 0.10366610437631607\n",
      "[18,   660] loss: 0.12085657566785812\n",
      "[18,   670] loss: 0.004413304850459099\n",
      "[18,   680] loss: 0.005181365180760622\n",
      "[18,   690] loss: 0.22185713052749634\n",
      "[18,   700] loss: 0.026541776955127716\n",
      "[18,   710] loss: 0.014790517278015614\n",
      "[18,   720] loss: 0.4846465587615967\n",
      "[18,   730] loss: 0.020062386989593506\n",
      "[18,   740] loss: 0.011370348744094372\n",
      "[18,   750] loss: 0.34845441579818726\n",
      "[18,   760] loss: 0.19111286103725433\n",
      "[18,   770] loss: 0.0028107143007218838\n",
      "[18,   780] loss: 0.1848161220550537\n",
      "[18,   790] loss: 0.005262159276753664\n",
      "[18,   800] loss: 0.06753142923116684\n",
      "[18,   810] loss: 0.10776733607053757\n",
      "[18,   820] loss: 0.026807036250829697\n",
      "[18,   830] loss: 0.008553355932235718\n",
      "[18,   840] loss: 0.08864256739616394\n",
      "[18,   850] loss: 0.2151462733745575\n",
      "[18,   860] loss: 0.05606041103601456\n",
      "[18,   870] loss: 0.08725231140851974\n",
      "[18,   880] loss: 0.09791161119937897\n",
      "[18,   890] loss: 0.29725536704063416\n",
      "[18,   900] loss: 0.07155905663967133\n",
      "[18,   910] loss: 0.39339038729667664\n",
      "[18,   920] loss: 0.02954118512570858\n",
      "[18,   930] loss: 0.3047271966934204\n",
      "[18,   940] loss: 0.05962412431836128\n",
      "[18,   950] loss: 0.08852560073137283\n",
      "[18,   960] loss: 0.018787626177072525\n",
      "[18,   970] loss: 0.020360002294182777\n",
      "[18,   980] loss: 0.297816663980484\n",
      "[18,   990] loss: 0.282135009765625\n",
      "[18,  1000] loss: 0.17655302584171295\n",
      "[18,  1010] loss: 0.009460303001105785\n",
      "Got 7846 / 8108 with accuracy: 96.76862358164774%\n",
      "Validating in progress\n",
      "[18,    10] loss: 0.011927246116101742\n",
      "[18,    20] loss: 0.007392111234366894\n",
      "[18,    30] loss: 0.0006771634798496962\n",
      "[18,    40] loss: 0.003891515312716365\n",
      "[18,    50] loss: 0.008023550733923912\n",
      "[18,    60] loss: 0.004007063340395689\n",
      "[18,    70] loss: 0.0026783414650708437\n",
      "[18,    80] loss: 0.011712544597685337\n",
      "[18,    90] loss: 0.008041705936193466\n",
      "[18,   100] loss: 0.012622972019016743\n",
      "[18,   110] loss: 0.0196517463773489\n",
      "[18,   120] loss: 0.002905141329392791\n",
      "[18,   130] loss: 0.018404386937618256\n",
      "[18,   140] loss: 0.025668958202004433\n",
      "[18,   150] loss: 0.008695036172866821\n",
      "[18,   160] loss: 0.016012413427233696\n",
      "[18,   170] loss: 0.0070425416342914104\n",
      "[18,   180] loss: 0.08688978105783463\n",
      "[18,   190] loss: 0.005638801027089357\n",
      "[18,   200] loss: 0.01692241244018078\n",
      "[18,   210] loss: 0.0007093084859661758\n",
      "[18,   220] loss: 0.0023354655131697655\n",
      "[18,   230] loss: 0.001267091603949666\n",
      "[18,   240] loss: 0.011466235853731632\n",
      "[18,   250] loss: 0.003175388090312481\n",
      "[18,   260] loss: 0.0023067693691700697\n",
      "[18,   270] loss: 0.0038185864686965942\n",
      "[18,   280] loss: 0.0004424551734700799\n",
      "[18,   290] loss: 0.0037689912132918835\n",
      "Got 2313 / 2315 with accuracy: 99.91360691144709%\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "Training in progress\n",
      "[19,    10] loss: 0.5133127570152283\n",
      "[19,    20] loss: 0.00899704173207283\n",
      "[19,    30] loss: 0.32467544078826904\n",
      "[19,    40] loss: 0.24548858404159546\n",
      "[19,    50] loss: 0.011270353570580482\n",
      "[19,    60] loss: 0.11824993789196014\n",
      "[19,    70] loss: 0.00928339920938015\n",
      "[19,    80] loss: 0.0015741429524496198\n",
      "[19,    90] loss: 0.11345008760690689\n",
      "[19,   100] loss: 0.07966858893632889\n",
      "[19,   110] loss: 0.12035547196865082\n",
      "[19,   120] loss: 0.006893870420753956\n",
      "[19,   130] loss: 0.10364773124456406\n",
      "[19,   140] loss: 0.05353613942861557\n",
      "[19,   150] loss: 0.02333424985408783\n",
      "[19,   160] loss: 0.023319769650697708\n",
      "[19,   170] loss: 0.1333511769771576\n",
      "[19,   180] loss: 0.08809062093496323\n",
      "[19,   190] loss: 0.030301110818982124\n",
      "[19,   200] loss: 0.035814687609672546\n",
      "[19,   210] loss: 0.01576630026102066\n",
      "[19,   220] loss: 0.05768946558237076\n",
      "[19,   230] loss: 0.14051088690757751\n",
      "[19,   240] loss: 0.10785721987485886\n",
      "[19,   250] loss: 0.017994118854403496\n",
      "[19,   260] loss: 0.057370997965335846\n",
      "[19,   270] loss: 0.03555991128087044\n",
      "[19,   280] loss: 0.04520602524280548\n",
      "[19,   290] loss: 0.06617629528045654\n",
      "[19,   300] loss: 0.2405318021774292\n",
      "[19,   310] loss: 0.12385719269514084\n",
      "[19,   320] loss: 0.16242599487304688\n",
      "[19,   330] loss: 0.19120505452156067\n",
      "[19,   340] loss: 0.029887283220887184\n",
      "[19,   350] loss: 0.04066265746951103\n",
      "[19,   360] loss: 0.036691680550575256\n",
      "[19,   370] loss: 0.017721125856041908\n",
      "[19,   380] loss: 0.12182999402284622\n",
      "[19,   390] loss: 0.013271113857626915\n",
      "[19,   400] loss: 0.010991762392222881\n",
      "[19,   410] loss: 0.008554804138839245\n",
      "[19,   420] loss: 0.08846459537744522\n",
      "[19,   430] loss: 0.007700642570853233\n",
      "[19,   440] loss: 0.0036246206145733595\n",
      "[19,   450] loss: 0.04806605353951454\n",
      "[19,   460] loss: 0.002443331526592374\n",
      "[19,   470] loss: 0.20573876798152924\n",
      "[19,   480] loss: 0.04956737533211708\n",
      "[19,   490] loss: 0.24614395201206207\n",
      "[19,   500] loss: 0.07670674473047256\n",
      "[19,   510] loss: 0.279261976480484\n",
      "[19,   520] loss: 0.0027824111748486757\n",
      "[19,   530] loss: 0.0057515427470207214\n",
      "[19,   540] loss: 0.050703585147857666\n",
      "[19,   550] loss: 0.0035711259115487337\n",
      "[19,   560] loss: 0.3449324071407318\n",
      "[19,   570] loss: 0.015698762610554695\n",
      "[19,   580] loss: 0.002193029271438718\n",
      "[19,   590] loss: 0.01740712858736515\n",
      "[19,   600] loss: 0.4814862310886383\n",
      "[19,   610] loss: 0.1995614767074585\n",
      "[19,   620] loss: 0.2636633515357971\n",
      "[19,   630] loss: 0.022835787385702133\n",
      "[19,   640] loss: 0.09941808134317398\n",
      "[19,   650] loss: 0.011413034982979298\n",
      "[19,   660] loss: 0.12250036746263504\n",
      "[19,   670] loss: 0.005169984418898821\n",
      "[19,   680] loss: 0.00622951565310359\n",
      "[19,   690] loss: 0.04238871484994888\n",
      "[19,   700] loss: 0.06878270208835602\n",
      "[19,   710] loss: 0.018887609243392944\n",
      "[19,   720] loss: 0.18215149641036987\n",
      "[19,   730] loss: 0.01758498139679432\n",
      "[19,   740] loss: 0.7590060234069824\n",
      "[19,   750] loss: 0.06038041040301323\n",
      "[19,   760] loss: 0.005430267192423344\n",
      "[19,   770] loss: 0.010498275980353355\n",
      "[19,   780] loss: 0.005183126777410507\n",
      "[19,   790] loss: 0.002756861736997962\n",
      "[19,   800] loss: 0.01510148961097002\n",
      "[19,   810] loss: 0.008941896259784698\n",
      "[19,   820] loss: 0.04098173603415489\n",
      "[19,   830] loss: 0.021391384303569794\n",
      "[19,   840] loss: 0.011930180713534355\n",
      "[19,   850] loss: 0.10350077599287033\n",
      "[19,   860] loss: 0.011099682189524174\n",
      "[19,   870] loss: 0.06580224633216858\n",
      "[19,   880] loss: 0.011330495588481426\n",
      "[19,   890] loss: 0.01357351616024971\n",
      "[19,   900] loss: 0.025890477001667023\n",
      "[19,   910] loss: 0.018416166305541992\n",
      "[19,   920] loss: 0.01986263133585453\n",
      "[19,   930] loss: 0.03741135820746422\n",
      "[19,   940] loss: 0.003970876801759005\n",
      "[19,   950] loss: 0.0364307202398777\n",
      "[19,   960] loss: 0.007025499362498522\n",
      "[19,   970] loss: 0.14369603991508484\n",
      "[19,   980] loss: 0.020664548501372337\n",
      "[19,   990] loss: 0.14408107101917267\n",
      "[19,  1000] loss: 0.2818271517753601\n",
      "[19,  1010] loss: 0.03196241334080696\n",
      "Got 7857 / 8108 with accuracy: 96.90429205722742%\n",
      "Validating in progress\n",
      "[19,    10] loss: 0.0023172846995294094\n",
      "[19,    20] loss: 0.0016041588969528675\n",
      "[19,    30] loss: 0.017978159710764885\n",
      "[19,    40] loss: 0.0025823006872087717\n",
      "[19,    50] loss: 0.010143118910491467\n",
      "[19,    60] loss: 0.021160786971449852\n",
      "[19,    70] loss: 0.003401373513042927\n",
      "[19,    80] loss: 0.037061143666505814\n",
      "[19,    90] loss: 0.04292038083076477\n",
      "[19,   100] loss: 0.007017965894192457\n",
      "[19,   110] loss: 0.0019242075504735112\n",
      "[19,   120] loss: 0.020177101716399193\n",
      "[19,   130] loss: 0.001207083580084145\n",
      "[19,   140] loss: 0.0013102652737870812\n",
      "[19,   150] loss: 0.003376802196726203\n",
      "[19,   160] loss: 0.0022522048093378544\n",
      "[19,   170] loss: 0.011541254818439484\n",
      "[19,   180] loss: 0.005731351673603058\n",
      "[19,   190] loss: 0.004561979323625565\n",
      "[19,   200] loss: 0.011263327673077583\n",
      "[19,   210] loss: 0.009926819242537022\n",
      "[19,   220] loss: 0.010806498117744923\n",
      "[19,   230] loss: 0.008521199226379395\n",
      "[19,   240] loss: 0.013323160819709301\n",
      "[19,   250] loss: 0.003243673127144575\n",
      "[19,   260] loss: 0.0018138573504984379\n",
      "[19,   270] loss: 0.008734769187867641\n",
      "[19,   280] loss: 0.026898473501205444\n",
      "[19,   290] loss: 0.010652938857674599\n",
      "Got 2314 / 2315 with accuracy: 99.95680345572354%\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "Training in progress\n",
      "[20,    10] loss: 0.017863305285573006\n",
      "[20,    20] loss: 0.011850404553115368\n",
      "[20,    30] loss: 0.11792103201150894\n",
      "[20,    40] loss: 0.01145569235086441\n",
      "[20,    50] loss: 0.1547253131866455\n",
      "[20,    60] loss: 0.0036785949487239122\n",
      "[20,    70] loss: 0.04038653150200844\n",
      "[20,    80] loss: 0.02167365886271\n",
      "[20,    90] loss: 0.010027622804045677\n",
      "[20,   100] loss: 0.008156150579452515\n",
      "[20,   110] loss: 0.22039571404457092\n",
      "[20,   120] loss: 0.076520174741745\n",
      "[20,   130] loss: 0.010763249360024929\n",
      "[20,   140] loss: 0.10816603899002075\n",
      "[20,   150] loss: 0.020180199295282364\n",
      "[20,   160] loss: 0.01900636777281761\n",
      "[20,   170] loss: 0.004855319857597351\n",
      "[20,   180] loss: 0.012918714433908463\n",
      "[20,   190] loss: 0.012230209074914455\n",
      "[20,   200] loss: 0.0094936927780509\n",
      "[20,   210] loss: 0.33575618267059326\n",
      "[20,   220] loss: 0.12867112457752228\n",
      "[20,   230] loss: 0.027203194797039032\n",
      "[20,   240] loss: 0.012146568857133389\n",
      "[20,   250] loss: 0.0992865189909935\n",
      "[20,   260] loss: 0.018365949392318726\n",
      "[20,   270] loss: 0.00935314130038023\n",
      "[20,   280] loss: 0.009224940091371536\n",
      "[20,   290] loss: 0.11830339580774307\n",
      "[20,   300] loss: 0.012197647243738174\n",
      "[20,   310] loss: 0.04572824761271477\n",
      "[20,   320] loss: 0.012687270529568195\n",
      "[20,   330] loss: 0.010711616836488247\n",
      "[20,   340] loss: 0.01670764572918415\n",
      "[20,   350] loss: 0.049593888223171234\n",
      "[20,   360] loss: 0.12807056307792664\n",
      "[20,   370] loss: 0.014397187158465385\n",
      "[20,   380] loss: 0.026473671197891235\n",
      "[20,   390] loss: 0.03755979239940643\n",
      "[20,   400] loss: 0.005395114421844482\n",
      "[20,   410] loss: 0.06426998972892761\n",
      "[20,   420] loss: 0.1705375760793686\n",
      "[20,   430] loss: 0.2119903713464737\n",
      "[20,   440] loss: 0.02416263334453106\n",
      "[20,   450] loss: 0.011619480326771736\n",
      "[20,   460] loss: 0.03868676349520683\n",
      "[20,   470] loss: 0.012537623755633831\n",
      "[20,   480] loss: 0.00857157539576292\n",
      "[20,   490] loss: 0.26057571172714233\n",
      "[20,   500] loss: 0.245686337351799\n",
      "[20,   510] loss: 0.027840204536914825\n",
      "[20,   520] loss: 0.003291667439043522\n",
      "[20,   530] loss: 0.1821521818637848\n",
      "[20,   540] loss: 0.3172823488712311\n",
      "[20,   550] loss: 0.06742428243160248\n",
      "[20,   560] loss: 0.05960879474878311\n",
      "[20,   570] loss: 0.3281031548976898\n",
      "[20,   580] loss: 0.05824759230017662\n",
      "[20,   590] loss: 0.0053760153241455555\n",
      "[20,   600] loss: 0.02846849150955677\n",
      "[20,   610] loss: 0.49462342262268066\n",
      "[20,   620] loss: 0.24017000198364258\n",
      "[20,   630] loss: 0.03961808979511261\n",
      "[20,   640] loss: 0.18912845849990845\n",
      "[20,   650] loss: 0.0179065503180027\n",
      "[20,   660] loss: 0.08152332156896591\n",
      "[20,   670] loss: 0.006038881838321686\n",
      "[20,   680] loss: 0.4168505072593689\n",
      "[20,   690] loss: 0.010098732076585293\n",
      "[20,   700] loss: 0.04540938511490822\n",
      "[20,   710] loss: 0.4436410367488861\n",
      "[20,   720] loss: 0.04409538954496384\n",
      "[20,   730] loss: 0.03522544726729393\n",
      "[20,   740] loss: 0.30587464570999146\n",
      "[20,   750] loss: 0.020076753571629524\n",
      "[20,   760] loss: 0.058222658932209015\n",
      "[20,   770] loss: 0.014502143487334251\n",
      "[20,   780] loss: 0.04293016716837883\n",
      "[20,   790] loss: 0.01258438266813755\n",
      "[20,   800] loss: 0.3086717426776886\n",
      "[20,   810] loss: 0.008925857953727245\n",
      "[20,   820] loss: 0.01230870746076107\n",
      "[20,   830] loss: 0.0349317267537117\n",
      "[20,   840] loss: 0.6350723505020142\n",
      "[20,   850] loss: 0.12593628466129303\n",
      "[20,   860] loss: 0.023624159395694733\n",
      "[20,   870] loss: 0.08264213800430298\n",
      "[20,   880] loss: 0.10087389498949051\n",
      "[20,   890] loss: 0.2018219530582428\n",
      "[20,   900] loss: 0.15436387062072754\n",
      "[20,   910] loss: 0.0068263644352555275\n",
      "[20,   920] loss: 0.08987623453140259\n",
      "[20,   930] loss: 0.02158951759338379\n",
      "[20,   940] loss: 0.41740620136260986\n",
      "[20,   950] loss: 0.011540614999830723\n",
      "[20,   960] loss: 0.23715786635875702\n",
      "[20,   970] loss: 0.0562778003513813\n",
      "[20,   980] loss: 0.013383856043219566\n",
      "[20,   990] loss: 0.0014376402832567692\n",
      "[20,  1000] loss: 0.027864862233400345\n",
      "[20,  1010] loss: 0.02159127965569496\n",
      "Got 7853 / 8108 with accuracy: 96.85495806610754%\n",
      "Validating in progress\n",
      "[20,    10] loss: 0.004314769990742207\n",
      "[20,    20] loss: 0.014594672247767448\n",
      "[20,    30] loss: 0.03169294819235802\n",
      "[20,    40] loss: 0.014366060495376587\n",
      "[20,    50] loss: 0.004064807202666998\n",
      "[20,    60] loss: 0.031471431255340576\n",
      "[20,    70] loss: 0.0049681635573506355\n",
      "[20,    80] loss: 0.0005473632481880486\n",
      "[20,    90] loss: 0.011993004940450191\n",
      "[20,   100] loss: 0.002778800204396248\n",
      "[20,   110] loss: 0.008558503352105618\n",
      "[20,   120] loss: 0.010794396512210369\n",
      "[20,   130] loss: 0.01058365497738123\n",
      "[20,   140] loss: 0.001981416717171669\n",
      "[20,   150] loss: 0.0016368504147976637\n",
      "[20,   160] loss: 0.04087434709072113\n",
      "[20,   170] loss: 0.00757634686306119\n",
      "[20,   180] loss: 0.011094748973846436\n",
      "[20,   190] loss: 0.011577729135751724\n",
      "[20,   200] loss: 0.0005888952873647213\n",
      "[20,   210] loss: 0.01618717424571514\n",
      "[20,   220] loss: 0.04919598251581192\n",
      "[20,   230] loss: 0.016928846016526222\n",
      "[20,   240] loss: 0.007276366464793682\n",
      "[20,   250] loss: 0.024648897349834442\n",
      "[20,   260] loss: 0.017476094886660576\n",
      "[20,   270] loss: 0.0026371460407972336\n",
      "[20,   280] loss: 0.0023882933892309666\n",
      "[20,   290] loss: 0.004410948138684034\n",
      "Got 2314 / 2315 with accuracy: 99.95680345572354%\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "Training in progress\n",
      "[21,    10] loss: 0.009773658588528633\n",
      "[21,    20] loss: 0.007634351029992104\n",
      "[21,    30] loss: 0.009422533214092255\n",
      "[21,    40] loss: 0.035609833896160126\n",
      "[21,    50] loss: 0.002368386136367917\n",
      "[21,    60] loss: 0.051308851689100266\n",
      "[21,    70] loss: 0.012042606249451637\n",
      "[21,    80] loss: 0.02785339206457138\n",
      "[21,    90] loss: 0.09198816865682602\n",
      "[21,   100] loss: 0.06377021223306656\n",
      "[21,   110] loss: 0.17681840062141418\n",
      "[21,   120] loss: 0.10709689557552338\n",
      "[21,   130] loss: 0.029189150780439377\n",
      "[21,   140] loss: 0.09135498851537704\n",
      "[21,   150] loss: 0.06736094504594803\n",
      "[21,   160] loss: 0.02227993682026863\n",
      "[21,   170] loss: 0.08155857771635056\n",
      "[21,   180] loss: 0.0021774538327008486\n",
      "[21,   190] loss: 0.016651151701807976\n",
      "[21,   200] loss: 0.0337313748896122\n",
      "[21,   210] loss: 0.10303812474012375\n",
      "[21,   220] loss: 0.00857324618846178\n",
      "[21,   230] loss: 0.005418546963483095\n",
      "[21,   240] loss: 0.023484352976083755\n",
      "[21,   250] loss: 0.026503954082727432\n",
      "[21,   260] loss: 0.03166327252984047\n",
      "[21,   270] loss: 0.012303122319281101\n",
      "[21,   280] loss: 0.20904728770256042\n",
      "[21,   290] loss: 0.10598763078451157\n",
      "[21,   300] loss: 0.029399249702692032\n",
      "[21,   310] loss: 0.21774601936340332\n",
      "[21,   320] loss: 0.14167602360248566\n",
      "[21,   330] loss: 0.027925074100494385\n",
      "[21,   340] loss: 0.18484024703502655\n",
      "[21,   350] loss: 0.02704019472002983\n",
      "[21,   360] loss: 0.027288135141134262\n",
      "[21,   370] loss: 0.005003134720027447\n",
      "[21,   380] loss: 0.03208651393651962\n",
      "[21,   390] loss: 0.004989457316696644\n",
      "[21,   400] loss: 0.15168695151805878\n",
      "[21,   410] loss: 0.0805763229727745\n",
      "[21,   420] loss: 0.09245557337999344\n",
      "[21,   430] loss: 0.24305246770381927\n",
      "[21,   440] loss: 0.003627568716183305\n",
      "[21,   450] loss: 0.014590902253985405\n",
      "[21,   460] loss: 0.21003127098083496\n",
      "[21,   470] loss: 0.10215707868337631\n",
      "[21,   480] loss: 0.40892839431762695\n",
      "[21,   490] loss: 0.05756470561027527\n",
      "[21,   500] loss: 0.002119559096172452\n",
      "[21,   510] loss: 0.0034202265087515116\n",
      "[21,   520] loss: 0.057392820715904236\n",
      "[21,   530] loss: 0.03825101628899574\n",
      "[21,   540] loss: 0.018752526491880417\n",
      "[21,   550] loss: 0.3819129765033722\n",
      "[21,   560] loss: 0.008762426674365997\n",
      "[21,   570] loss: 0.16059084236621857\n",
      "[21,   580] loss: 0.03675210103392601\n",
      "[21,   590] loss: 0.019652843475341797\n",
      "[21,   600] loss: 0.03042752854526043\n",
      "[21,   610] loss: 1.0309334993362427\n",
      "[21,   620] loss: 0.13401296734809875\n",
      "[21,   630] loss: 0.0778154656291008\n",
      "[21,   640] loss: 0.21945233643054962\n",
      "[21,   650] loss: 0.0031901667825877666\n",
      "[21,   660] loss: 0.14214611053466797\n",
      "[21,   670] loss: 0.00826349202543497\n",
      "[21,   680] loss: 0.08726401627063751\n",
      "[21,   690] loss: 0.022805090993642807\n",
      "[21,   700] loss: 0.00541670061647892\n",
      "[21,   710] loss: 0.0030027262400835752\n",
      "[21,   720] loss: 0.10174475610256195\n",
      "[21,   730] loss: 0.005425370763987303\n",
      "[21,   740] loss: 0.10481368005275726\n",
      "[21,   750] loss: 0.1534023880958557\n",
      "[21,   760] loss: 0.02742111124098301\n",
      "[21,   770] loss: 0.007701953407377005\n",
      "[21,   780] loss: 0.12576636672019958\n",
      "[21,   790] loss: 0.07652932405471802\n",
      "[21,   800] loss: 0.01727548986673355\n",
      "[21,   810] loss: 0.0074922158382833\n",
      "[21,   820] loss: 0.004783598706126213\n",
      "[21,   830] loss: 0.021212581545114517\n",
      "[21,   840] loss: 0.16678766906261444\n",
      "[21,   850] loss: 0.21340444684028625\n",
      "[21,   860] loss: 0.018228983506560326\n",
      "[21,   870] loss: 0.1502613127231598\n",
      "[21,   880] loss: 0.04079336300492287\n",
      "[21,   890] loss: 0.6284839510917664\n",
      "[21,   900] loss: 0.02700418047606945\n",
      "[21,   910] loss: 0.0048669856041669846\n",
      "[21,   920] loss: 0.02327078767120838\n",
      "[21,   930] loss: 0.00655320193618536\n",
      "[21,   940] loss: 0.09977925568819046\n",
      "[21,   950] loss: 0.11646364629268646\n",
      "[21,   960] loss: 0.28154540061950684\n",
      "[21,   970] loss: 0.006994309835135937\n",
      "[21,   980] loss: 0.03844980150461197\n",
      "[21,   990] loss: 0.04919901862740517\n",
      "[21,  1000] loss: 0.006713066715747118\n",
      "[21,  1010] loss: 0.25091037154197693\n",
      "Got 7859 / 8108 with accuracy: 96.92895905278736%\n",
      "Validating in progress\n",
      "[21,    10] loss: 0.02731555886566639\n",
      "[21,    20] loss: 0.007322520017623901\n",
      "[21,    30] loss: 0.027538375928997993\n",
      "[21,    40] loss: 0.012539100833237171\n",
      "[21,    50] loss: 0.01742745377123356\n",
      "[21,    60] loss: 0.04603896662592888\n",
      "[21,    70] loss: 0.022529473528265953\n",
      "[21,    80] loss: 0.0018166619120165706\n",
      "[21,    90] loss: 0.009806168265640736\n",
      "[21,   100] loss: 0.02069120481610298\n",
      "[21,   110] loss: 0.13230127096176147\n",
      "[21,   120] loss: 0.07824763655662537\n",
      "[21,   130] loss: 0.006772924214601517\n",
      "[21,   140] loss: 0.02229176089167595\n",
      "[21,   150] loss: 0.0062822140753269196\n",
      "[21,   160] loss: 0.043057821691036224\n",
      "[21,   170] loss: 0.0296823401004076\n",
      "[21,   180] loss: 0.0018273191526532173\n",
      "[21,   190] loss: 0.0035953251644968987\n",
      "[21,   200] loss: 0.009785478934645653\n",
      "[21,   210] loss: 0.010023212991654873\n",
      "[21,   220] loss: 0.004226636607199907\n",
      "[21,   230] loss: 0.04218802973628044\n",
      "[21,   240] loss: 0.015077337622642517\n",
      "[21,   250] loss: 0.0017276235157623887\n",
      "[21,   260] loss: 0.007409365382045507\n",
      "[21,   270] loss: 0.09627407044172287\n",
      "[21,   280] loss: 0.0019168772269040346\n",
      "[21,   290] loss: 0.008100766688585281\n",
      "Got 2313 / 2315 with accuracy: 99.91360691144709%\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "Training in progress\n",
      "[22,    10] loss: 0.006590281147509813\n",
      "[22,    20] loss: 0.0223415307700634\n",
      "[22,    30] loss: 0.030070427805185318\n",
      "[22,    40] loss: 0.014316809363663197\n",
      "[22,    50] loss: 0.010675875470042229\n",
      "[22,    60] loss: 0.10409891605377197\n",
      "[22,    70] loss: 0.007776567712426186\n",
      "[22,    80] loss: 0.25587379932403564\n",
      "[22,    90] loss: 0.06605275720357895\n",
      "[22,   100] loss: 0.027071906253695488\n",
      "[22,   110] loss: 0.010727083310484886\n",
      "[22,   120] loss: 0.015580329112708569\n",
      "[22,   130] loss: 0.0035612257197499275\n",
      "[22,   140] loss: 0.01607707142829895\n",
      "[22,   150] loss: 0.015537040308117867\n",
      "[22,   160] loss: 0.008741521276533604\n",
      "[22,   170] loss: 0.1110677719116211\n",
      "[22,   180] loss: 0.0023397537879645824\n",
      "[22,   190] loss: 0.009364623576402664\n",
      "[22,   200] loss: 0.49783530831336975\n",
      "[22,   210] loss: 0.0037162143271416426\n",
      "[22,   220] loss: 0.05565394461154938\n",
      "[22,   230] loss: 0.07885338366031647\n",
      "[22,   240] loss: 0.007770981639623642\n",
      "[22,   250] loss: 0.0223736222833395\n",
      "[22,   260] loss: 0.04338015243411064\n",
      "[22,   270] loss: 0.008753721602261066\n",
      "[22,   280] loss: 0.03524945303797722\n",
      "[22,   290] loss: 0.007839592173695564\n",
      "[22,   300] loss: 0.025723526254296303\n",
      "[22,   310] loss: 0.045032721012830734\n",
      "[22,   320] loss: 0.002566636772826314\n",
      "[22,   330] loss: 0.012060865759849548\n",
      "[22,   340] loss: 0.053002163767814636\n",
      "[22,   350] loss: 0.001478912658058107\n",
      "[22,   360] loss: 0.013923443853855133\n",
      "[22,   370] loss: 0.017710281535983086\n",
      "[22,   380] loss: 0.12337004393339157\n",
      "[22,   390] loss: 0.08611342310905457\n",
      "[22,   400] loss: 0.2600819170475006\n",
      "[22,   410] loss: 0.006732551846653223\n",
      "[22,   420] loss: 0.036488912999629974\n",
      "[22,   430] loss: 0.9696323871612549\n",
      "[22,   440] loss: 0.015753818675875664\n",
      "[22,   450] loss: 0.1756560057401657\n",
      "[22,   460] loss: 0.054188814014196396\n",
      "[22,   470] loss: 0.030656449496746063\n",
      "[22,   480] loss: 0.02043696865439415\n",
      "[22,   490] loss: 0.010366166941821575\n",
      "[22,   500] loss: 0.10007541626691818\n",
      "[22,   510] loss: 0.5912305116653442\n",
      "[22,   520] loss: 0.020530354231595993\n",
      "[22,   530] loss: 0.010729571804404259\n",
      "[22,   540] loss: 0.05206216871738434\n",
      "[22,   550] loss: 0.01586797647178173\n",
      "[22,   560] loss: 0.030243290588259697\n",
      "[22,   570] loss: 0.0051817200146615505\n",
      "[22,   580] loss: 0.1096552163362503\n",
      "[22,   590] loss: 0.055844105780124664\n",
      "[22,   600] loss: 0.08212558925151825\n",
      "[22,   610] loss: 0.07342132925987244\n",
      "[22,   620] loss: 0.12042740732431412\n",
      "[22,   630] loss: 0.07130101323127747\n",
      "[22,   640] loss: 0.36496859788894653\n",
      "[22,   650] loss: 0.006980994716286659\n",
      "[22,   660] loss: 0.039675068110227585\n",
      "[22,   670] loss: 0.07745461165904999\n",
      "[22,   680] loss: 0.0688188225030899\n",
      "[22,   690] loss: 0.0036701573990285397\n",
      "[22,   700] loss: 0.0028516300953924656\n",
      "[22,   710] loss: 0.05562082305550575\n",
      "[22,   720] loss: 0.013525133021175861\n",
      "[22,   730] loss: 0.47666335105895996\n",
      "[22,   740] loss: 0.008902051486074924\n",
      "[22,   750] loss: 0.034583717584609985\n",
      "[22,   760] loss: 0.031140461564064026\n",
      "[22,   770] loss: 0.0647624209523201\n",
      "[22,   780] loss: 0.09307359158992767\n",
      "[22,   790] loss: 0.014562221243977547\n",
      "[22,   800] loss: 0.28006821870803833\n",
      "[22,   810] loss: 0.3618807792663574\n",
      "[22,   820] loss: 0.09612666815519333\n",
      "[22,   830] loss: 0.0032278497237712145\n",
      "[22,   840] loss: 0.15646520256996155\n",
      "[22,   850] loss: 0.08564519137144089\n",
      "[22,   860] loss: 0.017199009656906128\n",
      "[22,   870] loss: 0.18946298956871033\n",
      "[22,   880] loss: 0.018063468858599663\n",
      "[22,   890] loss: 0.016503537073731422\n",
      "[22,   900] loss: 0.17784947156906128\n",
      "[22,   910] loss: 0.15350160002708435\n",
      "[22,   920] loss: 0.028341487050056458\n",
      "[22,   930] loss: 0.013473087921738625\n",
      "[22,   940] loss: 1.0424152612686157\n",
      "[22,   950] loss: 0.22855889797210693\n",
      "[22,   960] loss: 0.001527906395494938\n",
      "[22,   970] loss: 0.1342613399028778\n",
      "[22,   980] loss: 0.03674335032701492\n",
      "[22,   990] loss: 0.05433618277311325\n",
      "[22,  1000] loss: 0.011900479905307293\n",
      "[22,  1010] loss: 0.08403240144252777\n",
      "Got 7856 / 8108 with accuracy: 96.89195855944746%\n",
      "Validating in progress\n",
      "[22,    10] loss: 0.046080101281404495\n",
      "[22,    20] loss: 0.005507731810212135\n",
      "[22,    30] loss: 0.00882682204246521\n",
      "[22,    40] loss: 0.013505668379366398\n",
      "[22,    50] loss: 0.009046889841556549\n",
      "[22,    60] loss: 0.0035408991388976574\n",
      "[22,    70] loss: 0.005277988035231829\n",
      "[22,    80] loss: 0.008624251931905746\n",
      "[22,    90] loss: 0.015266438014805317\n",
      "[22,   100] loss: 0.00584015715867281\n",
      "[22,   110] loss: 0.0018125545466318727\n",
      "[22,   120] loss: 0.022066570818424225\n",
      "[22,   130] loss: 0.003553828690201044\n",
      "[22,   140] loss: 0.04058236628770828\n",
      "[22,   150] loss: 0.005065163131803274\n",
      "[22,   160] loss: 0.0047085960395634174\n",
      "[22,   170] loss: 0.011816696263849735\n",
      "[22,   180] loss: 0.02735481970012188\n",
      "[22,   190] loss: 0.008142152801156044\n",
      "[22,   200] loss: 0.011731852777302265\n",
      "[22,   210] loss: 0.008763022720813751\n",
      "[22,   220] loss: 0.0014122085412964225\n",
      "[22,   230] loss: 0.0028783257585018873\n",
      "[22,   240] loss: 0.029775571078062057\n",
      "[22,   250] loss: 0.03785552829504013\n",
      "[22,   260] loss: 0.009971018880605698\n",
      "[22,   270] loss: 0.006203536409884691\n",
      "[22,   280] loss: 0.0027274549938738346\n",
      "[22,   290] loss: 0.029513802379369736\n",
      "Got 2312 / 2315 with accuracy: 99.87041036717062%\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "Training in progress\n",
      "[23,    10] loss: 0.024292411282658577\n",
      "[23,    20] loss: 0.16595762968063354\n",
      "[23,    30] loss: 0.04587991163134575\n",
      "[23,    40] loss: 0.009466107003390789\n",
      "[23,    50] loss: 0.13812501728534698\n",
      "[23,    60] loss: 0.02155309170484543\n",
      "[23,    70] loss: 0.05869646370410919\n",
      "[23,    80] loss: 0.029369881376624107\n",
      "[23,    90] loss: 0.25927916169166565\n",
      "[23,   100] loss: 0.041004478931427\n",
      "[23,   110] loss: 0.021323446184396744\n",
      "[23,   120] loss: 0.15387757122516632\n",
      "[23,   130] loss: 0.012826919555664062\n",
      "[23,   140] loss: 0.04570296034216881\n",
      "[23,   150] loss: 0.02969282492995262\n",
      "[23,   160] loss: 0.004601638298481703\n",
      "[23,   170] loss: 0.06018364429473877\n",
      "[23,   180] loss: 0.008586683310568333\n",
      "[23,   190] loss: 0.013277051039040089\n",
      "[23,   200] loss: 0.06607484072446823\n",
      "[23,   210] loss: 0.07583485543727875\n",
      "[23,   220] loss: 0.0565192811191082\n",
      "[23,   230] loss: 0.012889317236840725\n",
      "[23,   240] loss: 0.14180472493171692\n",
      "[23,   250] loss: 0.12238194048404694\n",
      "[23,   260] loss: 0.020702822133898735\n",
      "[23,   270] loss: 0.0128829600289464\n",
      "[23,   280] loss: 0.00662780087441206\n",
      "[23,   290] loss: 0.0678672194480896\n",
      "[23,   300] loss: 0.3551737368106842\n",
      "[23,   310] loss: 0.18512015044689178\n",
      "[23,   320] loss: 0.021347885951399803\n",
      "[23,   330] loss: 0.09997468441724777\n",
      "[23,   340] loss: 0.06462714821100235\n",
      "[23,   350] loss: 0.24837897717952728\n",
      "[23,   360] loss: 0.006839252542704344\n",
      "[23,   370] loss: 0.020385270938277245\n",
      "[23,   380] loss: 0.002667785855010152\n",
      "[23,   390] loss: 0.006880940403789282\n",
      "[23,   400] loss: 0.006544826552271843\n",
      "[23,   410] loss: 0.008473316207528114\n",
      "[23,   420] loss: 0.031067805364727974\n",
      "[23,   430] loss: 0.05696544796228409\n",
      "[23,   440] loss: 0.3602783679962158\n",
      "[23,   450] loss: 0.045391201972961426\n",
      "[23,   460] loss: 0.003348754486069083\n",
      "[23,   470] loss: 0.15230192244052887\n",
      "[23,   480] loss: 0.010235863737761974\n",
      "[23,   490] loss: 0.0526125393807888\n",
      "[23,   500] loss: 0.01107119396328926\n",
      "[23,   510] loss: 0.0023170658387243748\n",
      "[23,   520] loss: 0.20209260284900665\n",
      "[23,   530] loss: 0.034552957862615585\n",
      "[23,   540] loss: 0.03526540845632553\n",
      "[23,   550] loss: 0.1268857717514038\n",
      "[23,   560] loss: 0.11405448615550995\n",
      "[23,   570] loss: 0.08120505511760712\n",
      "[23,   580] loss: 0.23837341368198395\n",
      "[23,   590] loss: 0.18597929179668427\n",
      "[23,   600] loss: 0.01293334923684597\n",
      "[23,   610] loss: 0.03471602872014046\n",
      "[23,   620] loss: 0.007325067650526762\n",
      "[23,   630] loss: 0.06658709794282913\n",
      "[23,   640] loss: 0.04405660927295685\n",
      "[23,   650] loss: 0.28724750876426697\n",
      "[23,   660] loss: 0.004289761185646057\n",
      "[23,   670] loss: 0.02759946696460247\n",
      "[23,   680] loss: 0.02468465268611908\n",
      "[23,   690] loss: 0.7004969120025635\n",
      "[23,   700] loss: 0.40513506531715393\n",
      "[23,   710] loss: 0.019930269569158554\n",
      "[23,   720] loss: 0.0076791392639279366\n",
      "[23,   730] loss: 0.2541271150112152\n",
      "[23,   740] loss: 0.006663520820438862\n",
      "[23,   750] loss: 0.10271111130714417\n",
      "[23,   760] loss: 0.03994434326887131\n",
      "[23,   770] loss: 0.0051849908195436\n",
      "[23,   780] loss: 0.007879930548369884\n",
      "[23,   790] loss: 0.02981504239141941\n",
      "[23,   800] loss: 0.033328261226415634\n",
      "[23,   810] loss: 0.09270472824573517\n",
      "[23,   820] loss: 0.16386966407299042\n",
      "[23,   830] loss: 0.1745213121175766\n",
      "[23,   840] loss: 0.0071218376979231834\n",
      "[23,   850] loss: 0.022828910499811172\n",
      "[23,   860] loss: 0.012437294237315655\n",
      "[23,   870] loss: 0.022506829351186752\n",
      "[23,   880] loss: 0.013074721209704876\n",
      "[23,   890] loss: 0.01190855074673891\n",
      "[23,   900] loss: 1.5338935852050781\n",
      "[23,   910] loss: 0.11952715367078781\n",
      "[23,   920] loss: 0.0036831777542829514\n",
      "[23,   930] loss: 0.021370960399508476\n",
      "[23,   940] loss: 0.014690466225147247\n",
      "[23,   950] loss: 0.009717557579278946\n",
      "[23,   960] loss: 0.06314461678266525\n",
      "[23,   970] loss: 0.06321579217910767\n",
      "[23,   980] loss: 0.10998542606830597\n",
      "[23,   990] loss: 0.020312955603003502\n",
      "[23,  1000] loss: 0.03293007239699364\n",
      "[23,  1010] loss: 0.06138049438595772\n",
      "Got 7883 / 8108 with accuracy: 97.22496299950666%\n",
      "Validating in progress\n",
      "[23,    10] loss: 0.024804264307022095\n",
      "[23,    20] loss: 0.005751463584601879\n",
      "[23,    30] loss: 0.003313913010060787\n",
      "[23,    40] loss: 0.007838946767151356\n",
      "[23,    50] loss: 0.009854784235358238\n",
      "[23,    60] loss: 0.005776526872068644\n",
      "[23,    70] loss: 0.005111021921038628\n",
      "[23,    80] loss: 0.006735372822731733\n",
      "[23,    90] loss: 0.009471043944358826\n",
      "[23,   100] loss: 0.06453942507505417\n",
      "[23,   110] loss: 0.012252789922058582\n",
      "[23,   120] loss: 0.036284856498241425\n",
      "[23,   130] loss: 0.027659842744469643\n",
      "[23,   140] loss: 0.013192212209105492\n",
      "[23,   150] loss: 0.004938211292028427\n",
      "[23,   160] loss: 0.011155527085065842\n",
      "[23,   170] loss: 0.0008091555209830403\n",
      "[23,   180] loss: 0.022239457815885544\n",
      "[23,   190] loss: 0.013663358986377716\n",
      "[23,   200] loss: 0.022755445912480354\n",
      "[23,   210] loss: 0.0010955827310681343\n",
      "[23,   220] loss: 0.017778849229216576\n",
      "[23,   230] loss: 0.011999418027698994\n",
      "[23,   240] loss: 0.01930040307343006\n",
      "[23,   250] loss: 0.012011525221168995\n",
      "[23,   260] loss: 0.0029813710134476423\n",
      "[23,   270] loss: 0.017627589404582977\n",
      "[23,   280] loss: 0.006126179825514555\n",
      "[23,   290] loss: 0.006826375611126423\n",
      "Got 2315 / 2315 with accuracy: 100.0%\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "Training in progress\n",
      "[24,    10] loss: 0.0054064299911260605\n",
      "[24,    20] loss: 0.5817018747329712\n",
      "[24,    30] loss: 0.2731824219226837\n",
      "[24,    40] loss: 0.007499853614717722\n",
      "[24,    50] loss: 0.006605803035199642\n",
      "[24,    60] loss: 0.11704383790493011\n",
      "[24,    70] loss: 0.5703263878822327\n",
      "[24,    80] loss: 0.4091150462627411\n",
      "[24,    90] loss: 0.11487163603305817\n",
      "[24,   100] loss: 0.00804087147116661\n",
      "[24,   110] loss: 0.08399638533592224\n",
      "[24,   120] loss: 0.012640518136322498\n",
      "[24,   130] loss: 0.05315639078617096\n",
      "[24,   140] loss: 0.04310392215847969\n",
      "[24,   150] loss: 0.006351183168590069\n",
      "[24,   160] loss: 0.029871946200728416\n",
      "[24,   170] loss: 0.19685623049736023\n",
      "[24,   180] loss: 0.05085504427552223\n",
      "[24,   190] loss: 0.12689608335494995\n",
      "[24,   200] loss: 0.05182543396949768\n",
      "[24,   210] loss: 0.05838862806558609\n",
      "[24,   220] loss: 0.006379281170666218\n",
      "[24,   230] loss: 0.05947304517030716\n",
      "[24,   240] loss: 0.01041956152766943\n",
      "[24,   250] loss: 0.21064023673534393\n",
      "[24,   260] loss: 0.0403519831597805\n",
      "[24,   270] loss: 0.06941721588373184\n",
      "[24,   280] loss: 0.479295939207077\n",
      "[24,   290] loss: 0.07536741346120834\n",
      "[24,   300] loss: 0.01919490471482277\n",
      "[24,   310] loss: 0.0039237928576767445\n",
      "[24,   320] loss: 0.016930926591157913\n",
      "[24,   330] loss: 0.06378402560949326\n",
      "[24,   340] loss: 0.032501328736543655\n",
      "[24,   350] loss: 0.013104187324643135\n",
      "[24,   360] loss: 0.18956927955150604\n",
      "[24,   370] loss: 0.010967164300382137\n",
      "[24,   380] loss: 0.019102616235613823\n",
      "[24,   390] loss: 0.04934621974825859\n",
      "[24,   400] loss: 0.2622548043727875\n",
      "[24,   410] loss: 0.015076604671776295\n",
      "[24,   420] loss: 0.04950101301074028\n",
      "[24,   430] loss: 0.03072330355644226\n",
      "[24,   440] loss: 0.01887926086783409\n",
      "[24,   450] loss: 0.7424962520599365\n",
      "[24,   460] loss: 0.014653914608061314\n",
      "[24,   470] loss: 0.05283143371343613\n",
      "[24,   480] loss: 0.03570861369371414\n",
      "[24,   490] loss: 0.004459945019334555\n",
      "[24,   500] loss: 0.3062925934791565\n",
      "[24,   510] loss: 0.018108688294887543\n",
      "[24,   520] loss: 0.005978912115097046\n",
      "[24,   530] loss: 0.23229341208934784\n",
      "[24,   540] loss: 0.07388171553611755\n",
      "[24,   550] loss: 0.007037182338535786\n",
      "[24,   560] loss: 0.04311230033636093\n",
      "[24,   570] loss: 0.0029099751263856888\n",
      "[24,   580] loss: 0.024109160527586937\n",
      "[24,   590] loss: 0.02172250673174858\n",
      "[24,   600] loss: 0.1159338727593422\n",
      "[24,   610] loss: 0.2332623451948166\n",
      "[24,   620] loss: 0.015787627547979355\n",
      "[24,   630] loss: 0.26910921931266785\n",
      "[24,   640] loss: 0.07189517468214035\n",
      "[24,   650] loss: 0.2600293755531311\n",
      "[24,   660] loss: 0.019806494936347008\n",
      "[24,   670] loss: 0.1808026134967804\n",
      "[24,   680] loss: 0.05721026659011841\n",
      "[24,   690] loss: 0.0239338930696249\n",
      "[24,   700] loss: 0.009138315916061401\n",
      "[24,   710] loss: 0.032234709709882736\n",
      "[24,   720] loss: 0.016980387270450592\n",
      "[24,   730] loss: 0.02279396541416645\n",
      "[24,   740] loss: 0.2716938853263855\n",
      "[24,   750] loss: 0.0455554835498333\n",
      "[24,   760] loss: 0.12975068390369415\n",
      "[24,   770] loss: 0.0007993977633304894\n",
      "[24,   780] loss: 0.01796077936887741\n",
      "[24,   790] loss: 0.0059299045242369175\n",
      "[24,   800] loss: 0.04577012360095978\n",
      "[24,   810] loss: 0.014341318979859352\n",
      "[24,   820] loss: 0.004980923607945442\n",
      "[24,   830] loss: 0.04196721315383911\n",
      "[24,   840] loss: 0.02121596410870552\n",
      "[24,   850] loss: 0.06318854540586472\n",
      "[24,   860] loss: 0.008281392976641655\n",
      "[24,   870] loss: 0.3536958396434784\n",
      "[24,   880] loss: 0.14306125044822693\n",
      "[24,   890] loss: 0.23562456667423248\n",
      "[24,   900] loss: 0.008986825123429298\n",
      "[24,   910] loss: 0.2585759460926056\n",
      "[24,   920] loss: 0.704331636428833\n",
      "[24,   930] loss: 0.026815693825483322\n",
      "[24,   940] loss: 0.010387705639004707\n",
      "[24,   950] loss: 0.006778831593692303\n",
      "[24,   960] loss: 0.03671245649456978\n",
      "[24,   970] loss: 0.012890227138996124\n",
      "[24,   980] loss: 0.022119363769888878\n",
      "[24,   990] loss: 0.1447240561246872\n",
      "[24,  1000] loss: 0.01438727043569088\n",
      "[24,  1010] loss: 0.009266365319490433\n",
      "Got 7843 / 8108 with accuracy: 96.73162308830784%\n",
      "Validating in progress\n",
      "[24,    10] loss: 0.031558144837617874\n",
      "[24,    20] loss: 0.005128798075020313\n",
      "[24,    30] loss: 0.013542558066546917\n",
      "[24,    40] loss: 0.003417192492634058\n",
      "[24,    50] loss: 0.03635508939623833\n",
      "[24,    60] loss: 0.0006348994793370366\n",
      "[24,    70] loss: 0.04474397003650665\n",
      "[24,    80] loss: 0.011860625818371773\n",
      "[24,    90] loss: 0.005953876301646233\n",
      "[24,   100] loss: 0.013504978269338608\n",
      "[24,   110] loss: 0.004981348756700754\n",
      "[24,   120] loss: 0.014007719233632088\n",
      "[24,   130] loss: 0.00858786329627037\n",
      "[24,   140] loss: 0.0030034140218049288\n",
      "[24,   150] loss: 0.012644492089748383\n",
      "[24,   160] loss: 0.005019255913794041\n",
      "[24,   170] loss: 0.008473014459013939\n",
      "[24,   180] loss: 0.089373879134655\n",
      "[24,   190] loss: 0.009962120093405247\n",
      "[24,   200] loss: 0.006493867374956608\n",
      "[24,   210] loss: 0.017478616908192635\n",
      "[24,   220] loss: 0.01285745482891798\n",
      "[24,   230] loss: 0.016512807458639145\n",
      "[24,   240] loss: 0.0024730954319238663\n",
      "[24,   250] loss: 0.00961728859692812\n",
      "[24,   260] loss: 0.0011602041777223349\n",
      "[24,   270] loss: 0.013237040489912033\n",
      "[24,   280] loss: 0.005457880906760693\n",
      "[24,   290] loss: 0.011343074962496758\n",
      "Got 2313 / 2315 with accuracy: 99.91360691144709%\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "Training in progress\n",
      "[25,    10] loss: 0.03637048974633217\n",
      "[25,    20] loss: 0.13547390699386597\n",
      "[25,    30] loss: 0.09995158016681671\n",
      "[25,    40] loss: 0.2534966766834259\n",
      "[25,    50] loss: 0.027145907282829285\n",
      "[25,    60] loss: 0.01716785505414009\n",
      "[25,    70] loss: 0.009336850605905056\n",
      "[25,    80] loss: 0.026885811239480972\n",
      "[25,    90] loss: 0.0065804217010736465\n",
      "[25,   100] loss: 0.5624725818634033\n",
      "[25,   110] loss: 0.006078861653804779\n",
      "[25,   120] loss: 0.11178168654441833\n",
      "[25,   130] loss: 0.0008152150548994541\n",
      "[25,   140] loss: 0.013394095003604889\n",
      "[25,   150] loss: 0.05930803716182709\n",
      "[25,   160] loss: 0.008942564949393272\n",
      "[25,   170] loss: 0.2188706248998642\n",
      "[25,   180] loss: 0.0013304775347933173\n",
      "[25,   190] loss: 0.00518348254263401\n",
      "[25,   200] loss: 0.015485638752579689\n",
      "[25,   210] loss: 0.1879262626171112\n",
      "[25,   220] loss: 0.12175433337688446\n",
      "[25,   230] loss: 0.017617717385292053\n",
      "[25,   240] loss: 0.24896258115768433\n",
      "[25,   250] loss: 0.024989977478981018\n",
      "[25,   260] loss: 0.22821931540966034\n",
      "[25,   270] loss: 0.008077886886894703\n",
      "[25,   280] loss: 0.0858888253569603\n",
      "[25,   290] loss: 0.95438551902771\n",
      "[25,   300] loss: 0.1001366525888443\n",
      "[25,   310] loss: 0.009510903619229794\n",
      "[25,   320] loss: 0.005417566746473312\n",
      "[25,   330] loss: 0.4260430932044983\n",
      "[25,   340] loss: 0.4236360788345337\n",
      "[25,   350] loss: 0.044440776109695435\n",
      "[25,   360] loss: 0.05258489027619362\n",
      "[25,   370] loss: 0.2093418538570404\n",
      "[25,   380] loss: 0.006857029162347317\n",
      "[25,   390] loss: 0.00987920816987753\n",
      "[25,   400] loss: 0.020583854988217354\n",
      "[25,   410] loss: 0.013344598934054375\n",
      "[25,   420] loss: 0.05770605802536011\n",
      "[25,   430] loss: 0.026963699609041214\n",
      "[25,   440] loss: 0.27604198455810547\n",
      "[25,   450] loss: 0.3202628791332245\n",
      "[25,   460] loss: 0.013982675038278103\n",
      "[25,   470] loss: 0.06633494794368744\n",
      "[25,   480] loss: 0.019215447828173637\n",
      "[25,   490] loss: 0.035572588443756104\n",
      "[25,   500] loss: 0.013804208487272263\n",
      "[25,   510] loss: 0.04140109941363335\n",
      "[25,   520] loss: 0.12577879428863525\n",
      "[25,   530] loss: 0.018715066835284233\n",
      "[25,   540] loss: 0.2592383027076721\n",
      "[25,   550] loss: 0.10887227952480316\n",
      "[25,   560] loss: 0.0024856601376086473\n",
      "[25,   570] loss: 0.13464084267616272\n",
      "[25,   580] loss: 0.5649811029434204\n",
      "[25,   590] loss: 0.18163414299488068\n",
      "[25,   600] loss: 0.05574382469058037\n",
      "[25,   610] loss: 0.08528846502304077\n",
      "[25,   620] loss: 0.01116512157022953\n",
      "[25,   630] loss: 0.021183954551815987\n",
      "[25,   640] loss: 0.1249694973230362\n",
      "[25,   650] loss: 0.013894322328269482\n",
      "[25,   660] loss: 0.010516731068491936\n",
      "[25,   670] loss: 0.04004700854420662\n",
      "[25,   680] loss: 0.01677577756345272\n",
      "[25,   690] loss: 0.014241823926568031\n",
      "[25,   700] loss: 0.12563030421733856\n",
      "[25,   710] loss: 0.3667127788066864\n",
      "[25,   720] loss: 0.1652715802192688\n",
      "[25,   730] loss: 0.08399621397256851\n",
      "[25,   740] loss: 0.03323134034872055\n",
      "[25,   750] loss: 0.0077699036337435246\n",
      "[25,   760] loss: 0.15987907350063324\n",
      "[25,   770] loss: 0.02328369952738285\n",
      "[25,   780] loss: 0.025002093985676765\n",
      "[25,   790] loss: 0.04171368479728699\n",
      "[25,   800] loss: 0.044594746083021164\n",
      "[25,   810] loss: 0.05894537642598152\n",
      "[25,   820] loss: 0.023356858640909195\n",
      "[25,   830] loss: 0.01568460650742054\n",
      "[25,   840] loss: 0.053902290761470795\n",
      "[25,   850] loss: 0.019385481253266335\n",
      "[25,   860] loss: 0.07017063349485397\n",
      "[25,   870] loss: 0.6763758659362793\n",
      "[25,   880] loss: 0.21554753184318542\n",
      "[25,   890] loss: 0.06807318329811096\n",
      "[25,   900] loss: 0.00447795819491148\n",
      "[25,   910] loss: 0.01232212409377098\n",
      "[25,   920] loss: 0.13477304577827454\n",
      "[25,   930] loss: 0.13216060400009155\n",
      "[25,   940] loss: 0.13671699166297913\n",
      "[25,   950] loss: 0.27054089307785034\n",
      "[25,   960] loss: 0.037312570959329605\n",
      "[25,   970] loss: 0.015641313046216965\n",
      "[25,   980] loss: 0.01092583779245615\n",
      "[25,   990] loss: 0.009428607299923897\n",
      "[25,  1000] loss: 0.03244839236140251\n",
      "[25,  1010] loss: 0.020005077123641968\n",
      "Got 7858 / 8108 with accuracy: 96.9166255550074%\n",
      "Validating in progress\n",
      "[25,    10] loss: 0.006235531065613031\n",
      "[25,    20] loss: 0.017822520807385445\n",
      "[25,    30] loss: 0.024120965972542763\n",
      "[25,    40] loss: 0.03167322650551796\n",
      "[25,    50] loss: 0.0075835129246115685\n",
      "[25,    60] loss: 0.011692002415657043\n",
      "[25,    70] loss: 0.015361286699771881\n",
      "[25,    80] loss: 0.0009374463115818799\n",
      "[25,    90] loss: 0.0047279540449380875\n",
      "[25,   100] loss: 0.017244739457964897\n",
      "[25,   110] loss: 0.003342156298458576\n",
      "[25,   120] loss: 0.0063286167569458485\n",
      "[25,   130] loss: 0.008888822980225086\n",
      "[25,   140] loss: 0.01088070310652256\n",
      "[25,   150] loss: 0.019262880086898804\n",
      "[25,   160] loss: 0.013352908194065094\n",
      "[25,   170] loss: 0.028418071568012238\n",
      "[25,   180] loss: 0.003229066962376237\n",
      "[25,   190] loss: 0.0044401767663657665\n",
      "[25,   200] loss: 0.013942968100309372\n",
      "[25,   210] loss: 0.012404447421431541\n",
      "[25,   220] loss: 0.00528985820710659\n",
      "[25,   230] loss: 0.00132819265127182\n",
      "[25,   240] loss: 0.006874558981508017\n",
      "[25,   250] loss: 0.003176442813128233\n",
      "[25,   260] loss: 0.022321028634905815\n",
      "[25,   270] loss: 0.005714118015021086\n",
      "[25,   280] loss: 0.02406987175345421\n",
      "[25,   290] loss: 0.020405462011694908\n",
      "Got 2315 / 2315 with accuracy: 100.0%\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "Training in progress\n",
      "[26,    10] loss: 0.05377037823200226\n",
      "[26,    20] loss: 0.003180925501510501\n",
      "[26,    30] loss: 0.16370247304439545\n",
      "[26,    40] loss: 0.020355092361569405\n",
      "[26,    50] loss: 0.23216570913791656\n",
      "[26,    60] loss: 0.2556266486644745\n",
      "[26,    70] loss: 0.08034849166870117\n",
      "[26,    80] loss: 0.0049378094263374805\n",
      "[26,    90] loss: 0.0255110040307045\n",
      "[26,   100] loss: 0.006351737305521965\n",
      "[26,   110] loss: 0.06459736078977585\n",
      "[26,   120] loss: 0.11920265108346939\n",
      "[26,   130] loss: 0.006417238153517246\n",
      "[26,   140] loss: 0.08123286068439484\n",
      "[26,   150] loss: 0.025039277970790863\n",
      "[26,   160] loss: 0.13214127719402313\n",
      "[26,   170] loss: 0.01743415929377079\n",
      "[26,   180] loss: 0.019340522587299347\n",
      "[26,   190] loss: 0.018162358552217484\n",
      "[26,   200] loss: 0.011049711145460606\n",
      "[26,   210] loss: 0.03704984113574028\n",
      "[26,   220] loss: 0.12330037355422974\n",
      "[26,   230] loss: 0.006845233030617237\n",
      "[26,   240] loss: 0.09491132944822311\n",
      "[26,   250] loss: 0.03366447240114212\n",
      "[26,   260] loss: 0.008010643534362316\n",
      "[26,   270] loss: 0.3561541438102722\n",
      "[26,   280] loss: 0.051292505115270615\n",
      "[26,   290] loss: 0.024672873318195343\n",
      "[26,   300] loss: 0.004363593179732561\n",
      "[26,   310] loss: 0.019697725772857666\n",
      "[26,   320] loss: 0.058402299880981445\n",
      "[26,   330] loss: 0.2758723199367523\n",
      "[26,   340] loss: 0.016642561182379723\n",
      "[26,   350] loss: 0.27707260847091675\n",
      "[26,   360] loss: 0.01639677956700325\n",
      "[26,   370] loss: 0.016221996396780014\n",
      "[26,   380] loss: 0.4141184985637665\n",
      "[26,   390] loss: 0.2020370215177536\n",
      "[26,   400] loss: 0.4972206950187683\n",
      "[26,   410] loss: 0.013891887851059437\n",
      "[26,   420] loss: 0.08706139028072357\n",
      "[26,   430] loss: 0.06270404160022736\n",
      "[26,   440] loss: 0.011631181463599205\n",
      "[26,   450] loss: 0.1605454981327057\n",
      "[26,   460] loss: 0.010062187910079956\n",
      "[26,   470] loss: 0.009568367153406143\n",
      "[26,   480] loss: 0.019625259563326836\n",
      "[26,   490] loss: 0.34956949949264526\n",
      "[26,   500] loss: 0.03404291719198227\n",
      "[26,   510] loss: 0.014513906091451645\n",
      "[26,   520] loss: 0.19320403039455414\n",
      "[26,   530] loss: 0.004451135639101267\n",
      "[26,   540] loss: 0.013146289624273777\n",
      "[26,   550] loss: 0.15085642039775848\n",
      "[26,   560] loss: 0.1169964149594307\n",
      "[26,   570] loss: 0.1464032381772995\n",
      "[26,   580] loss: 0.08050119876861572\n",
      "[26,   590] loss: 0.21058358252048492\n",
      "[26,   600] loss: 0.11977897584438324\n",
      "[26,   610] loss: 0.010767580941319466\n",
      "[26,   620] loss: 0.02034905180335045\n",
      "[26,   630] loss: 0.06025887280702591\n",
      "[26,   640] loss: 0.09648583084344864\n",
      "[26,   650] loss: 0.01877455785870552\n",
      "[26,   660] loss: 0.018744122236967087\n",
      "[26,   670] loss: 0.008452936075627804\n",
      "[26,   680] loss: 0.024804605171084404\n",
      "[26,   690] loss: 0.055885229259729385\n",
      "[26,   700] loss: 0.04993806779384613\n",
      "[26,   710] loss: 0.006078390404582024\n",
      "[26,   720] loss: 0.08062662184238434\n",
      "[26,   730] loss: 0.08643747866153717\n",
      "[26,   740] loss: 0.012945876456797123\n",
      "[26,   750] loss: 0.4756264388561249\n",
      "[26,   760] loss: 0.7231919169425964\n",
      "[26,   770] loss: 0.008637281134724617\n",
      "[26,   780] loss: 0.01916000060737133\n",
      "[26,   790] loss: 0.00480269081890583\n",
      "[26,   800] loss: 0.016005881130695343\n",
      "[26,   810] loss: 0.016065580770373344\n",
      "[26,   820] loss: 0.021723885089159012\n",
      "[26,   830] loss: 0.007854970172047615\n",
      "[26,   840] loss: 0.04995487257838249\n",
      "[26,   850] loss: 0.03566022217273712\n",
      "[26,   860] loss: 0.17194971442222595\n",
      "[26,   870] loss: 0.0074553703889250755\n",
      "[26,   880] loss: 0.18416190147399902\n",
      "[26,   890] loss: 0.04990430921316147\n",
      "[26,   900] loss: 0.0018407177412882447\n",
      "[26,   910] loss: 0.017960915341973305\n",
      "[26,   920] loss: 0.24953430891036987\n",
      "[26,   930] loss: 0.030207501724362373\n",
      "[26,   940] loss: 0.142035111784935\n",
      "[26,   950] loss: 0.09192368388175964\n",
      "[26,   960] loss: 0.0766410082578659\n",
      "[26,   970] loss: 0.19488516449928284\n",
      "[26,   980] loss: 0.034436050802469254\n",
      "[26,   990] loss: 0.08759237080812454\n",
      "[26,  1000] loss: 0.056058209389448166\n",
      "[26,  1010] loss: 0.1423330008983612\n",
      "Got 7865 / 8108 with accuracy: 97.00296003946718%\n",
      "Validating in progress\n",
      "[26,    10] loss: 0.018824690952897072\n",
      "[26,    20] loss: 0.002988223684951663\n",
      "[26,    30] loss: 0.009794930927455425\n",
      "[26,    40] loss: 0.005702280439436436\n",
      "[26,    50] loss: 0.004891593940556049\n",
      "[26,    60] loss: 0.013319194316864014\n",
      "[26,    70] loss: 0.010478703305125237\n",
      "[26,    80] loss: 0.018697254359722137\n",
      "[26,    90] loss: 0.03184857219457626\n",
      "[26,   100] loss: 0.004874746315181255\n",
      "[26,   110] loss: 0.009212976321578026\n",
      "[26,   120] loss: 0.005693130195140839\n",
      "[26,   130] loss: 0.015274377539753914\n",
      "[26,   140] loss: 0.01699923351407051\n",
      "[26,   150] loss: 0.014061713591217995\n",
      "[26,   160] loss: 0.005886174738407135\n",
      "[26,   170] loss: 0.005723631475120783\n",
      "[26,   180] loss: 0.0191242266446352\n",
      "[26,   190] loss: 0.024250006303191185\n",
      "[26,   200] loss: 0.0038599856197834015\n",
      "[26,   210] loss: 0.009683491662144661\n",
      "[26,   220] loss: 0.005938773974776268\n",
      "[26,   230] loss: 0.001900196191854775\n",
      "[26,   240] loss: 0.0074920025654137135\n",
      "[26,   250] loss: 0.01068004872649908\n",
      "[26,   260] loss: 0.0027870568446815014\n",
      "[26,   270] loss: 0.0036413660272955894\n",
      "[26,   280] loss: 0.011478872038424015\n",
      "[26,   290] loss: 0.02373112551867962\n",
      "Got 2315 / 2315 with accuracy: 100.0%\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "Training in progress\n",
      "[27,    10] loss: 0.0060525694862008095\n",
      "[27,    20] loss: 0.06594676524400711\n",
      "[27,    30] loss: 0.007149809971451759\n",
      "[27,    40] loss: 0.02029603347182274\n",
      "[27,    50] loss: 0.502802848815918\n",
      "[27,    60] loss: 0.5217116475105286\n",
      "[27,    70] loss: 0.16454337537288666\n",
      "[27,    80] loss: 0.0029369788244366646\n",
      "[27,    90] loss: 0.1616688072681427\n",
      "[27,   100] loss: 0.28199487924575806\n",
      "[27,   110] loss: 0.07183247059583664\n",
      "[27,   120] loss: 0.003169555217027664\n",
      "[27,   130] loss: 0.013638852164149284\n",
      "[27,   140] loss: 0.04607958346605301\n",
      "[27,   150] loss: 0.011029575951397419\n",
      "[27,   160] loss: 0.009164889343082905\n",
      "[27,   170] loss: 0.012490903027355671\n",
      "[27,   180] loss: 0.1689973920583725\n",
      "[27,   190] loss: 0.04152743145823479\n",
      "[27,   200] loss: 0.010710104368627071\n",
      "[27,   210] loss: 0.09384346008300781\n",
      "[27,   220] loss: 0.08267480134963989\n",
      "[27,   230] loss: 0.17240610718727112\n",
      "[27,   240] loss: 0.023424144834280014\n",
      "[27,   250] loss: 0.22618894279003143\n",
      "[27,   260] loss: 0.027527691796422005\n",
      "[27,   270] loss: 0.008746654726564884\n",
      "[27,   280] loss: 0.09722936898469925\n",
      "[27,   290] loss: 0.03670899197459221\n",
      "[27,   300] loss: 0.008417343720793724\n",
      "[27,   310] loss: 0.14150112867355347\n",
      "[27,   320] loss: 0.24776339530944824\n",
      "[27,   330] loss: 0.004387024324387312\n",
      "[27,   340] loss: 0.25017306208610535\n",
      "[27,   350] loss: 0.0190024022012949\n",
      "[27,   360] loss: 0.3879571855068207\n",
      "[27,   370] loss: 0.01605052873492241\n",
      "[27,   380] loss: 0.09541895985603333\n",
      "[27,   390] loss: 0.05069246515631676\n",
      "[27,   400] loss: 0.13966821134090424\n",
      "[27,   410] loss: 0.18860368430614471\n",
      "[27,   420] loss: 0.026280419901013374\n",
      "[27,   430] loss: 0.05670634284615517\n",
      "[27,   440] loss: 0.0448160320520401\n",
      "[27,   450] loss: 0.008092415519058704\n",
      "[27,   460] loss: 0.11811238527297974\n",
      "[27,   470] loss: 0.008322266861796379\n",
      "[27,   480] loss: 0.16052277386188507\n",
      "[27,   490] loss: 0.0020636911503970623\n",
      "[27,   500] loss: 0.03838092088699341\n",
      "[27,   510] loss: 0.010068049654364586\n",
      "[27,   520] loss: 0.14783647656440735\n",
      "[27,   530] loss: 0.018692560493946075\n",
      "[27,   540] loss: 0.085200235247612\n",
      "[27,   550] loss: 0.01573263481259346\n",
      "[27,   560] loss: 0.4216172695159912\n",
      "[27,   570] loss: 0.009910027496516705\n",
      "[27,   580] loss: 0.5229470133781433\n",
      "[27,   590] loss: 0.0461387112736702\n",
      "[27,   600] loss: 0.01737362891435623\n",
      "[27,   610] loss: 0.016749294474720955\n",
      "[27,   620] loss: 0.06296440958976746\n",
      "[27,   630] loss: 0.15318620204925537\n",
      "[27,   640] loss: 0.011994585394859314\n",
      "[27,   650] loss: 0.007733107544481754\n",
      "[27,   660] loss: 0.06655874103307724\n",
      "[27,   670] loss: 0.4668266475200653\n",
      "[27,   680] loss: 0.4253425598144531\n",
      "[27,   690] loss: 0.014346268959343433\n",
      "[27,   700] loss: 0.04805565997958183\n",
      "[27,   710] loss: 0.1674651801586151\n",
      "[27,   720] loss: 0.033619578927755356\n",
      "[27,   730] loss: 0.6607959270477295\n",
      "[27,   740] loss: 0.005319440737366676\n",
      "[27,   750] loss: 0.04599743336439133\n",
      "[27,   760] loss: 0.00870533287525177\n",
      "[27,   770] loss: 0.037033651024103165\n",
      "[27,   780] loss: 0.052617549896240234\n",
      "[27,   790] loss: 0.015085489489138126\n",
      "[27,   800] loss: 0.1344461292028427\n",
      "[27,   810] loss: 0.03926722705364227\n",
      "[27,   820] loss: 0.07669507712125778\n",
      "[27,   830] loss: 0.008465533144772053\n",
      "[27,   840] loss: 0.006535959430038929\n",
      "[27,   850] loss: 0.09226968884468079\n",
      "[27,   860] loss: 0.0020124344155192375\n",
      "[27,   870] loss: 0.009501571767032146\n",
      "[27,   880] loss: 0.0361093245446682\n",
      "[27,   890] loss: 0.21945136785507202\n",
      "[27,   900] loss: 0.017051739618182182\n",
      "[27,   910] loss: 0.1946212351322174\n",
      "[27,   920] loss: 0.07072898000478745\n",
      "[27,   930] loss: 0.018806667998433113\n",
      "[27,   940] loss: 0.03517984598875046\n",
      "[27,   950] loss: 0.009759906679391861\n",
      "[27,   960] loss: 0.15022115409374237\n",
      "[27,   970] loss: 0.13803541660308838\n",
      "[27,   980] loss: 0.030806127935647964\n",
      "[27,   990] loss: 0.31812936067581177\n",
      "[27,  1000] loss: 0.16502918303012848\n",
      "[27,  1010] loss: 0.15074506402015686\n",
      "Got 7877 / 8108 with accuracy: 97.15096201282684%\n",
      "Validating in progress\n",
      "[27,    10] loss: 0.024003393948078156\n",
      "[27,    20] loss: 0.025594525039196014\n",
      "[27,    30] loss: 0.01854259893298149\n",
      "[27,    40] loss: 0.001993902027606964\n",
      "[27,    50] loss: 0.0023489927407354116\n",
      "[27,    60] loss: 0.0034653812181204557\n",
      "[27,    70] loss: 0.00021161159384064376\n",
      "[27,    80] loss: 0.01730954274535179\n",
      "[27,    90] loss: 0.019466208294034004\n",
      "[27,   100] loss: 0.0036709047853946686\n",
      "[27,   110] loss: 0.008923737332224846\n",
      "[27,   120] loss: 0.04460370913147926\n",
      "[27,   130] loss: 0.0018442438449710608\n",
      "[27,   140] loss: 0.01377283874899149\n",
      "[27,   150] loss: 0.00842161662876606\n",
      "[27,   160] loss: 0.017317045480012894\n",
      "[27,   170] loss: 0.010154961608350277\n",
      "[27,   180] loss: 0.018329182639718056\n",
      "[27,   190] loss: 0.0026362582575529814\n",
      "[27,   200] loss: 0.010189884342253208\n",
      "[27,   210] loss: 0.0038085896521806717\n",
      "[27,   220] loss: 0.003679713699966669\n",
      "[27,   230] loss: 0.021362273022532463\n",
      "[27,   240] loss: 0.014072799123823643\n",
      "[27,   250] loss: 0.02521711401641369\n",
      "[27,   260] loss: 0.010821392759680748\n",
      "[27,   270] loss: 0.023924345150589943\n",
      "[27,   280] loss: 0.0021065305918455124\n",
      "[27,   290] loss: 0.031473759561777115\n",
      "Got 2315 / 2315 with accuracy: 100.0%\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "Training in progress\n",
      "[28,    10] loss: 0.6006385684013367\n",
      "[28,    20] loss: 0.003120125737041235\n",
      "[28,    30] loss: 0.012450040318071842\n",
      "[28,    40] loss: 0.03197343274950981\n",
      "[28,    50] loss: 0.007451958488672972\n",
      "[28,    60] loss: 0.012411083094775677\n",
      "[28,    70] loss: 0.016293024644255638\n",
      "[28,    80] loss: 0.016338806599378586\n",
      "[28,    90] loss: 0.010165633633732796\n",
      "[28,   100] loss: 0.0017656250856816769\n",
      "[28,   110] loss: 0.04012145474553108\n",
      "[28,   120] loss: 0.010469065979123116\n",
      "[28,   130] loss: 0.9712779521942139\n",
      "[28,   140] loss: 0.16047172248363495\n",
      "[28,   150] loss: 0.041706863790750504\n",
      "[28,   160] loss: 0.013887815177440643\n",
      "[28,   170] loss: 0.44267481565475464\n",
      "[28,   180] loss: 0.014991371892392635\n",
      "[28,   190] loss: 0.01119720283895731\n",
      "[28,   200] loss: 0.1904861330986023\n",
      "[28,   210] loss: 0.1279684603214264\n",
      "[28,   220] loss: 0.4530923068523407\n",
      "[28,   230] loss: 0.0227577593177557\n",
      "[28,   240] loss: 0.028083806857466698\n",
      "[28,   250] loss: 0.29340168833732605\n",
      "[28,   260] loss: 0.009895495139062405\n",
      "[28,   270] loss: 0.021170102059841156\n",
      "[28,   280] loss: 0.15179798007011414\n",
      "[28,   290] loss: 0.05581977218389511\n",
      "[28,   300] loss: 0.18127916753292084\n",
      "[28,   310] loss: 0.05952496826648712\n",
      "[28,   320] loss: 0.02035958133637905\n",
      "[28,   330] loss: 0.032337263226509094\n",
      "[28,   340] loss: 0.11531320959329605\n",
      "[28,   350] loss: 0.025405123829841614\n",
      "[28,   360] loss: 0.034356776624917984\n",
      "[28,   370] loss: 0.013738950714468956\n",
      "[28,   380] loss: 0.0072794826701283455\n",
      "[28,   390] loss: 0.2109462171792984\n",
      "[28,   400] loss: 0.017341971397399902\n",
      "[28,   410] loss: 0.018892090767621994\n",
      "[28,   420] loss: 0.21728666126728058\n",
      "[28,   430] loss: 0.016619067639112473\n",
      "[28,   440] loss: 0.14562878012657166\n",
      "[28,   450] loss: 0.019541218876838684\n",
      "[28,   460] loss: 0.004635261837393045\n",
      "[28,   470] loss: 0.003603992983698845\n",
      "[28,   480] loss: 0.0078755347058177\n",
      "[28,   490] loss: 0.09845709800720215\n",
      "[28,   500] loss: 0.03552624210715294\n",
      "[28,   510] loss: 0.16738101840019226\n",
      "[28,   520] loss: 0.017274577170610428\n",
      "[28,   530] loss: 0.042418114840984344\n",
      "[28,   540] loss: 0.01178387738764286\n",
      "[28,   550] loss: 0.07528375834226608\n",
      "[28,   560] loss: 0.012563169002532959\n",
      "[28,   570] loss: 0.3557310700416565\n",
      "[28,   580] loss: 0.2875012755393982\n",
      "[28,   590] loss: 0.05867606773972511\n",
      "[28,   600] loss: 0.02202463150024414\n",
      "[28,   610] loss: 0.015465927310287952\n",
      "[28,   620] loss: 0.02672543376684189\n",
      "[28,   630] loss: 0.015371238812804222\n",
      "[28,   640] loss: 0.18565939366817474\n",
      "[28,   650] loss: 0.003857557661831379\n",
      "[28,   660] loss: 0.10162149369716644\n",
      "[28,   670] loss: 0.06561857461929321\n",
      "[28,   680] loss: 0.0033163728658109903\n",
      "[28,   690] loss: 0.008892619982361794\n",
      "[28,   700] loss: 0.020524699240922928\n",
      "[28,   710] loss: 0.01205909252166748\n",
      "[28,   720] loss: 0.11693054437637329\n",
      "[28,   730] loss: 0.009898324497044086\n",
      "[28,   740] loss: 0.07244376093149185\n",
      "[28,   750] loss: 0.18126605451107025\n",
      "[28,   760] loss: 0.25353753566741943\n",
      "[28,   770] loss: 0.03778647258877754\n",
      "[28,   780] loss: 0.04415427893400192\n",
      "[28,   790] loss: 0.025889160111546516\n",
      "[28,   800] loss: 0.018246730789542198\n",
      "[28,   810] loss: 0.028104055672883987\n",
      "[28,   820] loss: 0.0221441350877285\n",
      "[28,   830] loss: 0.136799618601799\n",
      "[28,   840] loss: 0.08377593755722046\n",
      "[28,   850] loss: 0.10443461686372757\n",
      "[28,   860] loss: 0.048443503677845\n",
      "[28,   870] loss: 0.03849093243479729\n",
      "[28,   880] loss: 0.027606461197137833\n",
      "[28,   890] loss: 0.004358518403023481\n",
      "[28,   900] loss: 0.0050537376664578915\n",
      "[28,   910] loss: 0.08900656551122665\n",
      "[28,   920] loss: 0.11793603748083115\n",
      "[28,   930] loss: 0.16199132800102234\n",
      "[28,   940] loss: 0.10413350909948349\n",
      "[28,   950] loss: 0.003816122654825449\n",
      "[28,   960] loss: 0.030083153396844864\n",
      "[28,   970] loss: 0.033861223608255386\n",
      "[28,   980] loss: 0.05281686782836914\n",
      "[28,   990] loss: 0.01641090027987957\n",
      "[28,  1000] loss: 0.004202201031148434\n",
      "[28,  1010] loss: 0.022515304386615753\n",
      "Got 7841 / 8108 with accuracy: 96.7069560927479%\n",
      "Validating in progress\n",
      "[28,    10] loss: 0.011043156497180462\n",
      "[28,    20] loss: 0.002716083312407136\n",
      "[28,    30] loss: 0.0016523315571248531\n",
      "[28,    40] loss: 0.008347072638571262\n",
      "[28,    50] loss: 0.014727679081261158\n",
      "[28,    60] loss: 0.0006852121441625059\n",
      "[28,    70] loss: 0.0029398861806839705\n",
      "[28,    80] loss: 0.0010913399746641517\n",
      "[28,    90] loss: 0.013886701315641403\n",
      "[28,   100] loss: 0.015372984111309052\n",
      "[28,   110] loss: 0.004556228406727314\n",
      "[28,   120] loss: 0.0036342504899948835\n",
      "[28,   130] loss: 0.012933054938912392\n",
      "[28,   140] loss: 0.005988095887005329\n",
      "[28,   150] loss: 0.012759589590132236\n",
      "[28,   160] loss: 0.008082139305770397\n",
      "[28,   170] loss: 0.006157662719488144\n",
      "[28,   180] loss: 0.006509872619062662\n",
      "[28,   190] loss: 0.004436012357473373\n",
      "[28,   200] loss: 0.01788981631398201\n",
      "[28,   210] loss: 0.022991837933659554\n",
      "[28,   220] loss: 0.044302456080913544\n",
      "[28,   230] loss: 0.0023133247159421444\n",
      "[28,   240] loss: 0.0035140698309987783\n",
      "[28,   250] loss: 0.006588878110051155\n",
      "[28,   260] loss: 0.007087987381964922\n",
      "[28,   270] loss: 0.019723061472177505\n",
      "[28,   280] loss: 0.006658649537712336\n",
      "[28,   290] loss: 0.006599469110369682\n",
      "Got 2314 / 2315 with accuracy: 99.95680345572354%\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "Training in progress\n",
      "[29,    10] loss: 0.2565329968929291\n",
      "[29,    20] loss: 0.03924094885587692\n",
      "[29,    30] loss: 0.01604912430047989\n",
      "[29,    40] loss: 0.0027521676383912563\n",
      "[29,    50] loss: 0.45437929034233093\n",
      "[29,    60] loss: 0.03467153385281563\n",
      "[29,    70] loss: 0.014699608087539673\n",
      "[29,    80] loss: 0.08508171886205673\n",
      "[29,    90] loss: 0.28299155831336975\n",
      "[29,   100] loss: 0.09078330546617508\n",
      "[29,   110] loss: 0.04221868887543678\n",
      "[29,   120] loss: 0.012054099701344967\n",
      "[29,   130] loss: 0.011758174747228622\n",
      "[29,   140] loss: 0.12419351935386658\n",
      "[29,   150] loss: 0.01969129405915737\n",
      "[29,   160] loss: 0.11379849165678024\n",
      "[29,   170] loss: 0.13020873069763184\n",
      "[29,   180] loss: 0.02532089129090309\n",
      "[29,   190] loss: 0.060904067009687424\n",
      "[29,   200] loss: 0.03902662545442581\n",
      "[29,   210] loss: 0.03049020655453205\n",
      "[29,   220] loss: 0.016740228980779648\n",
      "[29,   230] loss: 0.028118200600147247\n",
      "[29,   240] loss: 0.0032543675042688847\n",
      "[29,   250] loss: 0.02555801160633564\n",
      "[29,   260] loss: 0.02357381395995617\n",
      "[29,   270] loss: 0.006200696341693401\n",
      "[29,   280] loss: 0.056612271815538406\n",
      "[29,   290] loss: 0.12568815052509308\n",
      "[29,   300] loss: 0.19471092522144318\n",
      "[29,   310] loss: 0.026050811633467674\n",
      "[29,   320] loss: 0.05577946826815605\n",
      "[29,   330] loss: 0.0025455958675593138\n",
      "[29,   340] loss: 0.04076270014047623\n",
      "[29,   350] loss: 0.0067085521295666695\n",
      "[29,   360] loss: 0.013196220621466637\n",
      "[29,   370] loss: 0.03185638040304184\n",
      "[29,   380] loss: 0.312289834022522\n",
      "[29,   390] loss: 0.12174692004919052\n",
      "[29,   400] loss: 0.04403220862150192\n",
      "[29,   410] loss: 0.16319431364536285\n",
      "[29,   420] loss: 0.01732010580599308\n",
      "[29,   430] loss: 0.009247461333870888\n",
      "[29,   440] loss: 0.025955364108085632\n",
      "[29,   450] loss: 0.014613877050578594\n",
      "[29,   460] loss: 0.02251792512834072\n",
      "[29,   470] loss: 0.007749167270958424\n",
      "[29,   480] loss: 0.007048150524497032\n",
      "[29,   490] loss: 0.191944882273674\n",
      "[29,   500] loss: 0.006060585379600525\n",
      "[29,   510] loss: 0.03009185381233692\n",
      "[29,   520] loss: 0.07702144980430603\n",
      "[29,   530] loss: 0.006146200466901064\n",
      "[29,   540] loss: 0.0004632765776477754\n",
      "[29,   550] loss: 0.04832946136593819\n",
      "[29,   560] loss: 0.023155899718403816\n",
      "[29,   570] loss: 0.2792958617210388\n",
      "[29,   580] loss: 0.25124847888946533\n",
      "[29,   590] loss: 0.14404775202274323\n",
      "[29,   600] loss: 0.0010867675300687551\n",
      "[29,   610] loss: 0.016294440254569054\n",
      "[29,   620] loss: 0.0014096362283453345\n",
      "[29,   630] loss: 0.5373561978340149\n",
      "[29,   640] loss: 0.00200732727535069\n",
      "[29,   650] loss: 0.022840239107608795\n",
      "[29,   660] loss: 0.03314422443509102\n",
      "[29,   670] loss: 0.06558851897716522\n",
      "[29,   680] loss: 0.021732429042458534\n",
      "[29,   690] loss: 0.017683368176221848\n",
      "[29,   700] loss: 0.18932323157787323\n",
      "[29,   710] loss: 0.031350504606962204\n",
      "[29,   720] loss: 0.010601981543004513\n",
      "[29,   730] loss: 0.025770487263798714\n",
      "[29,   740] loss: 0.030426885932683945\n",
      "[29,   750] loss: 0.02151232771575451\n",
      "[29,   760] loss: 0.1352708339691162\n",
      "[29,   770] loss: 0.021075988188385963\n",
      "[29,   780] loss: 0.1567867547273636\n",
      "[29,   790] loss: 0.06716340035200119\n",
      "[29,   800] loss: 0.010554950684309006\n",
      "[29,   810] loss: 0.029938755556941032\n",
      "[29,   820] loss: 0.0518382303416729\n",
      "[29,   830] loss: 0.018701959401369095\n",
      "[29,   840] loss: 0.01680881343781948\n",
      "[29,   850] loss: 0.02256707474589348\n",
      "[29,   860] loss: 0.02388499118387699\n",
      "[29,   870] loss: 0.4046939015388489\n",
      "[29,   880] loss: 0.02333912067115307\n",
      "[29,   890] loss: 0.0817389264702797\n",
      "[29,   900] loss: 0.04240764304995537\n",
      "[29,   910] loss: 0.0058723678812384605\n",
      "[29,   920] loss: 0.03267182037234306\n",
      "[29,   930] loss: 0.021433280780911446\n",
      "[29,   940] loss: 0.3366777002811432\n",
      "[29,   950] loss: 0.05384278669953346\n",
      "[29,   960] loss: 0.018431462347507477\n",
      "[29,   970] loss: 0.022279167547822\n",
      "[29,   980] loss: 0.05411921441555023\n",
      "[29,   990] loss: 0.02418849989771843\n",
      "[29,  1000] loss: 0.012077298015356064\n",
      "[29,  1010] loss: 0.007851631380617619\n",
      "Got 7885 / 8108 with accuracy: 97.2496299950666%\n",
      "Validating in progress\n",
      "[29,    10] loss: 0.004291144199669361\n",
      "[29,    20] loss: 0.005076100118458271\n",
      "[29,    30] loss: 0.008893724530935287\n",
      "[29,    40] loss: 0.002598692663013935\n",
      "[29,    50] loss: 0.029456429183483124\n",
      "[29,    60] loss: 0.006047937087714672\n",
      "[29,    70] loss: 0.007497366052120924\n",
      "[29,    80] loss: 0.004502537660300732\n",
      "[29,    90] loss: 0.005662128794938326\n",
      "[29,   100] loss: 0.007540534716099501\n",
      "[29,   110] loss: 0.025708358734846115\n",
      "[29,   120] loss: 0.000805575808044523\n",
      "[29,   130] loss: 0.006100368220359087\n",
      "[29,   140] loss: 0.016408538445830345\n",
      "[29,   150] loss: 0.02099282294511795\n",
      "[29,   160] loss: 0.006291502621024847\n",
      "[29,   170] loss: 0.007175980601459742\n",
      "[29,   180] loss: 0.009972207248210907\n",
      "[29,   190] loss: 0.0012161104241386056\n",
      "[29,   200] loss: 0.010584939271211624\n",
      "[29,   210] loss: 0.012205146253108978\n",
      "[29,   220] loss: 0.009082656353712082\n",
      "[29,   230] loss: 0.0030134753324091434\n",
      "[29,   240] loss: 0.01718265935778618\n",
      "[29,   250] loss: 0.00041904638055711985\n",
      "[29,   260] loss: 0.02496340498328209\n",
      "[29,   270] loss: 0.04670792073011398\n",
      "[29,   280] loss: 0.03710698336362839\n",
      "[29,   290] loss: 0.012059536762535572\n",
      "Got 2315 / 2315 with accuracy: 100.0%\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "Training in progress\n",
      "[30,    10] loss: 0.09623271226882935\n",
      "[30,    20] loss: 0.02964465692639351\n",
      "[30,    30] loss: 0.008785384707152843\n",
      "[30,    40] loss: 0.42205020785331726\n",
      "[30,    50] loss: 0.02188173308968544\n",
      "[30,    60] loss: 0.03439777344465256\n",
      "[30,    70] loss: 0.148820698261261\n",
      "[30,    80] loss: 0.003994942642748356\n",
      "[30,    90] loss: 0.005242612678557634\n",
      "[30,   100] loss: 0.06187678501009941\n",
      "[30,   110] loss: 0.004199416842311621\n",
      "[30,   120] loss: 0.3065478503704071\n",
      "[30,   130] loss: 0.006646075285971165\n",
      "[30,   140] loss: 0.001042457646690309\n",
      "[30,   150] loss: 0.0554317943751812\n",
      "[30,   160] loss: 0.6934261918067932\n",
      "[30,   170] loss: 0.30312854051589966\n",
      "[30,   180] loss: 0.006123007275164127\n",
      "[30,   190] loss: 0.02111135609447956\n",
      "[30,   200] loss: 0.033891018480062485\n",
      "[30,   210] loss: 0.08553363382816315\n",
      "[30,   220] loss: 0.008932116441428661\n",
      "[30,   230] loss: 0.07978139072656631\n",
      "[30,   240] loss: 0.21608291566371918\n",
      "[30,   250] loss: 0.010948775336146355\n",
      "[30,   260] loss: 0.08311513066291809\n",
      "[30,   270] loss: 0.01343647763133049\n",
      "[30,   280] loss: 0.34982144832611084\n",
      "[30,   290] loss: 0.047895047813653946\n",
      "[30,   300] loss: 0.026516644284129143\n",
      "[30,   310] loss: 0.006483105011284351\n",
      "[30,   320] loss: 0.03246869146823883\n",
      "[30,   330] loss: 0.011876634322106838\n",
      "[30,   340] loss: 0.08500511199235916\n",
      "[30,   350] loss: 0.047360651195049286\n",
      "[30,   360] loss: 0.08773946762084961\n",
      "[30,   370] loss: 0.07507039606571198\n",
      "[30,   380] loss: 0.07512564212083817\n",
      "[30,   390] loss: 0.2842664420604706\n",
      "[30,   400] loss: 0.28174903988838196\n",
      "[30,   410] loss: 0.44427168369293213\n",
      "[30,   420] loss: 0.023781301453709602\n",
      "[30,   430] loss: 0.002173603978008032\n",
      "[30,   440] loss: 0.004666520748287439\n",
      "[30,   450] loss: 0.038727980107069016\n",
      "[30,   460] loss: 0.0219868253916502\n",
      "[30,   470] loss: 0.011318258941173553\n",
      "[30,   480] loss: 0.019946493208408356\n",
      "[30,   490] loss: 0.0047728270292282104\n",
      "[30,   500] loss: 0.01280054822564125\n",
      "[30,   510] loss: 0.014555120840668678\n",
      "[30,   520] loss: 0.030965255573391914\n",
      "[30,   530] loss: 0.1518426537513733\n",
      "[30,   540] loss: 0.011860358528792858\n",
      "[30,   550] loss: 0.08944234997034073\n",
      "[30,   560] loss: 0.017665542662143707\n",
      "[30,   570] loss: 0.05829480662941933\n",
      "[30,   580] loss: 0.26817581057548523\n",
      "[30,   590] loss: 0.015831202268600464\n",
      "[30,   600] loss: 0.009753759950399399\n",
      "[30,   610] loss: 0.13781963288784027\n",
      "[30,   620] loss: 0.017723938450217247\n",
      "[30,   630] loss: 0.03512636199593544\n",
      "[30,   640] loss: 0.2755989134311676\n",
      "[30,   650] loss: 0.004882771987468004\n",
      "[30,   660] loss: 0.013794681057333946\n",
      "[30,   670] loss: 0.11944831162691116\n",
      "[30,   680] loss: 0.0070409020408988\n",
      "[30,   690] loss: 0.01134184468537569\n",
      "[30,   700] loss: 0.32979097962379456\n",
      "[30,   710] loss: 0.00819108635187149\n",
      "[30,   720] loss: 0.009779492393136024\n",
      "[30,   730] loss: 0.2543160319328308\n",
      "[30,   740] loss: 0.014962756074965\n",
      "[30,   750] loss: 0.16452531516551971\n",
      "[30,   760] loss: 0.12098053097724915\n",
      "[30,   770] loss: 0.013285745866596699\n",
      "[30,   780] loss: 0.0008669367525726557\n",
      "[30,   790] loss: 0.025051958858966827\n",
      "[30,   800] loss: 0.010207694955170155\n",
      "[30,   810] loss: 0.2780776917934418\n",
      "[30,   820] loss: 0.05773908644914627\n",
      "[30,   830] loss: 0.09793899953365326\n",
      "[30,   840] loss: 0.39050355553627014\n",
      "[30,   850] loss: 0.028683211654424667\n",
      "[30,   860] loss: 0.045713506639003754\n",
      "[30,   870] loss: 0.006541792303323746\n",
      "[30,   880] loss: 0.02394799515604973\n",
      "[30,   890] loss: 0.3468352258205414\n",
      "[30,   900] loss: 0.11840923875570297\n",
      "[30,   910] loss: 0.08824382722377777\n",
      "[30,   920] loss: 0.011722119525074959\n",
      "[30,   930] loss: 0.013331117108464241\n",
      "[30,   940] loss: 0.010513530112802982\n",
      "[30,   950] loss: 0.07037290930747986\n",
      "[30,   960] loss: 0.12389252334833145\n",
      "[30,   970] loss: 0.011466823518276215\n",
      "[30,   980] loss: 0.1352623552083969\n",
      "[30,   990] loss: 0.07731202244758606\n",
      "[30,  1000] loss: 0.17052501440048218\n",
      "[30,  1010] loss: 0.15161991119384766\n",
      "Got 7851 / 8108 with accuracy: 96.8302910705476%\n",
      "Validating in progress\n",
      "[30,    10] loss: 0.015232107602059841\n",
      "[30,    20] loss: 0.002691134810447693\n",
      "[30,    30] loss: 0.007476188242435455\n",
      "[30,    40] loss: 0.0016562361270189285\n",
      "[30,    50] loss: 0.0006887110066600144\n",
      "[30,    60] loss: 0.008390813134610653\n",
      "[30,    70] loss: 0.015972184017300606\n",
      "[30,    80] loss: 0.0027007339522242546\n",
      "[30,    90] loss: 0.015536326915025711\n",
      "[30,   100] loss: 0.0034006591886281967\n",
      "[30,   110] loss: 0.010791498236358166\n",
      "[30,   120] loss: 0.0025227630976587534\n",
      "[30,   130] loss: 0.018580466508865356\n",
      "[30,   140] loss: 0.005391664337366819\n",
      "[30,   150] loss: 0.0017953998176380992\n",
      "[30,   160] loss: 0.01865392178297043\n",
      "[30,   170] loss: 0.0011476879008114338\n",
      "[30,   180] loss: 0.007183310575783253\n",
      "[30,   190] loss: 0.013410779647529125\n",
      "[30,   200] loss: 0.004397195298224688\n",
      "[30,   210] loss: 0.005100860260426998\n",
      "[30,   220] loss: 0.0015039725694805384\n",
      "[30,   230] loss: 0.011025927029550076\n",
      "[30,   240] loss: 0.03148851916193962\n",
      "[30,   250] loss: 0.05918671935796738\n",
      "[30,   260] loss: 0.007624556310474873\n",
      "[30,   270] loss: 0.011318456381559372\n",
      "[30,   280] loss: 0.0004140340315643698\n",
      "[30,   290] loss: 0.0008810106082819402\n",
      "Got 2315 / 2315 with accuracy: 100.0%\n",
      "\n",
      "Training complete in 25m 0s\n",
      "Best val Acc: 1.000000\n",
      "Checking accuracy on training set..\n",
      "Test Accuracy of the model: 8082/ 8108 with accuracy of: 99.67932905772076%\n",
      "Checking accuracy on evaluation set\n",
      "Test Accuracy of the model: 2315/ 2315 with accuracy of: 100.0%\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device) #Fine tune model\n",
    "model_ft = model.features\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=30, batch_size=8)\n",
    "\n",
    "input(\"visualizing model\")\n",
    "test_accuracy(train_dataloaders['train'], model_ft)\n",
    "test_accuracy(train_dataloaders['val'], model_ft)\n",
    "writer.flush()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the model\n",
    "#torch.save(model_ft, \"data/image_model.pt\")\n",
    "\n",
    "#To load the model\n",
    "#torch.load(\"data/image_model.pt\")\n",
    "#model.eval()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b099c207f7071e7802aeb8fc32298ef1a4dd7cfb6d3bb3ada8c8b299b44af55f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 ('rtx_3060')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
