{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create image classification model\n",
    "#first create an image dataset with correct labelling\n",
    "#load batches of data samples in pytorch and use cleaned images dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shah/miniconda3/envs/selenium_project/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pickle\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import torchdata as td\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductImageCategoryDataset():\n",
    "    def __init__(self, learning_rate = 1e-3):\n",
    "        super().__init__()\n",
    "        self.X = pd.read_pickle('data/image_model_X.pkl')\n",
    "        #self.X['image_array'] = self.X.values.tolist()\n",
    "        #self.X = self.X['image_array']\n",
    "        self.y = pd.read_pickle('data/image_model_y.pkl')\n",
    "        #print(train_X.shape)\n",
    "        #print(len(self.X))\n",
    "        #print(len(self.y))\n",
    "        assert len(self.X) == len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.X.iloc[index]\n",
    "        label = self.y.iloc[index]\n",
    "        #print(index)\n",
    "        features = torch.tensor(features).float()\n",
    "        #3=num of batch(get 3 images at every iteration of training the network)\n",
    "        features = features.reshape(3, 64, 64)\n",
    "        #print(features.shape)\n",
    "        label = int(label)\n",
    "        \n",
    "        return (features, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "dataset = ProductImageCategoryDataset()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#print(len(dataset))\n",
    "#dataset[12667]\n",
    "#for idx in range(len(dataset)):\n",
    "    #example = dataset[idx]\n",
    "    #features, label = example\n",
    "    #print(features.shape)\n",
    "    #print(label)\n",
    "  #  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "test_split_size = 0.2\n",
    "shuffle_dataset = True\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split_size * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = {\n",
    "    'train' : torch.utils.data.DataLoader(dataset, batch_size=16, sampler=train_sampler, num_workers=1),\n",
    "    'test' : torch.utils.data.DataLoader(dataset, batch_size=16, sampler=test_sampler, num_workers=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    inp = inp.cpu() if device else inp\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406]) \n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    \n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "    \n",
    "images, label = next(iter(train_loader['train'])) \n",
    "print(\"images-size:\", images.shape)\n",
    "\n",
    "out = torchvision.utils.make_grid(images)\n",
    "print(\"out-size:\", out.shape)\n",
    "\n",
    "imshow(out, title=[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class CNN(torch.nn.Module):\n",
    " #   def __init__(self):\n",
    "  #      super(CNN, self).__init__()\n",
    "        #self.layers = torch.nn.Sequential(\n",
    "            #3 input image channels, 8 output channels,\n",
    "            #9x9 square convolution kernel\n",
    "   #     self.conv1 = torch.nn.Conv2d(3, 6, 5)\n",
    "            #Max pooling over a (2, 2) window, #kernel and stride=2\n",
    "    #    self.maxpool = torch.nn.MaxPool2d(2, 2)\n",
    "     #   self.conv2 = torch.nn.Conv2d(6, 16, 5)\n",
    "            #torch.nn.Flatten(), #flatten\n",
    "      #  self.fc1 = torch.nn.Linear(2704, 120) #simplifiy further with linear layers - output channel((128-5)/1+1) = 124, 124/2=62 so 62*62*6=23064\n",
    "      #  self.fc2 = torch.nn.Linear(120, 84)\n",
    "      #  self.fc3 = torch.nn.Linear(84, 14)\n",
    "      #  torch.nn.Softmax(dim=1)\n",
    "\n",
    "    #def forward(self, x):\n",
    "     #   x = self.maxpool(F.relu(self.conv1(x)))\n",
    "      #  x = self.maxpool(F.relu(self.conv2(x)))\n",
    "       # x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "       # x = F.relu(self.fc1(x))\n",
    "       # x = F.relu(self.fc2(x))\n",
    "       # x = self.fc3(x)  \n",
    "       # return x\n",
    "\n",
    "#model = CNN()\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = Optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-26 18:53:03.073873: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-26 18:53:03.073892: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.427\n",
      "[1,   200] loss: 1.393\n",
      "[1,   300] loss: 1.418\n",
      "[1,   400] loss: 1.418\n",
      "[1,   500] loss: 1.436\n",
      "[1,   600] loss: 1.434\n",
      "[1,   700] loss: 1.434\n",
      "epoch: 1, avg_loss: 2.841\n",
      "[2,   100] loss: 1.375\n",
      "[2,   200] loss: 1.396\n",
      "[2,   300] loss: 1.414\n",
      "[2,   400] loss: 1.390\n",
      "[2,   500] loss: 1.437\n",
      "[2,   600] loss: 1.421\n",
      "[2,   700] loss: 1.413\n",
      "epoch: 2, avg_loss: 2.932\n",
      "[3,   100] loss: 1.379\n",
      "[3,   200] loss: 1.374\n",
      "[3,   300] loss: 1.403\n",
      "[3,   400] loss: 1.391\n",
      "[3,   500] loss: 1.398\n",
      "[3,   600] loss: 1.412\n",
      "[3,   700] loss: 1.398\n",
      "epoch: 3, avg_loss: 2.909\n",
      "[4,   100] loss: 1.360\n",
      "[4,   200] loss: 1.383\n",
      "[4,   300] loss: 1.367\n",
      "[4,   400] loss: 1.411\n",
      "[4,   500] loss: 1.403\n",
      "[4,   600] loss: 1.409\n",
      "[4,   700] loss: 1.390\n",
      "epoch: 4, avg_loss: 2.850\n",
      "[5,   100] loss: 1.339\n",
      "[5,   200] loss: 1.368\n",
      "[5,   300] loss: 1.375\n",
      "[5,   400] loss: 1.376\n",
      "[5,   500] loss: 1.384\n",
      "[5,   600] loss: 1.368\n",
      "[5,   700] loss: 1.383\n",
      "epoch: 5, avg_loss: 2.655\n",
      "[6,   100] loss: 1.352\n",
      "[6,   200] loss: 1.362\n",
      "[6,   300] loss: 1.369\n",
      "[6,   400] loss: 1.375\n",
      "[6,   500] loss: 1.359\n",
      "[6,   600] loss: 1.419\n",
      "[6,   700] loss: 1.376\n",
      "epoch: 6, avg_loss: 2.801\n",
      "[7,   100] loss: 1.334\n",
      "[7,   200] loss: 1.363\n",
      "[7,   300] loss: 1.394\n",
      "[7,   400] loss: 1.394\n",
      "[7,   500] loss: 1.364\n",
      "[7,   600] loss: 1.375\n",
      "[7,   700] loss: 1.376\n",
      "epoch: 7, avg_loss: 2.803\n",
      "[8,   100] loss: 1.321\n",
      "[8,   200] loss: 1.345\n",
      "[8,   300] loss: 1.369\n",
      "[8,   400] loss: 1.358\n",
      "[8,   500] loss: 1.370\n",
      "[8,   600] loss: 1.382\n",
      "[8,   700] loss: 1.384\n",
      "epoch: 8, avg_loss: 2.754\n",
      "[9,   100] loss: 1.326\n",
      "[9,   200] loss: 1.343\n",
      "[9,   300] loss: 1.344\n",
      "[9,   400] loss: 1.360\n",
      "[9,   500] loss: 1.359\n",
      "[9,   600] loss: 1.389\n",
      "[9,   700] loss: 1.365\n",
      "epoch: 9, avg_loss: 2.678\n",
      "[10,   100] loss: 1.325\n",
      "[10,   200] loss: 1.348\n",
      "[10,   300] loss: 1.349\n",
      "[10,   400] loss: 1.364\n",
      "[10,   500] loss: 1.388\n",
      "[10,   600] loss: 1.354\n",
      "[10,   700] loss: 1.389\n",
      "epoch: 10, avg_loss: 2.934\n",
      "tensor([[-3.2557, -1.8103, -2.5176, -2.6411, -2.9708, -2.8987, -3.6385, -2.3895,\n",
      "         -3.1280, -3.1031, -3.4386, -2.5086, -3.2291],\n",
      "        [-4.4679, -3.0543, -2.9184, -3.6553, -2.7548, -4.1662, -3.3610, -4.2164,\n",
      "         -2.5644, -4.3052, -3.6071, -3.2010, -3.5188]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
    "#Train the model\n",
    "    batch_loss =[]\n",
    "    writer = SummaryWriter()\n",
    "    writer2 = SummaryWriter()\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "        \n",
    "            features, label = batch\n",
    "            #features = features.to(device)\n",
    "            #label = label.to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "   \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_loss.append(loss.item())\n",
    "            if (i+1) % 100 == 0:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch +1, i+1, running_loss / 200))\n",
    "                running_loss = 0.0\n",
    "                #print('Epoch [{}/{}, Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "        avg_loss = sum(batch_loss[-3:])/3\n",
    "        print('epoch: %d, avg_loss: %.3f' % (epoch + 1, avg_loss))\n",
    "        writer2.add_scalar('Avg Loss', avg_loss, epoch)\n",
    "    writer.flush()\n",
    "    writer2.flush()\n",
    "    #print('{} loss: {: {:.4f}, acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "        \n",
    "    print(outputs)\n",
    "model_ft = torchvision.models.resnet50(pretrained=True)\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#summary(model_ft, (3, 64, 64))\n",
    "num_fltrs = model_ft.fc.in_features\n",
    "\n",
    "model_ft.fc = torch.nn.Linear(num_fltrs, 13)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model = train_model(model_ft, criterion, optimizer, scheduler, num_epochs=10)\n",
    "\n",
    "#still getting more losses improve the loop and fine tune the network further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the test images: 0.50\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "model_ft.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in train_loader:\n",
    "            features, label = batch\n",
    "            test_output = model_ft(features)\n",
    "            last_layer = test_output\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == label).sum().item() / float(label.size(0))\n",
    "    print('Test Accuracy of the model on the test images: %.2f' % accuracy)\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c14a6692a215417f5263d283ebf372218c2d1b9771ac18dab80d5263bb04c7bf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('selenium_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
